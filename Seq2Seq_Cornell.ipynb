{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFtFWZ-KMf8a",
        "outputId": "a93d5b61-c868-45b0-eea0-87cb4e686ce0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5AtTALfMhQB",
        "outputId": "9f89630d-70bb-4be4-9a8e-c7e0ed6f8c77"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/cornell_movie_dialogs_corpus.zip\" -d \"/content/cornell_movie\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/cornell_movie_dialogs_corpus.zip\n",
            "   creating: /content/cornell_movie/cornell movie-dialogs corpus/\n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: /content/cornell_movie/__MACOSX/\n",
            "   creating: /content/cornell_movie/__MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: /content/cornell_movie/__MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: /content/cornell_movie/__MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: /content/cornell_movie/cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: /content/cornell_movie/__MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpXg1azbMuMR"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiSznumsMv2Q"
      },
      "source": [
        "import codecs\n",
        "\n",
        "with codecs.open(\"/content/cornell_movie/cornell movie-dialogs corpus/movie_lines.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "    conversations = []\n",
        "    for line in lines:\n",
        "        data = line.split(\" +++$+++ \")\n",
        "        conversations.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgROZjkLMxdA"
      },
      "source": [
        "chats = {}\n",
        "for tokens in conversations:\n",
        "    if len(tokens) > 4:\n",
        "        idx = tokens[0][1:]\n",
        "        chat = tokens[4]\n",
        "        chats[int(idx)] = chat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvAgl0-CMzLQ",
        "outputId": "77651d66-052d-4ecf-ea5a-565eac30bc95"
      },
      "source": [
        "sorted_chats = sorted(chats.items(), key = lambda x: x[0])\n",
        "sorted_chats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(49, 'Did you change your hair?'),\n",
              " (50, 'No.'),\n",
              " (51, 'You might wanna think about it'),\n",
              " (59, 'I missed you.'),\n",
              " (60, 'It says here you exposed yourself to a group of freshmen girls.'),\n",
              " (61, 'It was a bratwurst.  I was eating lunch.'),\n",
              " (62, 'With the teeth of your zipper?'),\n",
              " (63, 'You the new guy?'),\n",
              " (64, 'So they tell me...'),\n",
              " (65, \"C'mon.  I'm supposed to give you the tour.\"),\n",
              " (66, 'So -- which Dakota you from?'),\n",
              " (67, \"North, actually.  How'd you   ?\"),\n",
              " (68, 'I was kidding. People actually live there?'),\n",
              " (69, \"Yeah.  A couple.  We're outnumbered by the cows, though.\"),\n",
              " (70, 'How many people were in your old school?'),\n",
              " (71, 'Thirty-two.'),\n",
              " (72, 'Get out!'),\n",
              " (73, 'How many people go here?'),\n",
              " (74, 'Couple thousand. Most of them evil'),\n",
              " (77, \"That I'm used to.\"),\n",
              " (78,\n",
              "  'Yeah, but these guys have never seen a horse.  They just jack off to Clint Eastwood.'),\n",
              " (87, 'That girl -- I --'),\n",
              " (88, 'You burn, you pine, you perish?'),\n",
              " (89, 'Who is she?'),\n",
              " (90, \"Bianca Stratford.  Sophomore. Don't even think about it\"),\n",
              " (91, 'Why not?'),\n",
              " (92,\n",
              "  \"I could start with your haircut, but it doesn't matter.  She's not allowed to date until her older sister does.  And that's an impossibility.\"),\n",
              " (103,\n",
              "  \"Katarina Stratford.  My, my.  You've been terrorizing Ms. Blaise again.\"),\n",
              " (104, 'Expressing my opinion is not a terrorist action.'),\n",
              " (105,\n",
              "  \"Well, yes, compared to your other choices of expression this year, today's events are quite mild.  By the way, Bobby Rictor's gonad retrieval operation went quite well, in case you're interested.\"),\n",
              " (106,\n",
              "  'I still maintain that he kicked himself in the balls.  I was merely a spectator.'),\n",
              " (107, 'The point is Kat -- people perceive you as somewhat ...'),\n",
              " (108, 'Tempestuous?'),\n",
              " (109, 'No ... I believe \"heinous bitch\" is the term used most often.'),\n",
              " (123, \"Who's that?\"),\n",
              " (124, 'Patrick Verona   Random skid.'),\n",
              " (125,\n",
              "  \"That's Pat Verona? The one who was gone for a year? I heard he was doing porn movies.\"),\n",
              " (126,\n",
              "  \"I'm sure he's completely incapable of doing anything that interesting.\"),\n",
              " (127, 'He always look so'),\n",
              " (128, 'Block E?'),\n",
              " (129, 'Mandella, eat.  Starving yourself is a very slow way to die.'),\n",
              " (130, 'Just a little.'),\n",
              " (131, \"What's this?\"),\n",
              " (132, 'An attempted slit.'),\n",
              " (133,\n",
              "  \"I realize that the men of this fine institution are severely lacking, but killing yourself so you can be with William Shakespeare is beyond the scope of normal teenage obsessions.  You're venturing far past daytime talk show fodder and entering the world of those who need very expensive therapy.\"),\n",
              " (134, \"But imagine the things he'd say during sex.\"),\n",
              " (139, 'Why do girls like that always like guys like that?'),\n",
              " (140,\n",
              "  \"Because they're bred to.  Their mothers liked guys like that, and their grandmothers before them. Their gene pool is rarely diluted.\"),\n",
              " (141, 'He always have that shit-eating grin?'),\n",
              " (142,\n",
              "  \"Joey Dorsey?  Perma-shit-grin.  I wish I could say he's a moron, but he's number twelve in the class.  And a model.  Mostly regional stuff, but he's rumored to have a big tube sock ad coming out.\"),\n",
              " (143, 'You know French?'),\n",
              " (144, \"Sure do ... my Mom's from Canada\"),\n",
              " (145, 'Guess who just signed up for a tutor?'),\n",
              " (146, \"You mean I'd get a chance to talk to her?\"),\n",
              " (147, 'You could consecrate with her, my friend.'),\n",
              " (148, \"The vintage look is over, Kat. Haven't you been reading your Sassy?\"),\n",
              " (149,\n",
              "  'Yeah, and I noticed the only part of you featured in your big Kmart spread was your elbow.  Tough break.'),\n",
              " (150, \"They're running the rest of me next month.\"),\n",
              " (151, 'The people at this school are so incredibly foul.'),\n",
              " (152, \"You could always go with me.  I'm sure William has some friends.\"),\n",
              " (157, 'Yeah, just a minor encounter with the shrew.'),\n",
              " (158, \"That's her?  Bianca's sister?\"),\n",
              " (159, 'The mewling, rampalian wretch herself.'),\n",
              " (161, 'In the microwave.'),\n",
              " (162, 'Make anyone cry today?'),\n",
              " (164, \"Where've you been?\"),\n",
              " (165, 'Nowhere... Hi, Daddy.'),\n",
              " (170, \"What's a synonym for throbbing?\"),\n",
              " (171, 'Sarah Lawrence is on the other side of the country.'),\n",
              " (172, 'I know.'),\n",
              " (173, 'I thought we decided you were going to school here.  At U of 0.'),\n",
              " (174, 'You decided.'),\n",
              " (179,\n",
              "  \"Now don't get upset. Daddy, but there's this boy... and I think he might ask...\"),\n",
              " (180,\n",
              "  \"No! You're not dating until your sister starts dating.  End of discussion.\"),\n",
              " (181, 'What if she never starts dating?'),\n",
              " (182, \"Then neither will you.  And I'll get to sleep at night.\"),\n",
              " (183, \"But it's not fair -- she's a mutant, Daddy!\"),\n",
              " (184, 'This from someone whose diary is devoted to favorite grooming tips?'),\n",
              " (185, 'Enough!'),\n",
              " (189, \"But she doesn't want to date.\"),\n",
              " (190, 'Exactly my point'),\n",
              " (191,\n",
              "  'Jesus!  Can a man even grab a sandwich before you women start dilating?'),\n",
              " (192, 'Tumescent!'),\n",
              " (193, \"You're not helping.\"),\n",
              " (194,\n",
              "  'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'),\n",
              " (195,\n",
              "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"),\n",
              " (196, 'Not the hacking and gagging and spitting part.  Please.'),\n",
              " (197,\n",
              "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"),\n",
              " (198, \"You're asking me out.  That's so cute. What's your name again?\"),\n",
              " (199, 'Forget it.'),\n",
              " (200, \"No, no, it's my fault -- we didn't have a proper introduction ---\"),\n",
              " (201, 'Cameron.'),\n",
              " (202,\n",
              "  \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"),\n",
              " (203, 'Seems like she could get a date easy enough...'),\n",
              " (204, 'Why?'),\n",
              " (205,\n",
              "  'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'),\n",
              " (206, \"That's a shame.\"),\n",
              " (207, 'Gosh, if only we could find Kat a boyfriend...'),\n",
              " (208, 'Let me see what I can do.'),\n",
              " (210,\n",
              "  'I teach her French, get to know her, dazzle her with charm and she falls in love with me.'),\n",
              " (211,\n",
              "  \"Unlikely, but even so, she still can't go out with you.  So what's the point?\"),\n",
              " (212, 'What about him?'),\n",
              " (213, 'You wanna go out with him?'),\n",
              " (215, \"What makes you think he'll do it?\"),\n",
              " (216, 'He seems like he thrives on danger'),\n",
              " (217,\n",
              "  \"No kidding.  He's a criminal.  I heard he lit a state trooper on fire.  He just got out of Alcatraz...\"),\n",
              " (218, 'They always let felons sit in on Honors Biology?'),\n",
              " (219,\n",
              "  \"I'm serious, man, he's whacked.  He sold his own liver on the black market so he could buy new speakers.\"),\n",
              " (220, \"Forget his reputation.  Do you think we've got a plan or not?\"),\n",
              " (221, \"Did she actually say she'd go out with you?\"),\n",
              " (222, \"That's what I just said\"),\n",
              " (223,\n",
              "  \"You know, if you do go out with Bianca, you'd be set.  You'd outrank everyone. Strictly A-list.  With me by your side.\"),\n",
              " (224, 'I thought you hated those people.'),\n",
              " (225, \"Hey -- I've gotta have a few clients when I get to Wall Street.\"),\n",
              " (231, 'Hey.'),\n",
              " (232, 'Are you lost?'),\n",
              " (233, 'Nope - just came by to chat'),\n",
              " (234, \"We don't chat.\"),\n",
              " (235,\n",
              "  \"Well, actually, I thought I'd run an idea by you.  You know, just to see if you're interested.\"),\n",
              " (236, \"We're not.\"),\n",
              " (238,\n",
              "  \"But she can't go out with you because her sister is this insane head case and no one will go out with her. right?\"),\n",
              " (239, 'Does this conversation have a purpose?'),\n",
              " (240,\n",
              "  \"So what you need to do is recruit a guy who'll go out with her.  Someone who's up for the job.\"),\n",
              " (244, 'You got him involved?'),\n",
              " (245,\n",
              "  \"Like we had a choice?  Besides -- when you let the enemy think he's orchestrating the battle, you're in a position of power. We let him pretend he's calling the shots, and while he's busy setting up the plan, you have time to woo Bianca.\"),\n",
              " (247,\n",
              "  'So he has this huge raging fit about Sarah Lawrence and insists that I go to his male-dominated, puking frat boy, number one golf team school. I have no say at all.'),\n",
              " (248, 'William would never have gone to a state school.'),\n",
              " (249, \"William didn't even go to high school\"),\n",
              " (250, \"That's never been proven\"),\n",
              " (251, 'Neither has his heterosexuality.'),\n",
              " (252,\n",
              "  \"I appreciate your efforts toward a speedy death, but I'm consuming.  Do you mind?\"),\n",
              " (253, 'Does it matter?'),\n",
              " (254,\n",
              "  'If I was Bianca, it would be, \"Any school you want, precious.  Don\\'t forget your tiara.\"'),\n",
              " (257,\n",
              "  \"I don't understand, Patrick.  You haven't done anything asinine this week. Are you not feeling well?\"),\n",
              " (258, 'Touch of the flu.'),\n",
              " (259,\n",
              "  \"I'm at a loss, then.  What should we talk about? Your year of absence?\"),\n",
              " (261, \"Why don't we discuss your driving need to be a hemorrhoid?\"),\n",
              " (262, \"What's to discuss?\"),\n",
              " (263,\n",
              "  \"You weren't abused, you aren't stupid, and as far as I can tell, you're only slightly psychotic -- so why is it that you're such a fuck-up?\"),\n",
              " (264,\n",
              "  \"Well, you know -- there's the prestige of the job title... and the benefits package is pretty good...\"),\n",
              " (271, \"C'esc ma tete. This is my head\"),\n",
              " (272, \"Right.  See?  You're ready for the quiz.\"),\n",
              " (273,\n",
              "  \"I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\"),\n",
              " (274, \"That's because it's such a nice one.\"),\n",
              " (275, 'Forget French.'),\n",
              " (276, 'How is our little Find the Wench A Date plan progressing?'),\n",
              " (277, \"Well, there's someone I think might be --\"),\n",
              " (280, 'There.'),\n",
              " (281, 'Where?'),\n",
              " (285, 'Yeah'),\n",
              " (286, 'What do you think?'),\n",
              " (287, 'Two legs, nice rack...'),\n",
              " (288, 'Yeah, whatever.  I want you to go out with her.'),\n",
              " (289, \"Sure, Sparky.  I'll get right on it.\"),\n",
              " (290, 'You just said'),\n",
              " (291, 'You need money to take a girl out'),\n",
              " (292, \"But you'd go out with her if you had the cake?\"),\n",
              " (294, 'You got it, Verona.  I pick up the tab, you do the honors.'),\n",
              " (295, \"You're gonna pay me to take out some girl?\"),\n",
              " (296,\n",
              "  \"I can't date her sister until that one gets a boyfriend.  And that's the catch. She doesn't want a boyfriend.\"),\n",
              " (297, 'How much?'),\n",
              " (298, \"I can't take a girl like that out on twenty bucks.\"),\n",
              " (299, 'Fine, thirty.'),\n",
              " (300, \"Take it or leave it.  This isn't a negotiation.\"),\n",
              " (301, \"Fifty, and you've got your man.\"),\n",
              " (304, \"I mean Wo-man.  How ya doin'?\"),\n",
              " (305, 'Sweating like a pig, actually.  And yourself?'),\n",
              " (306, \"There's a way to get a guy's attention.\"),\n",
              " (307, 'My mission in life.'),\n",
              " (309, 'Pick you up Friday, then'),\n",
              " (310, 'Oh, right.  Friday.'),\n",
              " (311, \"The night I take you to places you've never been before.  And back.\"),\n",
              " (312,\n",
              "  'Like where?  The 7-Eleven on Burnside? Do you even know my name, screwboy?'),\n",
              " (313, 'I know a lot more than that'),\n",
              " (319,\n",
              "  \"I have the potential to smack the crap out of you if you don't get out of my way.\"),\n",
              " (320, 'Can you at least start wearing a bra?'),\n",
              " (322, \"You hate me don't you?\"),\n",
              " (323, \"I don't really think you warrant that strong an emotion.\"),\n",
              " (324, \"Then say you'll spend Dollar Night at the track with me.\"),\n",
              " (325, 'And why would I do that?'),\n",
              " (326,\n",
              "  'Come on -- the ponies, the flat beer, you with money in your eyes, me with my hand on your ass...'),\n",
              " (327, 'You -- covered in my vomit.'),\n",
              " (328, 'Seven-thirty?'),\n",
              " (330, 'Are you following me?'),\n",
              " (331, \"I was in the laundromat. I saw your car. Thought I'd say hi.\"),\n",
              " (332, 'Hi'),\n",
              " (333, \"You're not a big talker, are you?\"),\n",
              " (334,\n",
              "  \"Depends on the topic. My fenders don't really whip me into a verbal frenzy.\"),\n",
              " (335, 'Hey -- do you mind?'),\n",
              " (336, 'Not at all'),\n",
              " (338, 'My insurance does not cover PMS'),\n",
              " (339, 'Then tell them I had a seizure.'),\n",
              " (340, 'Is this about Sarah Lawrence? You punishing me?'),\n",
              " (341, 'I thought you were punishing me.'),\n",
              " (342, \"Why can't we agree on this?\"),\n",
              " (343, \"Because you're making decisions for me.\"),\n",
              " (344, \"As a parent, that's my right\"),\n",
              " (345, \"So what I want doesn't matter?\"),\n",
              " (346,\n",
              "  \"You're eighteen.  You don't know what you want.  You won't know until you're forty-five and you don't have it.\"),\n",
              " (347,\n",
              "  'I want to go to an East Coast school! I want you to trust me to make my own choices.  I want --'),\n",
              " (349, 'When I shell out fifty, I expect results.'),\n",
              " (350, \"I'm on it\"),\n",
              " (351, \"Watching the bitch trash my car doesn't count as a date.\"),\n",
              " (352,\n",
              "  'I got her under control. She just acts crazed in public to keep up the image.'),\n",
              " (354, 'I just upped my price'),\n",
              " (355, 'What?'),\n",
              " (356, 'A hundred bucks a date.'),\n",
              " (357, 'Forget it.'),\n",
              " (358, 'Forget her sister, then.'),\n",
              " (363, 'You got something on your mind?'),\n",
              " (364,\n",
              "  \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\"),\n",
              " (365, 'You have my word.  As a gentleman'),\n",
              " (366, \"You're sweet.\"),\n",
              " (367, 'How do you get your hair to look like that?'),\n",
              " (368,\n",
              "  \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\"),\n",
              " (371, 'Say it'),\n",
              " (372, 'What?'),\n",
              " (373, \"Whatever the hell it is you're standin' there waitin' to say.\"),\n",
              " (375, 'What plan?'),\n",
              " (376,\n",
              "  'The situation is, my man Cameron here has a major jones for Bianca Stratford.'),\n",
              " (377, 'What is it with this chick?  She have three tits?'),\n",
              " (378,\n",
              "  \"I think I speak correctly when I say that Cameron's love is pure.  Purer than say -- Joey Dorsey's.\"),\n",
              " (379, \"Dorsey can plow whoever he wants. I'm just in this for the cash.\"),\n",
              " (380, \"That's where we can help you.  With Kat.\"),\n",
              " (381, 'So Dorsey can get the girl?'),\n",
              " (382,\n",
              "  \"Patrick, Pat, you're not looking at the big picture.  Joey's just a pawn. We set this whole thing up so Cameron can get the girl.\"),\n",
              " (383, 'You two are gonna help me tame the wild beast?'),\n",
              " (384, \"We're your guys.\"),\n",
              " (385, 'And he means that strictly in a non- prison-movie type of way.'),\n",
              " (386, \"Yeah -- we'll see.\"),\n",
              " (388,\n",
              "  'This is it.  A golden opportunity. Patrick can ask Katarina to the party.'),\n",
              " (389, \"In that case, we'll need to make it a school-wide blow out.\"),\n",
              " (390, 'Will Bogey get bent?'),\n",
              " (391,\n",
              "  \"Are you kidding?  He'll piss himself with joy.  He's the ultimate kiss ass.\"),\n",
              " (394, \"It's more\"),\n",
              " (395, 'Expensive?'),\n",
              " (396, \"Exactly  So, you going to Bogey Lowenbrau's thing on Saturday?\"),\n",
              " (397, 'Hopefully.'),\n",
              " (401, 'Sure have.'),\n",
              " (402,\n",
              "  \"I really, really, really wanna go, but I can't.  Not unless my sister goes.\"),\n",
              " (403, \"I'm workin' on it. But she doesn't seem to be goin' for him.\"),\n",
              " (404, \"She's not a...\"),\n",
              " (405,\n",
              "  \"Lesbian?  No. I found a picture of Jared Leto in one of her drawers, so I'm pretty sure she's not harboring same-sex tendencies.\"),\n",
              " (406, \"So that's the kind of guy she likes? Pretty ones?\"),\n",
              " (407,\n",
              "  \"Who knows?  All I've ever heard her say is that she'd dip before dating a guy that smokes.\"),\n",
              " (411, \"What've you got for me?\"),\n",
              " (412,\n",
              "  \"I've retrieved certain pieces of information on Miss Katarina Stratford I think you'll find helpful.\"),\n",
              " (414, 'What?!'),\n",
              " (415, 'Good enough.'),\n",
              " (416, 'Number one.  She hates smokers'),\n",
              " (417, \"It's a lung cancer issue\"),\n",
              " (418, 'Her favorite uncle'),\n",
              " (419, 'Dead at forty-one.'),\n",
              " (420, 'Are you telling me I\\'m a -  \"non-smoker\"?'),\n",
              " (421, 'Just for now.'),\n",
              " (424, \"He's pretty!\"),\n",
              " (425, \"Okay!  I wasn't sure\"),\n",
              " (426,\n",
              "  'Okay -- Likes:  Thai food, feminist prose, and \"angry, stinky girl music of the indie-rock persuasion\".'),\n",
              " (427,\n",
              "  \"So what does that give me?  I'm supposed to buy her some noodles and a book and sit around listening to chicks who can't play their instruments?\"),\n",
              " (428, 'Ever been to Club Skunk?'),\n",
              " (429, 'Yeah.'),\n",
              " (430, 'Gigglepuss is playing there tomorrow night.'),\n",
              " (431, \"Don't make me do it, man\"),\n",
              " (432, 'Assail your ears for one night.'),\n",
              " (433, \"It's her favorite band.\"),\n",
              " (436,\n",
              "  'I prefer to think of it simply as an alternative to what the law allows.'),\n",
              " (437, \"I'm likin' you guys better\"),\n",
              " (441, \"Oh my God, does this mean you're becoming normal?\"),\n",
              " (442, \"It means that Gigglepuss is playing at Club Skunk and we're going.\"),\n",
              " (443,\n",
              "  \"Oh, I thought you might have a date  I don't know why I'm bothering to ask, but are you going to Bogey Lowenstein's party Saturday night?\"),\n",
              " (444, 'What do you think?'),\n",
              " (445,\n",
              "  \"I think you're a freak.  I think you do this to torture me.  And I think you suck.\"),\n",
              " (447, \"You think this'll work?\"),\n",
              " (448, 'No fear.'),\n",
              " (458, 'Always a pleasure, Brucie.'),\n",
              " (459,\n",
              "  \"Didn't have you pegged for a Gigglepuss fan.  Aren't they a little too pre-teen belly-button ring for you?\"),\n",
              " (460, 'Fan of a fan.  You see a couple of minors come in?'),\n",
              " (461, 'Never'),\n",
              " (462,\n",
              "  'Padua girls.  One tall, decent body. The other one kinda short and undersexed?'),\n",
              " (463, \"Just sent 'em through.\"),\n",
              " (473, 'hey.  Great show, huh?'),\n",
              " (474, ''),\n",
              " (475, 'Excuse me?'),\n",
              " (476, \"That's what you want, isn't it?\"),\n",
              " (477, \"Do you mind?  You're sort of ruining it for me.\"),\n",
              " (481,\n",
              "  \"You know, these guys are no Bikini Kill or The Raincoats, but they're right up there.\"),\n",
              " (482, 'You know who The Raincoats are?'),\n",
              " (483, \"Why, don't you?\"),\n",
              " (490, \"What'd he say?\"),\n",
              " (491, 'Who cares?'),\n",
              " (499, 'You told me that part already.'),\n",
              " (500, \"Hell, I've just been going over the whole thing in my head and -\"),\n",
              " (502, \"I hear you're helpin' Verona.\"),\n",
              " (503, \"Uh,  yeah.  We're old friend*\"),\n",
              " (504, 'You and Verona?'),\n",
              " (505, 'What?  We took bathes together when we were kids.'),\n",
              " (508, \"You better not fuck this up.  I'm heavily invested.\"),\n",
              " (509, \"Hey -- it's all for the higher good right?\"),\n",
              " (511, \"You're completely demented.\"),\n",
              " (512, 'See you next week!'),\n",
              " (517, 'Daddy, I --'),\n",
              " (518, \"And where're you going?\"),\n",
              " (519,\n",
              "  'If you must know, we were attempting to go to a small study group of friends.'),\n",
              " (520, 'Otherwise known as an orgy?'),\n",
              " (521,\n",
              "  'It\\'s just a party. Daddy, but I knew you\\'d forbid me to go since \"Gloria Steinem\" over there isn\\'t going --'),\n",
              " (523, 'Daddy, people expect me to be there!'),\n",
              " (524, \"If Kat's not going, you're not going.\"),\n",
              " (525,\n",
              "  \"You're ruining my life'  Because you won't be normal, I can't be normal.\"),\n",
              " (526, \"What's normal?\"),\n",
              " (527,\n",
              "  \"Bogey Lowenstein's party is normal, but you're too busy listening to Bitches Who Need Prozac to know that.\"),\n",
              " (529, \"Can't you forget for just one night that you're completely wretched?\"),\n",
              " (530, \"At least I'm not a clouted fen- sucked hedge-pig.\"),\n",
              " (531, \"Like I'm supposed to know what that even means.\"),\n",
              " (532, \"It's Shakespeare.  Maybe you've heard of him?\"),\n",
              " (533,\n",
              "  \"Yeah, he's your freak friend Mandella's boyfriend.  I guess since I'm not allowed to go out, I should obsess over a dead guy, too.\"),\n",
              " (536, \"Oh, God.  It's starting.\"),\n",
              " (537, \"It's just a party. Daddy.\"),\n",
              " (538, 'Wear the belly before you go.'),\n",
              " (539, 'Daddy, no!'),\n",
              " (540, 'Just for a minute'),\n",
              " (542, 'You are so completely unbalanced.'),\n",
              " (543, 'Can we go now?'),\n",
              " (544, \"Promise me you won't talk to any boys unless your sister is present.\"),\n",
              " (545, 'Why?'),\n",
              " (546, \"Because she'll scare them away.\"),\n",
              " (554, 'Where ya goin?'),\n",
              " (555, 'Away.'),\n",
              " (556, 'Your sister here?'),\n",
              " (557, 'Leave my sister alone.'),\n",
              " (558, 'And why would I do that?'),\n",
              " (564, \"What's this?\"),\n",
              " (565,\n",
              "  '\"I\\'m getting trashed, man.\" Isn\\'t that what you\\'re supposed to do at a party?'),\n",
              " (566, 'I say, do what you wanna do.'),\n",
              " (567, \"Funny, you're the only one\"),\n",
              " (571, 'Where did he go?  He was just here.'),\n",
              " (572, 'Who?'),\n",
              " (573, 'Joey.'),\n",
              " (575, 'Hi.'),\n",
              " (576, 'Looks like things worked out tonight, huh?'),\n",
              " (577, 'You know Chastity?'),\n",
              " (578, 'I believe we share an art instructor'),\n",
              " (579, 'Great'),\n",
              " (580, 'Would you mind getting me a drink, Cameron?'),\n",
              " (583, 'Extremely unfortunate maneuver.'),\n",
              " (584,\n",
              "  \"The hell is that?  What kind of 'guy just picks up a girl and carries her away while you're talking to her?\"),\n",
              " (585, \"Buttholus extremus.  But hey, you're making progress.\"),\n",
              " (586, \"No, I ' m not.\"),\n",
              " (589,\n",
              "  'So yeah, I\\'ve got the Sears catalog thing going -- and the tube sock gig \" that\\'s gonna be huge.  And then I\\'m up for an ad for Queen Harry next week.'),\n",
              " (590, 'Queen Harry?'),\n",
              " (591,\n",
              "  \"It's a gay cruise line, but I'll be, like, wearing a uniform and stuff.\"),\n",
              " (592, 'Neat...'),\n",
              " (593, \"My agent says I've got a good shot at being the Prada guy next year.\"),\n",
              " (595,\n",
              "  \"He practically proposed when he found out we had the same dermatologist. I mean. Dr. Bonchowski is great an all, but he's not exactly relevant party conversation.\"),\n",
              " (596, 'Is he oily or dry?'),\n",
              " (597,\n",
              "  \"Combination.  I don't know -- I thought he'd be different.  More of a gentleman...\"),\n",
              " (598,\n",
              "  \"Bianca, I don't think the highlights of dating Joey Dorsey are going to include door-opening and coat-holding.\"),\n",
              " (599,\n",
              "  \"Sometimes I wonder if the guys we're supposed to want to go out with are the ones we actually want to go out with, you know?\"),\n",
              " (600,\n",
              "  \"All I know is -- I'd give up my private line to go out with a guy like Joey.\"),\n",
              " (601, 'Bianca, I need to talk to you -- I need to tell you --'),\n",
              " (602, \"I really don't think I need any social advice from you right now.\"),\n",
              " (605, \"It's about time.\"),\n",
              " (606, \"A deal's a deal.\"),\n",
              " (607, \"How'd you do it?\"),\n",
              " (608, 'Do what?'),\n",
              " (609, 'Get her to act like a human'),\n",
              " (610, 'Okay?'),\n",
              " (611, \"I'm fine. I'm\"),\n",
              " (612, \"You're not okay.\"),\n",
              " (613, 'I just need to lie down for awhile'),\n",
              " (614, \"Uh, uh. You lie down and you'll go to sleep\"),\n",
              " (615, 'I know, just let me sleep'),\n",
              " (616,\n",
              "  'What if you have a concussion? My dog went to sleep with a concussion and woke up a vegetable. Not that I could tell the difference...'),\n",
              " (619, \"Cameron, I'm a little busy\"),\n",
              " (620, \"It's off. The whole thing.\"),\n",
              " (621, \"What 're you talking about?\"),\n",
              " (622, \"She's partial to Joey, not me\"),\n",
              " (623, 'Cameron -- do you like the girl?'),\n",
              " (624, 'Sure'),\n",
              " (625, 'Then, go get her'),\n",
              " (626, 'This is so patronizing.'),\n",
              " (627, \"Leave it to you to use big words when you're shitfaced.\"),\n",
              " (628, \"Why 're you doing this?\"),\n",
              " (629, 'I told you'),\n",
              " (630, \"You don't care if I die\"),\n",
              " (631, 'Sure, I do'),\n",
              " (632, 'Why?'),\n",
              " (633, \"Because then I'd have to start taking out girls who like me.\"),\n",
              " (634, 'Like you could find one'),\n",
              " (635, \"See that?  Who needs affection when I've got blind hatred?\"),\n",
              " (636, 'Just let me sit down.'),\n",
              " (639, \"Why'd you let him get to you?\"),\n",
              " (640, 'Who?'),\n",
              " (641, 'Dorsey.'),\n",
              " (642, 'I hate him.'),\n",
              " (643,\n",
              "  \"I know.  It'd have to be a pretty big deal to get you to mainline tequila. You don't seem like the type.\"),\n",
              " (644,\n",
              "  'Hey man. . .  You don \\' t think I can be \"cool\"?  You don\\'t think I can be \"laid back\" like everyone else?'),\n",
              " (645, 'I thought you were above all that'),\n",
              " (646, 'You know what they say'),\n",
              " (650, 'Kat! Wake up!'),\n",
              " (651, 'What?'),\n",
              " (655,\n",
              "  \"I don't get you.  You act like you're too good for any of this, and then you go totally apeshit when you get here.\"),\n",
              " (656, \"You're welcome.\"),\n",
              " (659, 'I have to be home in twenty minutes.'),\n",
              " (660, \"I don't have to be home 'til two.\"),\n",
              " (662, 'Have fun tonight?'),\n",
              " (663, 'Tons'),\n",
              " (668, \"And I'm in control of it.\"),\n",
              " (669, \"But it's Gigglepuss - I know you like them.  I saw you there.\"),\n",
              " (670, 'When you were gone last year -- where were you?'),\n",
              " (671, 'Busy'),\n",
              " (672, 'Were you in jail?'),\n",
              " (673, 'Maybe.'),\n",
              " (674, \"No, you weren't\"),\n",
              " (675, \"Then why'd you ask?\"),\n",
              " (676, \"Why'd you lie?\"),\n",
              " (677, 'I should do this.'),\n",
              " (678, 'Do what?'),\n",
              " (679, 'This.'),\n",
              " (680, 'Start a band?'),\n",
              " (681, \"My father wouldn't approve of that that\"),\n",
              " (682, \"You don't strike me as the type that would ask permission.\"),\n",
              " (683, 'Oh, so now you think you know me?'),\n",
              " (684, \"I'm gettin' there\"),\n",
              " (686, \"So what ' s up with your dad?  He a pain in the ass?\"),\n",
              " (687, \"He just wants me to be someone I'm not.\"),\n",
              " (688, 'Who?'),\n",
              " (689, 'BIANCA'),\n",
              " (690,\n",
              "  \"No offense, but you're sister is without.  I know everyone likes her and all, but ...\"),\n",
              " (693,\n",
              "  'I looked for you back at the party, but you always seemed to be \"occupied\".'),\n",
              " (694, 'I was?'),\n",
              " (695, \"You never wanted to go out with 'me, did you?\"),\n",
              " (696, 'Well, no...'),\n",
              " (697, \"Then that's all you had to say.\"),\n",
              " (698, 'But'),\n",
              " (699, 'You always been this selfish?'),\n",
              " (716,\n",
              "  'You went to the party?  I thought we were officially opposed to suburban social activity.'),\n",
              " (717, \"I didn't have a choice.\"),\n",
              " (718,\n",
              "  \"You didn't have a choice?  Where's Kat and what have you done with her?\"),\n",
              " (719, 'I did Bianca a favor and it backfired.'),\n",
              " (720, \"You didn't\"),\n",
              " (721, 'I got drunk.  I puked.  I got rejected. It was big fun.'),\n",
              " (723, 'So you got cozy with she who stings?'),\n",
              " (724, \"No - I've got a sweet-payin' job that I'm about to lose.\"),\n",
              " (725, \"What'd you do to her?\"),\n",
              " (726,\n",
              "  \"I don ' t know.  I decided not to nail her when she was too drunk to remember it.\"),\n",
              " (728, 'Hey there.  Tired of breathing?'),\n",
              " (729, 'Hi.'),\n",
              " (730, 'Cool pictures.  You a fan?'),\n",
              " (731, 'Yeah.  I guess.'),\n",
              " (732, 'You think?'),\n",
              " (733, 'Oh yeah.'),\n",
              " (735, 'Macbeth, right?'),\n",
              " (736, 'Right.'),\n",
              " (737, 'Kat a fan, too?'),\n",
              " (738, 'Yeah...'),\n",
              " (740,\n",
              "  \"She hates you with the fire of a thousand suns .  That's a direct quote\"),\n",
              " (741, \"She just needs time to cool off I'll give it a day.\"),\n",
              " (743, \"You makin' any headway?\"),\n",
              " (744, 'She kissed me.'),\n",
              " (745, 'Where?'),\n",
              " (747,\n",
              "  \"I don't know, Dorsey. ..the limo.-the flowers.  Another hundred for the tux --\"),\n",
              " (748, \"Enough with the Barbie n' Ken shit. I know.\"),\n",
              " (750,\n",
              "  'Can you even imagine?  Who the hell would go to this a bastion of commercial excess?'),\n",
              " (751, \"Well, I guess we're not, since we don't have dates .\"),\n",
              " (752,\n",
              "  'Listen to you!  You sound like Betty, all pissed off because Archie is taking Veronica.'),\n",
              " (753, \"Okay, okay, we won't go.  It's not like I have a dress anyway\"),\n",
              " (754,\n",
              "  \"You ' re looking at this from the wrong perspective.  We're making a statement.\"),\n",
              " (755, 'Oh, good.  Something new and different for us.'),\n",
              " (756, 'Hey, sweet cheeks.'),\n",
              " (757, 'Hi, Joey.'),\n",
              " (758, \"You're concentrating awfully hard considering it's gym class.\"),\n",
              " (759, 'Listen, I want to talk to you about the prom.'),\n",
              " (760, \"You know the deal.  I can ' t go if Kat doesn't go --\"),\n",
              " (764, 'Excuse me, have you seen The Feminine Mystique?  I lost my copy.'),\n",
              " (765, 'What are you doing here?'),\n",
              " (766, 'I heard there was a poetry reading.'),\n",
              " (767, \"You 're so --\"),\n",
              " (768, 'Pleasant?'),\n",
              " (769, 'Wholesome.'),\n",
              " (770, 'Unwelcome.'),\n",
              " (771, 'Unwelcome?  I guess someone still has her panties in a twist.'),\n",
              " (772,\n",
              "  \"Don't for one minute think that you had any effect whatsoever on my panties.\"),\n",
              " (773, 'So what did I have an effect on ?'),\n",
              " (774, 'Other than my upchuck reflex? Nothing.'),\n",
              " (775, \"You were right. She's still pissed.\"),\n",
              " (776, 'Sweet love, renew thy force!'),\n",
              " (777, \"Man -- don't say shit like that to  me. People can hear you.\"),\n",
              " (778,\n",
              "  'You humiliated the woman! Sacrifice yourself on the altar of dignity and even the score.'),\n",
              " (779, \"Best case scenario, you're back on the payroll for awhile.\"),\n",
              " (780, \"What's the worst?\"),\n",
              " (781, 'You get the girl.'),\n",
              " (798, 'He left!  I sprung the dickhead and he cruised on me.'),\n",
              " (799, 'Look up, sunshine'),\n",
              " (800, \"I guess I never told you I'm afraid of heights.\"),\n",
              " (801, \"C'mon.  It's not that bad\"),\n",
              " (802, \"Try lookin' at it from this angle\"),\n",
              " (803, 'Put your right foot there --'),\n",
              " (804, \"Forget it.  I'm stayin'.\"),\n",
              " (805, 'You want me to climb up and show you how to get down?'),\n",
              " (806, 'Maybe.'),\n",
              " (808, 'The Partridge Family?'),\n",
              " (809,\n",
              "  'I figured it had to be something ridiculous to win your respect.  And piss you off.'),\n",
              " (810, 'Good call.'),\n",
              " (811, \"So how'd you get Chapin to look the other way?\"),\n",
              " (812, 'I dazzled him with my wit'),\n",
              " (813, 'A soft side? Who knew?'),\n",
              " (814, \"Yeah, well, don't let it get out\"),\n",
              " (815, \"So what's your excuse?\"),\n",
              " (816, 'Acting the way we do.'),\n",
              " (817, 'Yes'),\n",
              " (818,\n",
              "  \"I don't like to do what people expect. Then they expect it all the time and they get disappointed when you change.\"),\n",
              " (819, \"So if you disappoint them from the start, you're covered?\"),\n",
              " (820, 'Something like that'),\n",
              " (821, 'Then you screwed up'),\n",
              " (822, 'How?'),\n",
              " (823, 'You never disappointed me.'),\n",
              " (824, 'You up for it?'),\n",
              " (825, 'For. . . ?'),\n",
              " (828, 'State trooper?'),\n",
              " (829, 'Fallacy.'),\n",
              " (830, 'The duck?'),\n",
              " (831, 'Hearsay.'),\n",
              " (832, \"I know the porn career's a lie.\"),\n",
              " (834, 'Tell me something true.'),\n",
              " (835, 'I hate peas.'),\n",
              " (836, 'No -- something real.  Something no one else knows.'),\n",
              " (837, \"You're sweet.  And sexy.  And completely hot for me.\"),\n",
              " (838, 'What?'),\n",
              " (839, 'No one else knows'),\n",
              " (840, \"You're amazingly self-assured. Has anyone ever told you that?\"),\n",
              " (841, 'Go to the prom with me'),\n",
              " (842, 'Is that a request or a command?'),\n",
              " (843, 'You know what I mean'),\n",
              " (844, 'No.'),\n",
              " (845, 'No what?'),\n",
              " (846, \"No, I won't go with you\"),\n",
              " (847, 'Why not?'),\n",
              " (848, \"Because I don't want to. It's a stupid tradition.\"),\n",
              " (852, 'Create a little drama?  Start a new rumor?  What?'),\n",
              " (853, 'So I have to have a motive to be with you?'),\n",
              " (854, 'You tell me.'),\n",
              " (855, 'You need therapy.  Has anyone ever told you that?'),\n",
              " (856, 'Answer the question, Patrick'),\n",
              " (857,\n",
              "  \"Nothing!  There's nothing in it for me. Just the pleasure of your company.\"),\n",
              " (860,\n",
              "  'Then Guillermo says, \"If you go any lighter, you\\'re gonna look like an extra on 90210.\"'),\n",
              " (861, 'No...'),\n",
              " (862, 'do you listen to this crap?'),\n",
              " (863, 'What crap?'),\n",
              " (864, \"Me.  This endless ...blonde babble. I'm like, boring myself.\"),\n",
              " (865, 'Thank God!  If I had to hear one more story about your coiffure...'),\n",
              " (866, \"I figured you'd get to the good stuff eventually.\"),\n",
              " (867, 'What good stuff?'),\n",
              " (868, 'The \"real you\".'),\n",
              " (869, 'Like my fear of wearing pastels?'),\n",
              " (870,\n",
              "  'I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?'),\n",
              " (871, 'No'),\n",
              " (872, \"Okay -- you're gonna need to learn how to lie.\"),\n",
              " (876, 'Would you rather be ravished by a pirate or a British rear admiral?'),\n",
              " (877, 'Pirate -- no question.'),\n",
              " (878, \"Daddy, I want to discuss the prom with you. It's tomorrow night --\"),\n",
              " (879, 'The prom?  Kat has a date?'),\n",
              " (880, 'No, but'),\n",
              " (881,\n",
              "  \"It's that hot rod Joey, right? That ' s who you want me to bend my rules for?\"),\n",
              " (882, 'He\\'s not a \"hot rod\".  Whatever that is.'),\n",
              " (883, \"You're not going unless your sister goes.  End of story.\"),\n",
              " (884,\n",
              "  \"Fine.  I see that I'm a prisoner in my own house.  I'm not a daughter. I'm a possession!\"),\n",
              " (886, \"They'll dance, they'll kiss, they'll come home.  Let her go.\"),\n",
              " (887,\n",
              "  \"Kissing?  Is that what you think happens?  Kissing isn't what keeps me up to my elbows in placenta all day.\"),\n",
              " (889,\n",
              "  \"Listen, I know you hate having to sit home because I'm not Susie High School.\"),\n",
              " (890, 'Like you care.'),\n",
              " (891,\n",
              "  \"I do care. But I'm a firm believer in doing something for your own reasons, not someone else ' s .\"),\n",
              " (892,\n",
              "  \"I wish I had that luxury. I'm the only sophomore that got asked to the prom and I can't go, because you won ' t.\"),\n",
              " (893, 'Joey never told you we went out, did he?'),\n",
              " (894, 'What?'),\n",
              " (895, 'In 9th.  For a month'),\n",
              " (896, 'Why?'),\n",
              " (897, 'He was, like, a total babe'),\n",
              " (898, 'But you hate Joey'),\n",
              " (899, 'Now I do.  Back then, was a different story.'),\n",
              " (900, 'As in...'),\n",
              " (901, 'He said everyone was doing it.  So I did it.'),\n",
              " (902, 'You did what?'),\n",
              " (903,\n",
              "  \"Just once.  Afterwards, I told him I didn't want to anymore.  I wasn't ready. He got pissed.  Then he broke up with me.\"),\n",
              " (904, 'But'),\n",
              " (905,\n",
              "  'After that, I swore I\\'d never do anything just because \"everyone else\" was doing it.  And I haven\\'t since. Except for Bogey\\'s party, and my stunning gastro-intestinal display --'),\n",
              " (906, \"Why didn't you tell me?\"),\n",
              " (907, 'I wanted to let you make up your own mind about him.'),\n",
              " (908,\n",
              "  \"No. you didn't!  If you really thought I could make my own decisions, you would've let me go out with him instead of helping Daddy hold me hostage.\"),\n",
              " (909, \"That's not\"),\n",
              " (910, \"I'm not stupid enough to repeat your mistakes.\"),\n",
              " (911, 'I guess I thought I was protecting you.'),\n",
              " (912,\n",
              "  \"God, you're just like him! Just keep me locked away in the dark, so I can't experience anything for myself\"),\n",
              " (913,\n",
              "  \"Not all experiences are good, Bianca. You can't always trust the people you want to.\"),\n",
              " (914, \"I guess I'll never know, will I?\"),\n",
              " (917, \"What do you wanna watch?  We've got crap, crap, crap or crap\"),\n",
              " (918, 'Dr. Ruth?'),\n",
              " (922, \"I'm missing something.\"),\n",
              " (923,\n",
              "  \"I have a date, Daddy.  And he ' s not a captain of oppression like some men we know.\"),\n",
              " (924, 'Wow'),\n",
              " (925, \"Let's go.\"),\n",
              " (926, 'Have a great time, honey!'),\n",
              " (927, 'But -- who -- what --?'),\n",
              " (929, 'What just happened?'),\n",
              " (930, 'Your daughters went to the prom.'),\n",
              " (931, 'Did I have anything to say about it?'),\n",
              " (932, 'Absolutely not.'),\n",
              " (933, \"That ' s what I thought\"),\n",
              " (936, \"How'd you get a tux at the last minute?\"),\n",
              " (937, \"It's Scurvy's.  His date got convicted. Where'd you get the dress?\"),\n",
              " (938, \"It's just something I had.  You know\"),\n",
              " (939, 'Oh huh'),\n",
              " (940, \"Look, I'm  -- sorry -- that I questioned your motives.  I was wrong.\"),\n",
              " (946, 'Have you seen him?'),\n",
              " (947, 'Who?'),\n",
              " (948, 'William - he asked me to meet him here.'),\n",
              " (949,\n",
              "  \"Oh, honey -- tell me we haven't' progressed to full-on hallucinations.\"),\n",
              " (952, \"You think you ' re the only sophomore at the prom?\"),\n",
              " (953, 'I did.'),\n",
              " (957, \"My grandmother's .\"),\n",
              " (958, 'What?'),\n",
              " (959,\n",
              "  \"That's where I was last year.  She'd never lived alone -- my grandfather died -- I stayed with her.  I wasn't in jail, I don't know Marilyn Manson, and I've never slept with a Spice Girl.  I spent a year sitting next to my grandma on the couch watching Wheel of Fortune.  End of story.\"),\n",
              " (960, \"That ' s completely adorable!\"),\n",
              " (961, 'It gets worse -- you still have your freshman yearbook?'),\n",
              " (973, 'Wait I...'),\n",
              " (974,\n",
              "  'You were paid to take me out!  By -- the one person I truly hate.  I knew it was a set-up!'),\n",
              " (975, \"It wasn't like that.\"),\n",
              " (976,\n",
              "  'Really?  What was it like?  A down payment now, then a bonus for sleeping with me?'),\n",
              " (977, \"I didn't care about the money.\"),\n",
              " (982, 'You looked beautiful last night, you know.'),\n",
              " (983, 'So did you'),\n",
              " (984, 'She okay?'),\n",
              " (985, 'I hope so.'),\n",
              " (986, 'Was that your sister?'),\n",
              " (987, 'Yeah.  She left with some bikers Big ones.  Full of sperm.'),\n",
              " (988, 'Funny.'),\n",
              " (989,\n",
              "  \"I don't understand the allure of dehydrated food.  Is this something I should be hip to?\"),\n",
              " (990, 'No, Daddy.'),\n",
              " (991, 'So tell me about this dance. Was it fun?'),\n",
              " (992, 'Parts of it.'),\n",
              " (993, 'Which parts?'),\n",
              " (994, 'The part where Bianca beat the hell out of some guy.'),\n",
              " (995, 'Bianca did what?'),\n",
              " (996, \"What's the matter?  Upset that I rubbed off on her?\"),\n",
              " (997, 'No -- impressed.'),\n",
              " (998,\n",
              "  \"You know, fathers don't like to admit that their daughters are capable of running their own lives.  It means we've become spectators.  Bianca still lets me play a few innings.  You've had me on the bleachers for years.  When you go to Sarah Lawrence, I won't even be able to watch the game.\"),\n",
              " (999, 'When I go?'),\n",
              " (1000,\n",
              "  \"Oh, Christ.  Don't tell me you've changed your mind.  I already sent 'em a check.\"),\n",
              " (1007, 'Let go!'),\n",
              " (1008, 'You set me up.'),\n",
              " (1009, 'I just wanted --'),\n",
              " (1010,\n",
              "  'What? To completely damage me?  To send me to therapy forever? What?'),\n",
              " (1011, 'No! I just wanted'),\n",
              " (1017,\n",
              "  'Am I supposed to feel better? Like, right now?  Or do I have some time to think about it?'),\n",
              " (1018, 'Just smack her now.'),\n",
              " (1021, 'Is that woman a complete fruit-loop or is it just me?'),\n",
              " (1022, \"It's just you.\"),\n",
              " (1031, 'A Fender Strat. You bought this?'),\n",
              " (1032, 'I thought you could use it. When you start your band.'),\n",
              " (1033,\n",
              "  'Besides, I had some extra cash. Some asshole paid me to take out a really great girl.'),\n",
              " (1034, 'Is that right?'),\n",
              " (1035, 'Yeah, but then I fucked up. I fell for her.'),\n",
              " (1040, 'Why is my veggie burger the only burnt object on this grill?'),\n",
              " (1041, 'Because I like to torture you.'),\n",
              " (1042, 'Oh, Bianca?  Can you get me my freshman yearbook?'),\n",
              " (1043, \"Don ' t you even dare. . .\"),\n",
              " (1044, 'They do to!'),\n",
              " (1045, 'They do not!'),\n",
              " (1051, 'Patrick -- is that- a.'),\n",
              " (1052, 'Perm?'),\n",
              " (1930, 'Is that the man I knew, Treasurer Sanchez?'),\n",
              " (1931, 'Yes, Your Majesty.'),\n",
              " (1942,\n",
              "  'Diego is a bright boy -- a pleasure to teach -- but so serious... Brothers should be raised together, Colon.  Even brothers from different mothers...'),\n",
              " (1943,\n",
              "  'Father, I am doing what I think is the best for him.  And he has the teacher I would have chosen for myself.'),\n",
              " (1947, \"God... That's in a week!\"),\n",
              " (1948, \"That's what it says.\"),\n",
              " (1949, 'How did you manage it?'),\n",
              " (1950,\n",
              "  'With some difficulty.  I had to promise them you were not a total fool.'),\n",
              " (1951, 'Why do you wish to sail west?'),\n",
              " (1952,\n",
              "  'To open a new route to Asia.  At the moment there are only two ways of reaching it...'),\n",
              " (1957, 'How can you be so certain?  The Ocean is said to be infinite.'),\n",
              " (1958,\n",
              "  'Ignorance!  I believe the Indies are no more than 750 leagues west of the Canary Islands.'),\n",
              " (1959, 'How can you be so certain?'),\n",
              " (1960, 'The calculations of Toscanelli Marin de Tyr, Esdras...'),\n",
              " (1961, 'Esdras is a Jew.'),\n",
              " (1962, 'So was Christ!'),\n",
              " (1963,\n",
              "  \"Two minutes... and already you're a dead man.  Don't let passion overwhelm you, Colon.\"),\n",
              " (1964, \"I'll try to remember that, Marchena...\"),\n",
              " (1965, 'Father Marchena!'),\n",
              " (1966, 'Passion is something one cannot control!'),\n",
              " (1967, 'You get so carried away when you are being contradicted!'),\n",
              " (1968, \"I've been contradicted all my life... Eternity!\"),\n",
              " (1969, 'Only God knows the meaning of such words, my son.'),\n",
              " (1979, 'I could be gone for years.'),\n",
              " (1980, 'I know.'),\n",
              " (1981, \"I haven't given you much of a life.\"),\n",
              " (1982,\n",
              "  \"Well... that's true.  I have a child by a man who won't marry me!  Who's always leaving...\"),\n",
              " (1983, 'Are we going to argue?'),\n",
              " (1984, \"I'd love to argue with you sometimes.  But you're never here!\"),\n",
              " (1985, 'Perhaps I was never meant to live with a woman...'),\n",
              " (1986, 'I find that hard to believe.'),\n",
              " (1989, 'You say Asia can be found by sailing west?'),\n",
              " (1990,\n",
              "  'Yes, your Eminence.  The voyage should not take more than six or seven weeks.'),\n",
              " (1991,\n",
              "  'Unfortunately, Don Colon, that is precisely where our opinions differ...  Are you familiar with the work of Aristotle?  Erathostene?  Ptolemeus?'),\n",
              " (1992, 'I am, Your Eminence'),\n",
              " (1993,\n",
              "  'Then you cannot ignore that according to their calculations, the circumference of the Earth is approximately...  22,000 leagues or more.  Which makes the ocean... uncrossable.'),\n",
              " (2005,\n",
              "  'Senor Colon, an experienced captain such as yourself will understand our concern with the crew.  I am not willing to have on my conscience the loss of men who would have relied upon our judgment.'),\n",
              " (2006, 'Excellency, you are right.'),\n",
              " (2010,\n",
              "  'Your Eminence, there is only one way to settle the matter.  And that is to make the journey.  I am ready to risk my life to prove it possible.'),\n",
              " (2011, 'Your life, and that of others!'),\n",
              " (2012, 'If they agree to follow me, yes.'),\n",
              " (2014,\n",
              "  'Trade, Your Excellency.  According to Marco Polo, the Kingdom of China is one of the richest of the world. Even the meanest buildings are roofed with gold.'),\n",
              " (2015, 'Is that all that interests you? Gold?'),\n",
              " (2016,\n",
              "  'No.  The Portuguese have already discovered black-skinned people.  I, too, will find other populations -- and bring them to the word of God.'),\n",
              " (2019,\n",
              "  'If God intended our proximity to Asia, do you believe he would have waited for you to show it to the world?'),\n",
              " (2020, \"Did He not choose a carpenter's son to reveal Himself to the world?\"),\n",
              " (2022, \"Don't you realize your words could be considered heretical?\"),\n",
              " (2023, 'Blind faith is what I consider heresy!'),\n",
              " (2024, 'Asia can be found to the west -- and I will prove it.'),\n",
              " (2025, 'IF-GOD-WILLS-IT!'),\n",
              " (2027,\n",
              "  \"The State has some reason to be interested in this man's proposition, Your Eminence...\"),\n",
              " (2028, 'The Judgment is ours!'),\n",
              " (2029,\n",
              "  'Naturally.  But I would really deplore the loss of such a potential opportunity for Spain for a... dispute over a point of geography.'),\n",
              " (2030,\n",
              "  'He is a mercenary!  Did he not already try to convince the King of Portugal of his absurd notions?'),\n",
              " (2031,\n",
              "  'Indeed.  The world is full of mercenaries -- and states often make use of them, when it benefits them.  My only concern is the welfare and prosperity of Spain.'),\n",
              " (2037, \"You mustn't give way to despair. You must wait.\"),\n",
              " (2038,\n",
              "  \"Wait!  I've waited seven years already!  How much longer do you want me to wait?\"),\n",
              " (2039, 'If God intends you to go, then you will go.'),\n",
              " (2040, 'Damn God!'),\n",
              " (2041, 'Colon!'),\n",
              " (2042,\n",
              "  'Damn all of you!  You all set up theories based on what?  You never leave the safety of your studies! Go out!  Find out what the world is about and then tell me something I can listen to!'),\n",
              " (2045, 'All of them!  Just lies!'),\n",
              " (2046, \"Colon!  Don't!\"),\n",
              " (2064, 'Where can I meet this man?'),\n",
              " (2065, 'Immediately.'),\n",
              " (2071,\n",
              "  'I should not even be listening to you, since my council said no.  But Santangel tells me you are a man of honor and sincerity... And Sanchez, that you are not a fool.'),\n",
              " (2072,\n",
              "  'No more than the woman who said she would take Granada from the Moors.'),\n",
              " (2073, 'The ocean is uncrossable?'),\n",
              " (2074, 'What did they say about Granada before today?'),\n",
              " (2075, 'That she was impregnable.'),\n",
              " (2076, 'I cannot ignore the verdict of my council.'),\n",
              " (2077, 'Surely you can do anything you want.'),\n",
              " (2079, 'May I speak freely?'),\n",
              " (2080, 'You show no inclination to speak otherwise!'),\n",
              " (2081,\n",
              "  'I know what I see.  I see someone who doesn\\'t accept the world as it is.  Who\\'s not afraid.  I see a women who thinks... \"What if?\"...'),\n",
              " (2082, 'A woman?'),\n",
              " (2085, 'How old are you, Senor Colon?'),\n",
              " (2086, 'Thirty seven, Your Majesty... And you?'),\n",
              " (2103, 'No...'),\n",
              " (2104, 'No?'),\n",
              " (2105,\n",
              "  'NO...!  I have waited too long, fought too hard.  Now you expect me to take all the risks while you take the profit!  No... I will not be your servant!'),\n",
              " (2106,\n",
              "  'I remind you, Senor Colon, that you are in no position to bargain with me.'),\n",
              " (2107, \"I'm not bargaining!\"),\n",
              " (2108, 'Then you are too ambitious.'),\n",
              " (2109,\n",
              "  'And were you never ambitious, Excellency?  Or is ambition only a virtue among the nobles, a fault for the rest of us?'),\n",
              " (2110,\n",
              "  \"If you won't accept our proposal, we'll simply find someone who will.\"),\n",
              " (2112, 'You were right, Don Sanchez... His demands could never be granted.'),\n",
              " (2113, 'Never, Your Majesty.  Although...'),\n",
              " (2115, '... Into a monk...'),\n",
              " (2116, \"Yes.  It would be a pity, wouldn't it?  Call him back!\"),\n",
              " (2118, 'She said yes.'),\n",
              " (2119, 'Thank God...'),\n",
              " (2121, \"I'm not asking you to swear to anything.\"),\n",
              " (2122, \"I don't want you to wait for me.\"),\n",
              " (2123, \"That's something you can't decide.\"),\n",
              " (2127, 'I want to go with you!'),\n",
              " (2128, \"There'll be a time.\"),\n",
              " (2129, 'You promise?  Do you swear on St. Christopher...?'),\n",
              " (2130, 'Do you swear on all the Holy Saints in heaven?'),\n",
              " (2131, 'Yes... Yes, I do... On all of them!'),\n",
              " (2132, 'In Nomine Patris et Filius, et Spiritus Sancti.'),\n",
              " (2133, 'Forgive me, Father.  For I have sinned.'),\n",
              " (2134, 'I am listening, my son.'),\n",
              " (2135,\n",
              "  'Father, I have betrayed my family. I betrayed my men.  And I betrayed you.'),\n",
              " (2136, 'What are you saying?'),\n",
              " (2137, 'I lied.  The journey will be longer than I said.'),\n",
              " (2138, 'How long?'),\n",
              " (2139, 'I am not sure... It could be twice the distance.'),\n",
              " (2140,\n",
              "  'May God forgive you...!  You must tell them!  You must tell your men!'),\n",
              " (2141,\n",
              "  \"If I tell them, they won't follow me.  You know that I am right, Father.  You trust me...\"),\n",
              " (2142,\n",
              "  \"My son, my son...  Your certitudes are sometimes frightening...  Christopher, you must speak to them. And if you don't I will.\"),\n",
              " (2143, 'You are bound by an oath, Father.'),\n",
              " (2144, 'I believed in you...'),\n",
              " (2145, 'Give me absolution.'),\n",
              " (2154, 'Due west, Captain Mendez.  And may God be with us...'),\n",
              " (2155, 'God be with us admiral.'),\n",
              " (2157,\n",
              "  \"Well... It's the men, Sir.  They wonder how you know our position. We've lost sight from land days ago...\"),\n",
              " (2158, 'And what do you think Mendez?'),\n",
              " (2159,\n",
              "  \"Well, I surely know what a quadrant is!  But I've never seen it used at night before.\"),\n",
              " (2160, 'Come over here.'),\n",
              " (2164, 'What do you read?'),\n",
              " (2165, 'Twenty eight.'),\n",
              " (2170, 'I never seen heat like this!  Not even in Las Minas!'),\n",
              " (2171, \"The water's going putrid in the barrels.\"),\n",
              " (2172,\n",
              "  \"You'll be drinking your own piss... For the glory of Spain... and Admiral Colon...!  Bastard!\"),\n",
              " (2173, 'What are you listening to, chicken ass?'),\n",
              " (2174, \"Ah, leave him alone.  He's doing no harm.\"),\n",
              " (2175, \"With a face like that?  I don't want you looking at me.  You hear?\"),\n",
              " (2176, \"He's the devil's child...\"),\n",
              " (2177, \"We'll all go crazy...\"),\n",
              " (2179, 'We should have seen land.'),\n",
              " (2180, \"We left three weeks ago, Alonso. Can't be that near.\"),\n",
              " (2181,\n",
              "  \"Can't be that far, I say.  Also, I don't like the smell of the sea around here.  Smells like a cunt. Bad sign...\"),\n",
              " (2196, \"You lied!  You cheated!  We're way past 750 leagues!\"),\n",
              " (2197, 'Six days ago, yes.'),\n",
              " (2198, 'You must be mad...!'),\n",
              " (2199, 'We have to keep the hopes of these men alive!'),\n",
              " (2200, \"We're on the verge of a mutiny, Colon!\"),\n",
              " (2201, \"You think I don't know that?\"),\n",
              " (2202, \"We're lost!\"),\n",
              " (2203, 'The land is there.  I know it!'),\n",
              " (2204,\n",
              "  \"You don't know anything!  Listen Colon, these are my ships, right? So I'm telling you we're turning back!\"),\n",
              " (2205,\n",
              "  'And then what?  Half of the water has gone, the rest is nearly putrid! You know that!'),\n",
              " (2206, 'Jesus Maria!  I should have never listened to you!'),\n",
              " (2207, 'You never did.  You did all the talking for both of us, remember?'),\n",
              " (2208, 'You bloody...'),\n",
              " (2209,\n",
              "  'Pinzon, Pinzon... All we can do now is go forward!  Think about that!'),\n",
              " (2210, 'You tell that to them!'),\n",
              " (2211, \"You're right.  Let the men decide.\"),\n",
              " (2237, 'Say not here!  Cuba!'),\n",
              " (2238, 'What is it?  A tribe?  An island?'),\n",
              " (2239, 'Island.  Far.'),\n",
              " (2248, 'You come!  You speak first!'),\n",
              " (2249, 'Tell the Chief we thank him.'),\n",
              " (2250, 'Chief knows.'),\n",
              " (2251,\n",
              "  'Tell him his country is very beautiful.  Tell him we are leaving men here -- to build a fort.'),\n",
              " (2253, 'Chief says -- how many?'),\n",
              " (2254, 'Thousands.'),\n",
              " (2255, 'Why?'),\n",
              " (2256, 'To bring the word of God.'),\n",
              " (2257, 'Chief says -- he has a God.'),\n",
              " (2258, '... and also to bring medicine.'),\n",
              " (2259, 'Chief says...'),\n",
              " (2260, 'He has medicine.  Tell him we admire his people.'),\n",
              " (2274, \"It won't be easy to get rid of your prophet now, Don Sanchez.\"),\n",
              " (2275,\n",
              "  'On the contrary, Your Eminence.  It seems to me the man is preparing his own cross.'),\n",
              " (2277,\n",
              "  \"And you say this is an Indian vice? By God!  I don't see any kind of pleasure that would make this a sin.\"),\n",
              " (2278, 'The Indians have no such word, Don Moxica.'),\n",
              " (2279, 'Do they have such thoughts?'),\n",
              " (2280, 'They come and go as naked as the day God created them...'),\n",
              " (2281,\n",
              "  \"They don't see sin in their nakedness.  They live according to nature, in a never ending summer. The islands are covered with trees, filled with blossoms and fruits. And...\"),\n",
              " (2282, 'Forgive me, Don Colon.  But what about gold?'),\n",
              " (2286, 'You defend yourself admirably...'),\n",
              " (2287, '... for a commoner?'),\n",
              " (2290,\n",
              "  'I understand that you will soon be appointing Governors for the islands?  Is it not so?'),\n",
              " (2291,\n",
              "  'Forgive me, Don Bobadilla -- those positions have already been taken.'),\n",
              " (2292, 'May I ask by whom?'),\n",
              " (2293, 'Bartolome and Giacomo Colon.'),\n",
              " (2295,\n",
              "  'But we do have a lack of notaries. You should contact my administration.'),\n",
              " (2296, 'Don Bobadilla is already a judge, my Dear Don Cristobal.'),\n",
              " (2297, 'Good!  We are also in need of judges.  Except there are no thieves!'),\n",
              " (2299, 'You seem to have a special talent for making friends.'),\n",
              " (2300, 'What...?  Do I have so many already?'),\n",
              " (2301,\n",
              "  'To rise so high, in so short a time, is a dangerous occupation.  A little hypocrisy goes a long way.'),\n",
              " (2318, 'Beatrix, I want to ask you something.'),\n",
              " (2319, \"You don't usually ask.\"),\n",
              " (2320,\n",
              "  'I can arrange for the Queen to take Fernando and Diego into her service.'),\n",
              " (2328, 'We lost cousins, friends.  We will wash this in blood.'),\n",
              " (2329,\n",
              "  \"If you want to keep your head on your shoulders, you'll do as I say.\"),\n",
              " (2331,\n",
              "  'You want a war?  Fine.  We are a thousand.  They outnumber us by ten! Who will you kill?  Which tribe?'),\n",
              " (2332, \"We don't need to know.\"),\n",
              " (2333,\n",
              "  'We came here to stay!  To build! Not to start a crusade.  In this forest, there is enough danger to sweep us away in days!  So we will be brave and swallow our grief.  And in the name of those who died, we will accomplish what we came for.'),\n",
              " (2341,\n",
              "  'We will work with his people.  We want peace.  Ask the Chief if he understands?'),\n",
              " (2342, 'He understands.'),\n",
              " (2343, 'Ask him if he will help.'),\n",
              " (2345, \"We can't raise the wheel without it.\"),\n",
              " (2346, \"My horse doesn't work.\"),\n",
              " (2347, 'Don Moxica -- we all have to work.'),\n",
              " (2348, 'You did not hear me, Don Colon.  Not my horse.'),\n",
              " (2376,\n",
              "  'Every ship returns with a cargo of sick and dying.  But with no gold! The new world proves expensive, Your Majesty.'),\n",
              " (2377,\n",
              "  \"We weren't expecting immediate profits, were we?  We must have faith.  We must give time for time.\"),\n",
              " (2386,\n",
              "  'In one act of brutality, you have created chaos.  Tribes who were fighting each other are now joining forces against us!  All that because of your criminal savagery!'),\n",
              " (2387, 'Savagery is what monkeys understand.'),\n",
              " (2388,\n",
              "  \"You'll be held in detention, deprived of your privileges and possessions.  Until you are returned to Spain where you will be judged. Have you anything to say?\"),\n",
              " (2389, 'You will regret this.'),\n",
              " (2393, 'You have to find them, Utapan.  Look what they did!'),\n",
              " (2394, 'You did the same to your God!'),\n",
              " (2409,\n",
              "  \"Utapan, won't you speak to me?  You used to know how to speak to me.\"),\n",
              " (2410, 'You never learned how to speak my language.'),\n",
              " (2414,\n",
              "  '... But there is worse.  He ordered the execution of five members of the nobility...'),\n",
              " (2415, 'Is this true, Brother Buyl?'),\n",
              " (2417, 'Then, what do you suggest, Don Sanchez?'),\n",
              " (2418, 'He must be replaced.'),\n",
              " (2419, 'And who would you think of, for such a task?'),\n",
              " (2421, 'Don Alonso de Bobadilla.'),\n",
              " (2422, 'Yes... I remember...'),\n",
              " (2423, 'My letters of appointment.'),\n",
              " (2424, 'Appointment to what?'),\n",
              " (2425, 'Viceroy of the West Indies.'),\n",
              " (2426, 'Congratulations.  Then I am free to search for the mainland.'),\n",
              " (2429, 'How far from here?'),\n",
              " (2430,\n",
              "  'I am not a seaman.  But I heard it is no more than a week at sea.  I hope you are not too disappointed.'),\n",
              " (2431,\n",
              "  'How could I be?  The mainland has been found.  Exactly as I said it would.'),\n",
              " (2432, 'I am afraid this is not the worst news.'),\n",
              " (2439, 'I have to explore the mainland.'),\n",
              " (2440, 'This time with me!'),\n",
              " (2456,\n",
              "  'But without your brothers.  Nor are you to return to Santo Domingo or any of the other colonies.  You may explore the continent.'),\n",
              " (2457, 'Thank you.'),\n",
              " (2458,\n",
              "  \"There is one thing I'd like to understand... Why do you want to go back, after all this?\"),\n",
              " (2459,\n",
              "  'Your Majesty -- some men are content to read about things.  I must see them with my own eyes.  I cannot be other than I am.'),\n",
              " (2460, 'I know, I should not tolerate his impertinence.'),\n",
              " (2461, 'Then why?'),\n",
              " (2462, 'Because he is not afraid of me.'),\n",
              " (2463, 'All I have to do is call the guards.'),\n",
              " (2464, 'Call them.'),\n",
              " (2465, 'I am not afraid of you.  You are nothing but a dreamer.'),\n",
              " (2466, 'Look out of that window.'),\n",
              " (2467, 'What do you see?'),\n",
              " (2468, 'Roofs... towers, palaces... spires...'),\n",
              " (2469, 'All of them created by people like me.'),\n",
              " (2471,\n",
              "  \"God... you're so beautiful!  I can't believe no other man has ever taken you away from me...\"),\n",
              " (2472, \"They tried... but I didn't let them.\"),\n",
              " (2473, 'They took everything...'),\n",
              " (2474,\n",
              "  \"Not everything... Do you think I care?  I'm a free man again.  Riches don't make a man rich, they only make him busier...\"),\n",
              " (2484, 'How are you feeling, Fernando?'),\n",
              " (2485, 'Not bad.'),\n",
              " (2490, \"What's he doing?\"),\n",
              " (2491, \"He's drawing an isthmus... He's saying we're on an isthmus.\"),\n",
              " (2492, \"We can't be.\"),\n",
              " (2500, 'Father...'),\n",
              " (2501, 'There must be a passage to that other ocean.'),\n",
              " (2522, 'You can see for yourself.'),\n",
              " (2523, 'What a tragedy... what a waste of a life...'),\n",
              " (2524,\n",
              "  'A waste...?  Let me tell you something, Arojaz.  If your name, or mine, is ever remembered -- it will only be because of his.'),\n",
              " (2525, \"I suppose we're both old men now.\"),\n",
              " (2526, \"You'll always be older than me, Father.\"),\n",
              " (2531, 'I have to disagree.'),\n",
              " (2532, 'I knew you would.'),\n",
              " (2533, 'New worlds create new people.'),\n",
              " (2534, 'Oh?  So you are a new man?'),\n",
              " (2535,\n",
              "  \"I don't know... I have the impression that I didn't change that much.  I still can't accept the world as it is!\"),\n",
              " (2537, \"Can't you stay with us a little?\"),\n",
              " (2538, 'I am busy inside.'),\n",
              " (2539, 'What is it, now?  Tell me...'),\n",
              " (2540,\n",
              "  \"I can't keep my eyes off you.  I would like to catch up with all the moments I didn't spend with you.\"),\n",
              " (2541, 'What are you listening to?'),\n",
              " (2542, \"I am not listening, Father.  But I can't help hearing.\"),\n",
              " (2547, 'What does he say?'),\n",
              " (2548, 'He asks when he can come to visit you.  He left his address.'),\n",
              " (2549, 'He never had one... except aboard my ships!'),\n",
              " (2550,\n",
              "  'I want you to tell me everything you remember, Father.  From the beginning.  Everything.'),\n",
              " (2551, \"Really?  God... I wouldn't know where to start... and yet...\"),\n",
              " (2552, 'Tell me the first thing that comes to your mind.'),\n",
              " (2557,\n",
              "  \"Just do what I do.  Say the same thing I say.  Don't open your mouth.\"),\n",
              " (2558, 'Okay.'),\n",
              " (2559, \"Don't fool around.\"),\n",
              " (2560, 'Okay.'),\n",
              " (2561, 'Did you hear what I said?'),\n",
              " (2562, 'I want to document my trip to America.'),\n",
              " (2563, 'Next.  Could I see your documents, please?'),\n",
              " (2564, 'Yes sir.'),\n",
              " (2565, 'What is your intended purpose of your visit to the United States?'),\n",
              " (2566, 'Two weeks holiday.'),\n",
              " (2567, 'How much money are you carrying with you?'),\n",
              " (2568, 'I have five-hundred dollars.'),\n",
              " (2569, 'Can you show me?  Sir, no cameras in the FIS area!'),\n",
              " (2570, 'Is he with you?  Are you travelling together?'),\n",
              " (2571, 'Yes.'),\n",
              " (2572, 'Please join us.  Come on forward.'),\n",
              " (2573, 'Is there a problem?'),\n",
              " (2574,\n",
              "  \"No, you're travelling together.  I want to talk to you together.  Hi, how are you?  Can I take a look at your documents?  Are you related?\"),\n",
              " (2575, \"Yes...he's my friend.\"),\n",
              " (2576,\n",
              "  \"Okay.  You're a Czech national and you're a Russian national.  How do you know one another?\"),\n",
              " (2577, 'We are both from Prague.'),\n",
              " (2578, 'How long are you planning to stay?'),\n",
              " (2579, 'Two weeks.'),\n",
              " (2580, \"I'd like to speak for himself, okay?\"),\n",
              " (2581, \"He doesn't speak English.\"),\n",
              " (2582, 'I speak English.'),\n",
              " (2583,\n",
              "  \"Then answer my questions.  Where were you planning to stay during the two weeks that you're here?\"),\n",
              " (2584, 'New York.'),\n",
              " (2585,\n",
              "  \"Yes, we're in New York now.  But where are you planning to stay in New York?\"),\n",
              " (2586, 'A cheap hotel.'),\n",
              " (2587, 'What are you coming here to do?'),\n",
              " (2588, \"I'm here for movies.\"),\n",
              " (2589, 'Movies...to be in the movies or to see movies?'),\n",
              " (2590,\n",
              "  'Yes.  No.  Both.  When I was a boy, I see movie at school called \"It\\'s a Wonderful Life\" directed by Frank Capra. Ever since I want to come to America. Land of the free.  Home of the brave.  A land where anyone can be anything. As long as they are white.'),\n",
              " (2591, 'Excuse me?'),\n",
              " (2595,\n",
              "  \"So we're waitin' to hit this warrant - we got Emergency Service with the heavy weapons standin' by - ready to go.  I say, lemme get a cigar outta the car.  I go to get the cigar and BOOM!  All the sudden I turn around and a kid with a shotgun let one go.  Right where I was standin'.  That coulda been it.  I coulda had my head blown off and for what?  Some stupid kid got panicky, takes the safety off and it's over.  If I hadn't gone back for that cigar - for a bad habit - I would've had my head blown off.\"),\n",
              " (2596, 'Jesus Christ.'),\n",
              " (2599, 'Coffee for me, I gotta slow down.'),\n",
              " (2600, 'Vodka tonic.'),\n",
              " (2601, 'Maybe you could just put in a shot of Martell?'),\n",
              " (2602, \"It was freaky, I'll tell you.  Stupid kid.\"),\n",
              " (2603,\n",
              "  \"What's the kid gonna say - sorry? Meanwhile I'm not here anymore. Like last week - we were at the morgue and this guy was all chopped up - spleen here - liver there - his heart in a pan. Six hours ago this guy was walkin' his dog or buyin' a quart of milk.  Who knows?  But some kid's robbed him for $3 or some shit and shot him and now you can't tell if he's a piece of beef or a human being and I'm thinkin' that's me. Sooner or later.  That's me.\"),\n",
              " (2604, \"Sooner or later that's everybody.\"),\n",
              " (2605,\n",
              "  'Not chopped up.  Not chopped up like that.  I mean, what do I got left? Coupla articles.  A medal or two. Plaque here and there and in a coupla years no one remembers me anymore.'),\n",
              " (2606, \"I think you're getting a little moody there, Eddie.\"),\n",
              " (2607, \"I'm not moody.\"),\n",
              " (2608, \"Isn't he a little moody?\"),\n",
              " (2609, \"Of course he's moody.  He thinks he's in love.\"),\n",
              " (2610, 'In love?  With who?'),\n",
              " (2611, 'How old are your kids?'),\n",
              " (2612,\n",
              "  \"My kids?  Let's see...Susan's 15. Aundrea's 9.  Don't tell me you're thinking about having a kid!  How old are you?  Never mind. Let me just tell you this: Every stupid cliche you hear about kids - they change your life, they make you a better person, they make you whole...  It's all true!  Before I had kids when friends talked about their kids, I wanted to vomit.  Now -- I get it.  Am I right, Leon?\"),\n",
              " (2614, 'Yeah?'),\n",
              " (2615, \"Paulie, you've got kids, right?\"),\n",
              " (2621,\n",
              "  'Okay.  You work in a vodka factory.  I understand that.  And what kind of work do you do?'),\n",
              " (2622, 'I am butcher.'),\n",
              " (2623, \"You're a butcher?  What do you use pig intestines for?\"),\n",
              " (2624, 'You stuff sausage in it.'),\n",
              " (2625, 'And what do you do with the bones?'),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMW6dIHFM03w"
      },
      "source": [
        "conves_dict = {}\n",
        "counter = 1\n",
        "conves_ids = []\n",
        "for i in range(1, len(sorted_chats)+1):\n",
        "    if i < len(sorted_chats):\n",
        "        if (sorted_chats[i][0] - sorted_chats[i-1][0]) == 1:\n",
        "            if sorted_chats[i-1][1] not in conves_ids:\n",
        "                conves_ids.append(sorted_chats[i-1][1])\n",
        "            conves_ids.append(sorted_chats[i][1])\n",
        "        elif (sorted_chats[i][0] - sorted_chats[i-1][0]) > 1:            \n",
        "            conves_dict[counter] = conves_ids\n",
        "            conves_ids = []\n",
        "        counter += 1\n",
        "    else:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hL5pxIpM3ZQ"
      },
      "source": [
        "context_and_target = []\n",
        "#dicarding dialogue without a reply\n",
        "for conves in conves_dict.values():\n",
        "    if len(conves) % 2 != 0:\n",
        "        conves = conves[:-1]\n",
        "    for i in range(0, len(conves), 2):\n",
        "        context_and_target.append((conves[i], conves[i+1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_06Mm376M5AA"
      },
      "source": [
        "context, target = zip(*context_and_target)\n",
        "context = list(context)\n",
        "target = list(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeiERBhM6g_"
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    # Clean text by removing unnecessary characters and altering the format of words\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3rrYlHJNJLO"
      },
      "source": [
        "tidy_context = []\n",
        "for conve in context:\n",
        "    text = clean_text(conve)\n",
        "    tidy_context.append(text)\n",
        "tidy_target = []\n",
        "for conve in target:\n",
        "    text = clean_text(conve)\n",
        "    tidy_target.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFbWLr9iNMQ-"
      },
      "source": [
        "length_list=[]\n",
        "for l in tidy_context:\n",
        "    length_list.append(len(l.split(' ')))\n",
        "for l in tidy_target:\n",
        "    length_list.append(len(l.split(' ')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAXbB_MsNN7_",
        "outputId": "b3383aae-ad6c-4ebc-8965-0ece5ac0d79b"
      },
      "source": [
        "lengths = pd.DataFrame(length_list, columns=['counts'])\n",
        "lengths['counts'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    287728.000000\n",
              "mean         11.307634\n",
              "std          13.115849\n",
              "min           1.000000\n",
              "25%           4.000000\n",
              "50%           7.000000\n",
              "75%          14.000000\n",
              "max        1128.000000\n",
              "Name: counts, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcXW0A6bNPxP",
        "outputId": "08cd0642-4259-457a-d1c6-e9d97c4d3037"
      },
      "source": [
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))\n",
        "print(np.percentile(lengths, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n",
            "20.0\n",
            "25.0\n",
            "34.0\n",
            "61.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q647p_MYNR6_",
        "outputId": "254e7b88-aef9-4048-c53f-a2fa4d95724a"
      },
      "source": [
        "len(tidy_context)==len(tidy_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0m6gTHVNWG-",
        "outputId": "b74b3274-3912-4973-c60e-fc7ea2e6365e"
      },
      "source": [
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(tidy_context):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(tidy_target[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(tidy_target):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(tidy_context[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113507\n",
            "113507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbusgA-6NZau",
        "outputId": "0de6b592-f43b-4c36-ea1d-515fb3dfbfd5"
      },
      "source": [
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i mean a parking lot\n",
            "yah well seven hundred and fifty thousand dollars is a lot  ha ha ha\n",
            "\n",
            "well you know stan will say no dice that is why you pay him  i am asking you here wade this could work out real good for me and jean and scotty \n",
            "jean and scotty never have to worry\n",
            "\n",
            "come on man okay here's an idea we will stop outside of brainerd i know a place there we can get laid wuddya think\n",
            "i am fucking hungry now you know\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrkqQ6lxNctA"
      },
      "source": [
        "#choosing number of samples\n",
        "num_samples = 30000  # Number of samples to train on.\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9gfWznMNhBu"
      },
      "source": [
        "#append start and end tokens for the answers\n",
        "short_answers2 = []\n",
        "for ans in short_answers:\n",
        "    ans = '<SOS> ' + ans + ' <EOS>'\n",
        "    short_answers2.append(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4obhdbUJNjBu"
      },
      "source": [
        "# Create a dictionary for the frequency of the vocabulary\n",
        "vocab = {}\n",
        "for question in short_questions:\n",
        "    for word in question.split():\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "            \n",
        "for answer in short_answers2:\n",
        "    for word in answer.split():\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mMpEziuNugP"
      },
      "source": [
        "# Remove rare words from the vocabulary.\n",
        "# We will aim to replace fewer than 5% of words with <UNK>\n",
        "# You will see this ratio soon.\n",
        "threshold = 20\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_gU5fUHNwb-",
        "outputId": "887d8881-a75f-4d0e-ed78-d30f2adc3b99"
      },
      "source": [
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 23909\n",
            "Size of vocab we will use: 1788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7TkWz7ANyke"
      },
      "source": [
        "#we will create dictionaries to provide a unique integer for each word.\n",
        "vocab_to_int = {}\n",
        "\n",
        "word_num = 0\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold:\n",
        "        vocab_to_int[word] = word_num\n",
        "        word_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEzupy2iN0ve",
        "outputId": "104dc765-fcd8-45bc-f019-41503cdab4a1"
      },
      "source": [
        "len(vocab_to_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMSd98MvN2V9"
      },
      "source": [
        "# Add the unique tokens (pad and unknown vocab) to the vocabulary dictionaries.\n",
        "codes = ['<PAD>','<UNK>']\n",
        "for code in codes:\n",
        "    code_int = len(vocab_to_int)\n",
        "    vocab_to_int[code] = code_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaziBA4AN4Wu",
        "outputId": "21f8879f-4bfc-4c95-bc35-d1dbd0c7fa79"
      },
      "source": [
        "#switch <PAD> value to well's value of 0 for padding purposes later\n",
        "print(vocab_to_int['<PAD>'])\n",
        "for i, v in vocab_to_int.items():\n",
        "    if v == 0:\n",
        "        print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1788\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyFnq7DoN6X-"
      },
      "source": [
        "for i, v in vocab_to_int.items():\n",
        "    if v == 0:\n",
        "        vocab_to_int[i] = vocab_to_int['<PAD>']\n",
        "vocab_to_int['<PAD>'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tixAlA2pN_F-",
        "outputId": "a5088fef-97a7-4c38-d640-ee05685d0849"
      },
      "source": [
        "len(vocab_to_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1790"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Jc6tnuJUOBuN",
        "outputId": "273a9cdc-ab6f-41c5-db56-6031c82e8cf7"
      },
      "source": [
        "faq=pd.read_csv('/content/drive/MyDrive/faq.csv')\n",
        "faq.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Are session pre-class quizzes graded?</td>\n",
              "      <td>No. Pre-class quiz is to just check your under...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When is the deadline for the submission of ses...</td>\n",
              "      <td>5 PM on the day of the following lecture.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Will there be any extension allowed for the qu...</td>\n",
              "      <td>Only for exceptional cases.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours will I need to dedicate to succ...</td>\n",
              "      <td>About 15 hours per week.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade my exercise?</td>\n",
              "      <td>The exercises are auto-graded once you click t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0              Are session pre-class quizzes graded?  No. Pre-class quiz is to just check your under...\n",
              "1  When is the deadline for the submission of ses...          5 PM on the day of the following lecture.\n",
              "2  Will there be any extension allowed for the qu...                        Only for exceptional cases.\n",
              "3  How many hours will I need to dedicate to succ...                           About 15 hours per week.\n",
              "4                        Who will grade my exercise?  The exercises are auto-graded once you click t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0QOTAbMOFF8"
      },
      "source": [
        "tidy_faq_ques = []\n",
        "for conve in faq.Question:\n",
        "    text = clean_text(conve)\n",
        "    tidy_faq_ques.append(text)\n",
        "tidy_faq_ans = []\n",
        "for conve in faq.Answer:\n",
        "    text = clean_text(conve)\n",
        "    tidy_faq_ans.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERW8f8X9OKte",
        "outputId": "a316ec6d-5262-4c38-89ac-55d9c664575d"
      },
      "source": [
        "length_list_faq=[]\n",
        "for l in tidy_faq_ques :\n",
        "    length_list_faq.append(len(l.split(' ')))\n",
        "for l in tidy_faq_ans:\n",
        "    length_list_faq.append(len(l.split(' ')))\n",
        "lengths_faq = pd.DataFrame(length_list_faq, columns=['counts'])\n",
        "lengths_faq['counts'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    66.000000\n",
              "mean      9.878788\n",
              "std       6.505655\n",
              "min       2.000000\n",
              "25%       6.250000\n",
              "50%       9.000000\n",
              "75%      12.000000\n",
              "max      51.000000\n",
              "Name: counts, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY21HP5hONat",
        "outputId": "3960dd96-776d-482f-9e8a-9197d8a99ddb"
      },
      "source": [
        "print(np.percentile(lengths_faq, 80))\n",
        "print(np.percentile(lengths_faq, 85))\n",
        "print(np.percentile(lengths_faq, 90))\n",
        "print(np.percentile(lengths_faq, 95))\n",
        "print(np.percentile(lengths_faq, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13.0\n",
            "13.0\n",
            "15.0\n",
            "17.75\n",
            "31.49999999999983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFTNj7JOPnN"
      },
      "source": [
        "vocabs = {}\n",
        "for question in tidy_faq_ques:\n",
        "    for word in question.split():\n",
        "        if word not in vocabs:\n",
        "            vocabs[word] = 1\n",
        "        else:\n",
        "            vocabs[word] += 1\n",
        "            \n",
        "for answer in tidy_faq_ans:\n",
        "    for word in answer.split():\n",
        "        if word not in vocabs:\n",
        "            vocabs[word] = 1\n",
        "        else:\n",
        "            vocabs[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTCAf-WtOR1N"
      },
      "source": [
        "for i in vocab_to_int:\n",
        "  if i in vocabs:\n",
        "    vocabs[i]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x1tfzP-OT89"
      },
      "source": [
        "word_num=1790\n",
        "for word, count in vocabs.items():\n",
        "    if count > 0:\n",
        "        vocab_to_int[word] = word_num\n",
        "        word_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tLjAiLeOXct"
      },
      "source": [
        "# Create dictionaries to map the unique integers to their respective words.\n",
        "# i.e. an inverse dictionary for vocab_to_int.\n",
        "int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2zoYvuTOZZd",
        "outputId": "577916e5-7fbb-4f50-f98b-69864b77de20"
      },
      "source": [
        "# Check the length of the dictionaries.\n",
        "print(len(vocab_to_int))\n",
        "print(len(int_to_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1911\n",
            "1911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRxzjpr9Obu8"
      },
      "source": [
        "tidy_faq_ans2 = []\n",
        "for ans in tidy_faq_ans:\n",
        "    ans = '<SOS> ' + ans + ' <EOS>'\n",
        "    tidy_faq_ans2.append(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3JLiQVLPalo"
      },
      "source": [
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "faq_ques_final = []\n",
        "faq_ans_final = []\n",
        "faq_ans2_final = []\n",
        "\n",
        "for i, answer in enumerate(tidy_faq_ans):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        faq_ans_final.append(answer)\n",
        "        faq_ques_final.append(tidy_faq_ques[i])\n",
        "        faq_ans2_final.append(tidy_faq_ans2[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss0Z_3LKPTyM",
        "outputId": "0fc6afdf-153f-4c92-eb5d-45585ba423a1"
      },
      "source": [
        "print(len(faq_ques_final))\n",
        "print(len(faq_ans_final))\n",
        "print(len(faq_ans2_final))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "31\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvPZKVGePsXG",
        "outputId": "d8ab89e2-0e11-4c9d-b9a3-24f0de64455d"
      },
      "source": [
        "print(len(short_questions))\n",
        "print(len(short_answers))\n",
        "print(len(short_answers2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcsB5Y6nPg8Q",
        "outputId": "6f7c5c4d-c829-4de9-e5ca-bd3e284807a2"
      },
      "source": [
        "# Convert the text to integers with paddings\n",
        "# Replace any words that are not in the respective vocabulary with <UNK> \n",
        "questions_int = []\n",
        "for question in short_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in vocab_to_int:\n",
        "            ints.append(vocab_to_int['<UNK>'])\n",
        "        else:\n",
        "            ints.append(vocab_to_int[word])\n",
        "    questions_int.append(ints)\n",
        "    \n",
        "answers_int = []\n",
        "for answer in short_answers2:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in vocab_to_int:\n",
        "            ints.append(vocab_to_int['<UNK>'])\n",
        "        else:\n",
        "            ints.append(vocab_to_int[word])\n",
        "    answers_int.append(ints)\n",
        "# Check the lengths\n",
        "print(len(questions_int))\n",
        "print(len(answers_int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYpUvd2FP8aq",
        "outputId": "767d512b-21a2-4cab-f178-101b53472eb5"
      },
      "source": [
        "# Convert the text to integers with paddings\n",
        "# Replace any words that are not in the respective vocabulary with <UNK> \n",
        "questions_int_faq = []\n",
        "for question in faq_ques_final:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in vocab_to_int:\n",
        "            ints.append(vocab_to_int['<UNK>'])\n",
        "        else:\n",
        "            ints.append(vocab_to_int[word])\n",
        "    questions_int_faq.append(ints)\n",
        "    \n",
        "answers_int_faq = []\n",
        "for answer in faq_ans2_final:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in vocab_to_int:\n",
        "            ints.append(vocab_to_int['<UNK>'])\n",
        "        else:\n",
        "            ints.append(vocab_to_int[word])\n",
        "    answers_int_faq.append(ints)\n",
        "# Check the lengths\n",
        "print(len(questions_int_faq))\n",
        "print(len(answers_int_faq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoEl8yKTQY-p"
      },
      "source": [
        "#include padding\n",
        "encoder_input_data = pad_sequences(questions_int, maxlen=max_line_length, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length\n",
        "decoder_input_data = pad_sequences(answers_int, maxlen=max_line_length+2, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length + start and end tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv5nv8b0Qd0K"
      },
      "source": [
        "#include padding\n",
        "encoder_input_data_faq = pad_sequences(questions_int_faq, maxlen=max_line_length, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length\n",
        "decoder_input_data_faq = pad_sequences(answers_int_faq, maxlen=max_line_length+2, value=vocab_to_int['<PAD>'], padding='post') #pad to max_line_length + start and end tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbOV6PUSQmDp"
      },
      "source": [
        "#decoder target is 1 timestep (word) ahead of decoder input, in a 3-d array\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(answers_int), max_line_length+2, len(vocab_to_int)), #memory error occurs after 3500\n",
        "    dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbx_ikR6QpHI"
      },
      "source": [
        "#decoder target is 1 timestep (word) ahead of decoder input, in a 3-d array\n",
        "decoder_target_data_faq = np.zeros(\n",
        "    (len(answers_int_faq), max_line_length+2, len(vocab_to_int)), #memory error occurs after 3500\n",
        "    dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QT5di90QtjJ"
      },
      "source": [
        "for i, target_seq in enumerate(answers_int):\n",
        "    for t, seq in enumerate(target_seq):\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, seq] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK0dIDPJQyD7"
      },
      "source": [
        "for i, target_seq in enumerate(answers_int_faq):\n",
        "    for t, seq in enumerate(target_seq):\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data_faq[i, t - 1, seq] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSIInIP7Q17I",
        "outputId": "100f4b9f-1b15-485a-8ff1-6ae1bb52d3aa"
      },
      "source": [
        "print (encoder_input_data.shape)\n",
        "print (decoder_input_data.shape)\n",
        "print (decoder_target_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 20)\n",
            "(30000, 22)\n",
            "(30000, 22, 1911)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouQNgFVuQ374",
        "outputId": "9033d888-79e3-4990-802b-b3613765e15e"
      },
      "source": [
        "print (encoder_input_data_faq.shape)\n",
        "print (decoder_input_data_faq.shape)\n",
        "print (decoder_target_data_faq.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31, 20)\n",
            "(31, 22)\n",
            "(31, 22, 1911)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Guxk_JvQ745",
        "outputId": "89946db4-5671-4a5c-ef55-f7da2c401991"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/glove.6B.50d.txt.zip\" -d \"/content/glove\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/glove.6B.50d.txt.zip\n",
            "  inflating: /content/glove/glove.6B.50d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SAwb59qRIGI",
        "outputId": "0e2bd5e9-57db-4800-8a1e-0e7c0aa5fb97"
      },
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "with open('/content/glove/glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "print(\"Glove Loded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Loded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ79G6ybRKoI"
      },
      "source": [
        "embeddings_index['<SOS>']=np.random.rand(50)\n",
        "embeddings_index['EOS']=np.random.rand(50)\n",
        "embeddings_index['UNK']=np.random.rand(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dJEUZvARMTH"
      },
      "source": [
        "embedding_dimention = 50\n",
        "def embedding_matrix_creater(embedding_dimention, word_index):\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqzdxSHYRSen"
      },
      "source": [
        "embedding_matrix = embedding_matrix_creater(embedding_dimention, vocab_to_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW8X0r_rRUan",
        "outputId": "7d0053d1-0402-40cc-d3be-f089f92c73de"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1912, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wop3Q5ERWLt"
      },
      "source": [
        "embedding_size = 50\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "en_x=  Embedding(len(vocab_to_int)+1, embedding_size,weights=[embedding_matrix])(encoder_inputs)\n",
        "encoder = Bidirectional(LSTM(100, return_state=True))\n",
        "encoder_outputs, state_h_1, state_c_1, state_h_2, state_c_2 = encoder(en_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdrKJpkDRX_d"
      },
      "source": [
        "state_h = concatenate([state_h_1, state_h_2], axis=1)\n",
        "state_c = concatenate([state_c_1, state_c_1], axis=1)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElKNrSaRZtt"
      },
      "source": [
        "#non-bidirectional approach\n",
        "# encoder_inputs = Input(shape=(None,))\n",
        "# en_x=  Embedding(len(vocab_to_int)+1, embedding_size,weights=[embedding_matrix])(encoder_inputs)\n",
        "# encoder = LSTM(50, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "# # We discard `encoder_outputs` and only keep the states.\n",
        "# encoder_states = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tml7Vn3RbSN"
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dex=  Embedding(len(vocab_to_int), embedding_size)\n",
        "final_dex= dex(decoder_inputs)\n",
        "decoder_lstm = LSTM(200, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(vocab_to_int), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array|"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtG3M1ADRc_M",
        "outputId": "753983c2-b34c-40d1-d1ed-87c76afd2fe2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_51\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_58 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, None, 50)     95600       input_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_59 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) [(None, 200), (None, 120800      embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_14 (Embedding)        (None, None, 50)     95550       input_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 200)          0           bidirectional_6[0][1]            \n",
            "                                                                 bidirectional_6[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 200)          0           bidirectional_6[0][2]            \n",
            "                                                                 bidirectional_6[0][2]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  [(None, None, 200),  200800      embedding_14[0][0]               \n",
            "                                                                 concatenate_16[0][0]             \n",
            "                                                                 concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, None, 1911)   384111      lstm_14[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 896,861\n",
            "Trainable params: 896,861\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP3cPUqmRe0M",
        "outputId": "c93ab648-21d1-48c0-9fad-71027ce1d820"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=16,\n",
        "          epochs=20,\n",
        "          validation_split=0.05\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1782/1782 [==============================] - 29s 14ms/step - loss: 2.1201 - acc: 0.0518 - val_loss: 2.0380 - val_acc: 0.0643\n",
            "Epoch 2/20\n",
            "1782/1782 [==============================] - 24s 13ms/step - loss: 1.9930 - acc: 0.0652 - val_loss: 1.8840 - val_acc: 0.0819\n",
            "Epoch 3/20\n",
            "1782/1782 [==============================] - 24s 13ms/step - loss: 1.8416 - acc: 0.0820 - val_loss: 1.8011 - val_acc: 0.0888\n",
            "Epoch 4/20\n",
            "1782/1782 [==============================] - 24s 13ms/step - loss: 1.7797 - acc: 0.0906 - val_loss: 1.7706 - val_acc: 0.0956\n",
            "Epoch 5/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.7391 - acc: 0.0933 - val_loss: 1.7532 - val_acc: 0.0944\n",
            "Epoch 6/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.7143 - acc: 0.0956 - val_loss: 1.7466 - val_acc: 0.0948\n",
            "Epoch 7/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.7002 - acc: 0.0970 - val_loss: 1.7396 - val_acc: 0.0972\n",
            "Epoch 8/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.6818 - acc: 0.0979 - val_loss: 1.7248 - val_acc: 0.0973\n",
            "Epoch 9/20\n",
            "1782/1782 [==============================] - 25s 14ms/step - loss: 1.6623 - acc: 0.0984 - val_loss: 1.7156 - val_acc: 0.0979\n",
            "Epoch 10/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.6441 - acc: 0.0993 - val_loss: 1.7400 - val_acc: 0.0898\n",
            "Epoch 11/20\n",
            "1782/1782 [==============================] - 24s 13ms/step - loss: 1.6457 - acc: 0.0996 - val_loss: 1.7264 - val_acc: 0.0987\n",
            "Epoch 12/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.6452 - acc: 0.1007 - val_loss: 1.7335 - val_acc: 0.0954\n",
            "Epoch 13/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.6439 - acc: 0.1017 - val_loss: 1.7274 - val_acc: 0.0968\n",
            "Epoch 14/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.6295 - acc: 0.1011 - val_loss: 1.7308 - val_acc: 0.0977\n",
            "Epoch 15/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.6244 - acc: 0.1020 - val_loss: 1.7546 - val_acc: 0.0954\n",
            "Epoch 16/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.6276 - acc: 0.1022 - val_loss: 1.7365 - val_acc: 0.0977\n",
            "Epoch 17/20\n",
            "1782/1782 [==============================] - 24s 14ms/step - loss: 1.6130 - acc: 0.1027 - val_loss: 1.7377 - val_acc: 0.0954\n",
            "Epoch 18/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.6038 - acc: 0.1031 - val_loss: 1.7232 - val_acc: 0.0969\n",
            "Epoch 19/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.5937 - acc: 0.1033 - val_loss: 1.7322 - val_acc: 0.0931\n",
            "Epoch 20/20\n",
            "1782/1782 [==============================] - 23s 13ms/step - loss: 1.5758 - acc: 0.1039 - val_loss: 1.7471 - val_acc: 0.0949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03dae36b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 356
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzF0xW9nRnFo"
      },
      "source": [
        "model.save('/content/drive/MyDrive/final6_before_fine.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPw5HiWuumJV",
        "outputId": "5467b18b-d2de-482c-9653-31c8fcb0b9f4"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_52\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_58 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, None, 50)     95600       input_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) [(None, 200), (None, 120800      embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 200)          0           bidirectional_6[0][1]            \n",
            "                                                                 bidirectional_6[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 200)          0           bidirectional_6[0][2]            \n",
            "                                                                 bidirectional_6[0][2]            \n",
            "==================================================================================================\n",
            "Total params: 216,400\n",
            "Trainable params: 216,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t9jB8Esun2d"
      },
      "source": [
        "#Create sampling model\n",
        "decoder_state_input_h  = Input(shape=(200,))\n",
        "decoder_state_input_c = Input(shape=(200,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "final_dex2= dex(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwdKDph3up4n"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = vocab_to_int['<SOS>']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = int_to_vocab[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GwM7-xputRQ",
        "outputId": "3fb472b7-9e0f-4299-ea72-3a9b60257e97"
      },
      "source": [
        "for i in range(5):\n",
        "    seq_index = np.random.randint(1, len(encoder_input_data))\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', short_answers[seq_index: seq_index + 1])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ['why me']\n",
            "Decoded sentence:  i am sorry <EOS>\n",
            "-\n",
            "Input sentence: [' armed robbery ']\n",
            "Decoded sentence:  <UNK> <UNK> <UNK> <EOS>\n",
            "-\n",
            "Input sentence: ['nobody else can']\n",
            "Decoded sentence:  you are <UNK> <EOS>\n",
            "-\n",
            "Input sentence: ['does not look anything like her to me']\n",
            "Decoded sentence:  i am sorry <EOS>\n",
            "-\n",
            "Input sentence: ['he is like the pope like we are gonna snake a girl away from the pope']\n",
            "Decoded sentence:  i am sorry <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VcmrgnxE-KI"
      },
      "source": [
        "model.load_weights('/content/drive/MyDrive/final6_before_fine.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_WzIDnFYRF",
        "outputId": "0a85fc34-e504-4f26-bc15-0ab63b78bb45"
      },
      "source": [
        "model.fit([encoder_input_data_faq, decoder_input_data_faq], decoder_target_data_faq,\n",
        "          batch_size=16,\n",
        "          epochs=300,\n",
        "          validation_split=0.05\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 4.5443 - acc: 0.0596 - val_loss: 2.8912 - val_acc: 0.0455\n",
            "Epoch 2/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 3.5686 - acc: 0.0878 - val_loss: 2.7095 - val_acc: 0.0455\n",
            "Epoch 3/300\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 3.1137 - acc: 0.0925 - val_loss: 2.6220 - val_acc: 0.0455\n",
            "Epoch 4/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.8353 - acc: 0.0972 - val_loss: 2.5520 - val_acc: 0.0455\n",
            "Epoch 5/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 2.6783 - acc: 0.1034 - val_loss: 2.5805 - val_acc: 0.0455\n",
            "Epoch 6/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 2.5609 - acc: 0.1034 - val_loss: 2.4990 - val_acc: 0.0455\n",
            "Epoch 7/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.4654 - acc: 0.1097 - val_loss: 2.4952 - val_acc: 0.0455\n",
            "Epoch 8/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.3816 - acc: 0.1113 - val_loss: 2.4280 - val_acc: 0.0455\n",
            "Epoch 9/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.3128 - acc: 0.1129 - val_loss: 2.4314 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.2595 - acc: 0.1254 - val_loss: 2.3851 - val_acc: 0.0455\n",
            "Epoch 11/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.1777 - acc: 0.1160 - val_loss: 2.3922 - val_acc: 0.0455\n",
            "Epoch 12/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.1263 - acc: 0.1317 - val_loss: 2.3810 - val_acc: 0.0455\n",
            "Epoch 13/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.0742 - acc: 0.1348 - val_loss: 2.3288 - val_acc: 0.0455\n",
            "Epoch 14/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 2.0221 - acc: 0.1379 - val_loss: 2.3437 - val_acc: 0.0455\n",
            "Epoch 15/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 1.9975 - acc: 0.1442 - val_loss: 2.3251 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.9635 - acc: 0.1426 - val_loss: 2.2618 - val_acc: 0.0455\n",
            "Epoch 17/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.9033 - acc: 0.1442 - val_loss: 2.2875 - val_acc: 0.0455\n",
            "Epoch 18/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.8788 - acc: 0.1473 - val_loss: 2.2713 - val_acc: 0.0455\n",
            "Epoch 19/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.8314 - acc: 0.1489 - val_loss: 2.2729 - val_acc: 0.0455\n",
            "Epoch 20/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 1.7892 - acc: 0.1489 - val_loss: 2.2647 - val_acc: 0.0455\n",
            "Epoch 21/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.7648 - acc: 0.1583 - val_loss: 2.2814 - val_acc: 0.0455\n",
            "Epoch 22/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.7311 - acc: 0.1552 - val_loss: 2.2401 - val_acc: 0.0455\n",
            "Epoch 23/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.7276 - acc: 0.1583 - val_loss: 2.2425 - val_acc: 0.0455\n",
            "Epoch 24/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.6996 - acc: 0.1552 - val_loss: 2.2109 - val_acc: 0.0455\n",
            "Epoch 25/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.6539 - acc: 0.1677 - val_loss: 2.2480 - val_acc: 0.0455\n",
            "Epoch 26/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.6381 - acc: 0.1661 - val_loss: 2.2552 - val_acc: 0.0455\n",
            "Epoch 27/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.6417 - acc: 0.1599 - val_loss: 2.2459 - val_acc: 0.0455\n",
            "Epoch 28/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.6080 - acc: 0.1740 - val_loss: 2.2242 - val_acc: 0.0455\n",
            "Epoch 29/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.5647 - acc: 0.1755 - val_loss: 2.2003 - val_acc: 0.0455\n",
            "Epoch 30/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.5395 - acc: 0.1818 - val_loss: 2.1913 - val_acc: 0.0455\n",
            "Epoch 31/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.5443 - acc: 0.1834 - val_loss: 2.2213 - val_acc: 0.0455\n",
            "Epoch 32/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.5148 - acc: 0.1865 - val_loss: 2.2062 - val_acc: 0.0455\n",
            "Epoch 33/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 1.4862 - acc: 0.1944 - val_loss: 2.2104 - val_acc: 0.0455\n",
            "Epoch 34/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.4686 - acc: 0.1944 - val_loss: 2.1702 - val_acc: 0.0455\n",
            "Epoch 35/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.4451 - acc: 0.2132 - val_loss: 2.2032 - val_acc: 0.0455\n",
            "Epoch 36/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.4244 - acc: 0.2069 - val_loss: 2.2176 - val_acc: 0.0455\n",
            "Epoch 37/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 1.4062 - acc: 0.2273 - val_loss: 2.1904 - val_acc: 0.0455\n",
            "Epoch 38/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.3852 - acc: 0.2241 - val_loss: 2.2064 - val_acc: 0.0455\n",
            "Epoch 39/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.3716 - acc: 0.2257 - val_loss: 2.1674 - val_acc: 0.0455\n",
            "Epoch 40/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.3699 - acc: 0.2288 - val_loss: 2.2299 - val_acc: 0.0455\n",
            "Epoch 41/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.3658 - acc: 0.2288 - val_loss: 2.1959 - val_acc: 0.0455\n",
            "Epoch 42/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.3273 - acc: 0.2367 - val_loss: 2.1791 - val_acc: 0.0455\n",
            "Epoch 43/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 1.3065 - acc: 0.2304 - val_loss: 2.1744 - val_acc: 0.0455\n",
            "Epoch 44/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.2898 - acc: 0.2429 - val_loss: 2.1638 - val_acc: 0.0455\n",
            "Epoch 45/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.2755 - acc: 0.2398 - val_loss: 2.1269 - val_acc: 0.0455\n",
            "Epoch 46/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.2508 - acc: 0.2492 - val_loss: 2.1600 - val_acc: 0.0455\n",
            "Epoch 47/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.2363 - acc: 0.2524 - val_loss: 2.1963 - val_acc: 0.0455\n",
            "Epoch 48/300\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 1.2348 - acc: 0.2429 - val_loss: 2.2115 - val_acc: 0.0455\n",
            "Epoch 49/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.2347 - acc: 0.2602 - val_loss: 2.1761 - val_acc: 0.0455\n",
            "Epoch 50/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.2228 - acc: 0.2539 - val_loss: 2.1558 - val_acc: 0.0455\n",
            "Epoch 51/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 1.1773 - acc: 0.2649 - val_loss: 2.1860 - val_acc: 0.0455\n",
            "Epoch 52/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.1609 - acc: 0.2665 - val_loss: 2.1555 - val_acc: 0.0455\n",
            "Epoch 53/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.1544 - acc: 0.2696 - val_loss: 2.1697 - val_acc: 0.0455\n",
            "Epoch 54/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 1.1444 - acc: 0.2837 - val_loss: 2.1946 - val_acc: 0.0455\n",
            "Epoch 55/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.1345 - acc: 0.2727 - val_loss: 2.1850 - val_acc: 0.0455\n",
            "Epoch 56/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.1174 - acc: 0.2884 - val_loss: 2.1432 - val_acc: 0.0455\n",
            "Epoch 57/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.1087 - acc: 0.2915 - val_loss: 2.1786 - val_acc: 0.0455\n",
            "Epoch 58/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.0902 - acc: 0.2806 - val_loss: 2.1510 - val_acc: 0.0455\n",
            "Epoch 59/300\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.0798 - acc: 0.2900 - val_loss: 2.1330 - val_acc: 0.0455\n",
            "Epoch 60/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.0558 - acc: 0.2947 - val_loss: 2.1679 - val_acc: 0.0455\n",
            "Epoch 61/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 1.0462 - acc: 0.3041 - val_loss: 2.1714 - val_acc: 0.0455\n",
            "Epoch 62/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 1.0501 - acc: 0.2947 - val_loss: 2.2012 - val_acc: 0.0455\n",
            "Epoch 63/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 1.0283 - acc: 0.3025 - val_loss: 2.1568 - val_acc: 0.0455\n",
            "Epoch 64/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.0314 - acc: 0.3166 - val_loss: 2.1775 - val_acc: 0.0455\n",
            "Epoch 65/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.0122 - acc: 0.3103 - val_loss: 2.1324 - val_acc: 0.0455\n",
            "Epoch 66/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.0061 - acc: 0.3025 - val_loss: 2.1629 - val_acc: 0.0455\n",
            "Epoch 67/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.9825 - acc: 0.3182 - val_loss: 2.1226 - val_acc: 0.0455\n",
            "Epoch 68/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.9759 - acc: 0.3166 - val_loss: 2.1118 - val_acc: 0.0455\n",
            "Epoch 69/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.9554 - acc: 0.3245 - val_loss: 2.1290 - val_acc: 0.0455\n",
            "Epoch 70/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.9458 - acc: 0.3401 - val_loss: 2.1801 - val_acc: 0.0455\n",
            "Epoch 71/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.9342 - acc: 0.3292 - val_loss: 2.1176 - val_acc: 0.0455\n",
            "Epoch 72/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.9212 - acc: 0.3323 - val_loss: 2.1780 - val_acc: 0.0455\n",
            "Epoch 73/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.9207 - acc: 0.3323 - val_loss: 2.1799 - val_acc: 0.0455\n",
            "Epoch 74/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.9161 - acc: 0.3574 - val_loss: 2.1462 - val_acc: 0.0455\n",
            "Epoch 75/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.9143 - acc: 0.3448 - val_loss: 2.1470 - val_acc: 0.0455\n",
            "Epoch 76/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.8894 - acc: 0.3542 - val_loss: 2.1376 - val_acc: 0.0455\n",
            "Epoch 77/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.8709 - acc: 0.3636 - val_loss: 2.1450 - val_acc: 0.0455\n",
            "Epoch 78/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.8619 - acc: 0.3652 - val_loss: 2.1525 - val_acc: 0.0455\n",
            "Epoch 79/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.8655 - acc: 0.3574 - val_loss: 2.2337 - val_acc: 0.0455\n",
            "Epoch 80/300\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.8818 - acc: 0.3527 - val_loss: 2.1549 - val_acc: 0.0455\n",
            "Epoch 81/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.8448 - acc: 0.3589 - val_loss: 2.1449 - val_acc: 0.0455\n",
            "Epoch 82/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.8279 - acc: 0.3715 - val_loss: 2.1440 - val_acc: 0.0455\n",
            "Epoch 83/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.8212 - acc: 0.3793 - val_loss: 2.1549 - val_acc: 0.0455\n",
            "Epoch 84/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.8106 - acc: 0.3809 - val_loss: 2.1346 - val_acc: 0.0455\n",
            "Epoch 85/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.8083 - acc: 0.3824 - val_loss: 2.1507 - val_acc: 0.0455\n",
            "Epoch 86/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.7993 - acc: 0.3777 - val_loss: 2.1890 - val_acc: 0.0455\n",
            "Epoch 87/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.7996 - acc: 0.3856 - val_loss: 2.1864 - val_acc: 0.0455\n",
            "Epoch 88/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.7927 - acc: 0.3793 - val_loss: 2.2080 - val_acc: 0.0455\n",
            "Epoch 89/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.8252 - acc: 0.3746 - val_loss: 2.1620 - val_acc: 0.0455\n",
            "Epoch 90/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.7640 - acc: 0.3981 - val_loss: 2.1401 - val_acc: 0.0455\n",
            "Epoch 91/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.7562 - acc: 0.3887 - val_loss: 2.1626 - val_acc: 0.0455\n",
            "Epoch 92/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.7442 - acc: 0.4013 - val_loss: 2.1341 - val_acc: 0.0455\n",
            "Epoch 93/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.7381 - acc: 0.3966 - val_loss: 2.1364 - val_acc: 0.0455\n",
            "Epoch 94/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.7299 - acc: 0.4013 - val_loss: 2.1454 - val_acc: 0.0455\n",
            "Epoch 95/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.7297 - acc: 0.3997 - val_loss: 2.1518 - val_acc: 0.0455\n",
            "Epoch 96/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.7404 - acc: 0.3981 - val_loss: 2.1811 - val_acc: 0.0455\n",
            "Epoch 97/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.7373 - acc: 0.4044 - val_loss: 2.1593 - val_acc: 0.0455\n",
            "Epoch 98/300\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.7125 - acc: 0.4169 - val_loss: 2.1484 - val_acc: 0.0455\n",
            "Epoch 99/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.7023 - acc: 0.4154 - val_loss: 2.1167 - val_acc: 0.0455\n",
            "Epoch 100/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.6985 - acc: 0.4154 - val_loss: 2.1404 - val_acc: 0.0455\n",
            "Epoch 101/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.6964 - acc: 0.4122 - val_loss: 2.1859 - val_acc: 0.0455\n",
            "Epoch 102/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.6969 - acc: 0.4091 - val_loss: 2.1896 - val_acc: 0.0455\n",
            "Epoch 103/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.6808 - acc: 0.4216 - val_loss: 2.1486 - val_acc: 0.0455\n",
            "Epoch 104/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.6871 - acc: 0.4201 - val_loss: 2.2035 - val_acc: 0.0455\n",
            "Epoch 105/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.6763 - acc: 0.4201 - val_loss: 2.1424 - val_acc: 0.0455\n",
            "Epoch 106/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6538 - acc: 0.4248 - val_loss: 2.1998 - val_acc: 0.0455\n",
            "Epoch 107/300\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.6479 - acc: 0.4263 - val_loss: 2.1394 - val_acc: 0.0455\n",
            "Epoch 108/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.6460 - acc: 0.4357 - val_loss: 2.1517 - val_acc: 0.0455\n",
            "Epoch 109/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6472 - acc: 0.4357 - val_loss: 2.1449 - val_acc: 0.0455\n",
            "Epoch 110/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.6311 - acc: 0.4357 - val_loss: 2.1771 - val_acc: 0.0455\n",
            "Epoch 111/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6279 - acc: 0.4389 - val_loss: 2.1619 - val_acc: 0.0455\n",
            "Epoch 112/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.6375 - acc: 0.4389 - val_loss: 2.2056 - val_acc: 0.0455\n",
            "Epoch 113/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.6390 - acc: 0.4279 - val_loss: 2.2007 - val_acc: 0.0455\n",
            "Epoch 114/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.6172 - acc: 0.4420 - val_loss: 2.1458 - val_acc: 0.0455\n",
            "Epoch 115/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6021 - acc: 0.4436 - val_loss: 2.1508 - val_acc: 0.0455\n",
            "Epoch 116/300\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.5964 - acc: 0.4545 - val_loss: 2.1704 - val_acc: 0.0455\n",
            "Epoch 117/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5938 - acc: 0.4389 - val_loss: 2.1513 - val_acc: 0.0455\n",
            "Epoch 118/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5887 - acc: 0.4514 - val_loss: 2.1285 - val_acc: 0.0455\n",
            "Epoch 119/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5980 - acc: 0.4436 - val_loss: 2.1914 - val_acc: 0.0455\n",
            "Epoch 120/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5990 - acc: 0.4342 - val_loss: 2.1837 - val_acc: 0.0455\n",
            "Epoch 121/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.5746 - acc: 0.4545 - val_loss: 2.1541 - val_acc: 0.0455\n",
            "Epoch 122/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5709 - acc: 0.4561 - val_loss: 2.1520 - val_acc: 0.0455\n",
            "Epoch 123/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5644 - acc: 0.4514 - val_loss: 2.1533 - val_acc: 0.0455\n",
            "Epoch 124/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5615 - acc: 0.4592 - val_loss: 2.1353 - val_acc: 0.0455\n",
            "Epoch 125/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.5625 - acc: 0.4545 - val_loss: 2.1643 - val_acc: 0.0455\n",
            "Epoch 126/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.5676 - acc: 0.4467 - val_loss: 2.1691 - val_acc: 0.0455\n",
            "Epoch 127/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.5542 - acc: 0.4561 - val_loss: 2.1477 - val_acc: 0.0455\n",
            "Epoch 128/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5397 - acc: 0.4655 - val_loss: 2.1254 - val_acc: 0.0455\n",
            "Epoch 129/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5348 - acc: 0.4624 - val_loss: 2.1431 - val_acc: 0.0455\n",
            "Epoch 130/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5352 - acc: 0.4624 - val_loss: 2.1382 - val_acc: 0.0455\n",
            "Epoch 131/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5324 - acc: 0.4608 - val_loss: 2.1582 - val_acc: 0.0455\n",
            "Epoch 132/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5228 - acc: 0.4624 - val_loss: 2.1088 - val_acc: 0.0455\n",
            "Epoch 133/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.5202 - acc: 0.4639 - val_loss: 2.1479 - val_acc: 0.0455\n",
            "Epoch 134/300\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.5187 - acc: 0.4639 - val_loss: 2.1490 - val_acc: 0.0455\n",
            "Epoch 135/300\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.5217 - acc: 0.4608 - val_loss: 2.1657 - val_acc: 0.0455\n",
            "Epoch 136/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.5210 - acc: 0.4561 - val_loss: 2.1350 - val_acc: 0.0455\n",
            "Epoch 137/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.5136 - acc: 0.4624 - val_loss: 2.0982 - val_acc: 0.0455\n",
            "Epoch 138/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4997 - acc: 0.4671 - val_loss: 2.1187 - val_acc: 0.0455\n",
            "Epoch 139/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4933 - acc: 0.4687 - val_loss: 2.1154 - val_acc: 0.0455\n",
            "Epoch 140/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4895 - acc: 0.4749 - val_loss: 2.1035 - val_acc: 0.0455\n",
            "Epoch 141/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.4902 - acc: 0.4734 - val_loss: 2.0956 - val_acc: 0.0455\n",
            "Epoch 142/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4783 - acc: 0.4749 - val_loss: 2.1058 - val_acc: 0.0455\n",
            "Epoch 143/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4757 - acc: 0.4749 - val_loss: 2.1362 - val_acc: 0.0455\n",
            "Epoch 144/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4845 - acc: 0.4702 - val_loss: 2.1232 - val_acc: 0.0455\n",
            "Epoch 145/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.5003 - acc: 0.4608 - val_loss: 2.1112 - val_acc: 0.0455\n",
            "Epoch 146/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.4681 - acc: 0.4765 - val_loss: 2.1114 - val_acc: 0.0455\n",
            "Epoch 147/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4612 - acc: 0.4749 - val_loss: 2.0842 - val_acc: 0.0455\n",
            "Epoch 148/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4564 - acc: 0.4765 - val_loss: 2.1042 - val_acc: 0.0455\n",
            "Epoch 149/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4530 - acc: 0.4765 - val_loss: 2.1266 - val_acc: 0.0455\n",
            "Epoch 150/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4522 - acc: 0.4781 - val_loss: 2.1078 - val_acc: 0.0455\n",
            "Epoch 151/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4470 - acc: 0.4781 - val_loss: 2.0977 - val_acc: 0.0455\n",
            "Epoch 152/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4462 - acc: 0.4749 - val_loss: 2.1258 - val_acc: 0.0455\n",
            "Epoch 153/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4524 - acc: 0.4781 - val_loss: 2.1754 - val_acc: 0.0909\n",
            "Epoch 154/300\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.4760 - acc: 0.4687 - val_loss: 2.1290 - val_acc: 0.0455\n",
            "Epoch 155/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4401 - acc: 0.4859 - val_loss: 2.1145 - val_acc: 0.0455\n",
            "Epoch 156/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4298 - acc: 0.4906 - val_loss: 2.1348 - val_acc: 0.0455\n",
            "Epoch 157/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4276 - acc: 0.4859 - val_loss: 2.1040 - val_acc: 0.0455\n",
            "Epoch 158/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4229 - acc: 0.4906 - val_loss: 2.1264 - val_acc: 0.0455\n",
            "Epoch 159/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4196 - acc: 0.4922 - val_loss: 2.0770 - val_acc: 0.0455\n",
            "Epoch 160/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4178 - acc: 0.4890 - val_loss: 2.1092 - val_acc: 0.0455\n",
            "Epoch 161/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.4151 - acc: 0.4906 - val_loss: 2.1248 - val_acc: 0.0455\n",
            "Epoch 162/300\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.4139 - acc: 0.4922 - val_loss: 2.0517 - val_acc: 0.0455\n",
            "Epoch 163/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4176 - acc: 0.4843 - val_loss: 2.1031 - val_acc: 0.0455\n",
            "Epoch 164/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.4231 - acc: 0.4843 - val_loss: 2.1737 - val_acc: 0.0909\n",
            "Epoch 165/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4179 - acc: 0.4875 - val_loss: 2.0775 - val_acc: 0.0455\n",
            "Epoch 166/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4051 - acc: 0.4922 - val_loss: 2.1522 - val_acc: 0.0455\n",
            "Epoch 167/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.3973 - acc: 0.4937 - val_loss: 2.0951 - val_acc: 0.0455\n",
            "Epoch 168/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.3947 - acc: 0.4922 - val_loss: 2.1267 - val_acc: 0.0455\n",
            "Epoch 169/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.3913 - acc: 0.4937 - val_loss: 2.0947 - val_acc: 0.0455\n",
            "Epoch 170/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.3885 - acc: 0.4953 - val_loss: 2.0802 - val_acc: 0.0455\n",
            "Epoch 171/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3864 - acc: 0.4937 - val_loss: 2.0790 - val_acc: 0.0455\n",
            "Epoch 172/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.3819 - acc: 0.4937 - val_loss: 2.1206 - val_acc: 0.0455\n",
            "Epoch 173/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3828 - acc: 0.4937 - val_loss: 2.1479 - val_acc: 0.0909\n",
            "Epoch 174/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3989 - acc: 0.4906 - val_loss: 2.1165 - val_acc: 0.0909\n",
            "Epoch 175/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3839 - acc: 0.4922 - val_loss: 2.0869 - val_acc: 0.0455\n",
            "Epoch 176/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.3728 - acc: 0.4984 - val_loss: 2.0722 - val_acc: 0.0455\n",
            "Epoch 177/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3703 - acc: 0.4984 - val_loss: 2.1182 - val_acc: 0.0455\n",
            "Epoch 178/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.3693 - acc: 0.4969 - val_loss: 2.1117 - val_acc: 0.0455\n",
            "Epoch 179/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3632 - acc: 0.4984 - val_loss: 2.0612 - val_acc: 0.0455\n",
            "Epoch 180/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3609 - acc: 0.4969 - val_loss: 2.0784 - val_acc: 0.0455\n",
            "Epoch 181/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3614 - acc: 0.4969 - val_loss: 2.0843 - val_acc: 0.0455\n",
            "Epoch 182/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3624 - acc: 0.5000 - val_loss: 2.0776 - val_acc: 0.0455\n",
            "Epoch 183/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3578 - acc: 0.4984 - val_loss: 2.1142 - val_acc: 0.0909\n",
            "Epoch 184/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3656 - acc: 0.4969 - val_loss: 2.1874 - val_acc: 0.0909\n",
            "Epoch 185/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3565 - acc: 0.5000 - val_loss: 2.1043 - val_acc: 0.0455\n",
            "Epoch 186/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3466 - acc: 0.4984 - val_loss: 2.0663 - val_acc: 0.0455\n",
            "Epoch 187/300\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.3448 - acc: 0.4984 - val_loss: 2.0771 - val_acc: 0.0455\n",
            "Epoch 188/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3427 - acc: 0.5000 - val_loss: 2.0624 - val_acc: 0.0455\n",
            "Epoch 189/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3390 - acc: 0.5000 - val_loss: 2.0406 - val_acc: 0.0455\n",
            "Epoch 190/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.3377 - acc: 0.5000 - val_loss: 2.0484 - val_acc: 0.0455\n",
            "Epoch 191/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3383 - acc: 0.4984 - val_loss: 2.0695 - val_acc: 0.0455\n",
            "Epoch 192/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3340 - acc: 0.4984 - val_loss: 2.0937 - val_acc: 0.0455\n",
            "Epoch 193/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3322 - acc: 0.5000 - val_loss: 2.0775 - val_acc: 0.0455\n",
            "Epoch 194/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3436 - acc: 0.5000 - val_loss: 2.0145 - val_acc: 0.0455\n",
            "Epoch 195/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3379 - acc: 0.4953 - val_loss: 2.0873 - val_acc: 0.0455\n",
            "Epoch 196/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3261 - acc: 0.5000 - val_loss: 2.0843 - val_acc: 0.0455\n",
            "Epoch 197/300\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.3212 - acc: 0.5016 - val_loss: 2.0802 - val_acc: 0.0455\n",
            "Epoch 198/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3197 - acc: 0.5016 - val_loss: 2.0360 - val_acc: 0.0455\n",
            "Epoch 199/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3191 - acc: 0.5016 - val_loss: 2.0548 - val_acc: 0.0455\n",
            "Epoch 200/300\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.3156 - acc: 0.5016 - val_loss: 2.0575 - val_acc: 0.0455\n",
            "Epoch 201/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3133 - acc: 0.5016 - val_loss: 2.1198 - val_acc: 0.0682\n",
            "Epoch 202/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3124 - acc: 0.5016 - val_loss: 2.0972 - val_acc: 0.0909\n",
            "Epoch 203/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.3181 - acc: 0.5016 - val_loss: 2.2007 - val_acc: 0.0909\n",
            "Epoch 204/300\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.3227 - acc: 0.5000 - val_loss: 2.0495 - val_acc: 0.0682\n",
            "Epoch 205/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3089 - acc: 0.5016 - val_loss: 2.0276 - val_acc: 0.0455\n",
            "Epoch 206/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3045 - acc: 0.5016 - val_loss: 2.0810 - val_acc: 0.0455\n",
            "Epoch 207/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.3021 - acc: 0.5031 - val_loss: 2.0326 - val_acc: 0.0455\n",
            "Epoch 208/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.3004 - acc: 0.5016 - val_loss: 2.0200 - val_acc: 0.0455\n",
            "Epoch 209/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2981 - acc: 0.5031 - val_loss: 2.0606 - val_acc: 0.0455\n",
            "Epoch 210/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2951 - acc: 0.5031 - val_loss: 2.0442 - val_acc: 0.0455\n",
            "Epoch 211/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2936 - acc: 0.5047 - val_loss: 2.0299 - val_acc: 0.0682\n",
            "Epoch 212/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2933 - acc: 0.5047 - val_loss: 2.0724 - val_acc: 0.0909\n",
            "Epoch 213/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2955 - acc: 0.5031 - val_loss: 2.0870 - val_acc: 0.0909\n",
            "Epoch 214/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2924 - acc: 0.5031 - val_loss: 2.0922 - val_acc: 0.0909\n",
            "Epoch 215/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2957 - acc: 0.5016 - val_loss: 2.1607 - val_acc: 0.0909\n",
            "Epoch 216/300\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.2949 - acc: 0.5047 - val_loss: 2.0767 - val_acc: 0.0909\n",
            "Epoch 217/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.2841 - acc: 0.5031 - val_loss: 2.0530 - val_acc: 0.0682\n",
            "Epoch 218/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2816 - acc: 0.5031 - val_loss: 2.0528 - val_acc: 0.0909\n",
            "Epoch 219/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2790 - acc: 0.5031 - val_loss: 2.0642 - val_acc: 0.0455\n",
            "Epoch 220/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2775 - acc: 0.5047 - val_loss: 2.0326 - val_acc: 0.0682\n",
            "Epoch 221/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2769 - acc: 0.5047 - val_loss: 2.0343 - val_acc: 0.0909\n",
            "Epoch 222/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2752 - acc: 0.5063 - val_loss: 2.0568 - val_acc: 0.0909\n",
            "Epoch 223/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2720 - acc: 0.5063 - val_loss: 2.0604 - val_acc: 0.0909\n",
            "Epoch 224/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2721 - acc: 0.5063 - val_loss: 2.1142 - val_acc: 0.0909\n",
            "Epoch 225/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2801 - acc: 0.5047 - val_loss: 2.1356 - val_acc: 0.0909\n",
            "Epoch 226/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2815 - acc: 0.5047 - val_loss: 2.0714 - val_acc: 0.0909\n",
            "Epoch 227/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2673 - acc: 0.5078 - val_loss: 2.0496 - val_acc: 0.0909\n",
            "Epoch 228/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2658 - acc: 0.5063 - val_loss: 2.0508 - val_acc: 0.0909\n",
            "Epoch 229/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2632 - acc: 0.5110 - val_loss: 2.0414 - val_acc: 0.0909\n",
            "Epoch 230/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2612 - acc: 0.5094 - val_loss: 2.0429 - val_acc: 0.0909\n",
            "Epoch 231/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2600 - acc: 0.5094 - val_loss: 2.0385 - val_acc: 0.0909\n",
            "Epoch 232/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2582 - acc: 0.5094 - val_loss: 2.0693 - val_acc: 0.0909\n",
            "Epoch 233/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2569 - acc: 0.5094 - val_loss: 2.0602 - val_acc: 0.0909\n",
            "Epoch 234/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2578 - acc: 0.5110 - val_loss: 2.1300 - val_acc: 0.0909\n",
            "Epoch 235/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2606 - acc: 0.5078 - val_loss: 2.0304 - val_acc: 0.0909\n",
            "Epoch 236/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2551 - acc: 0.5110 - val_loss: 2.0960 - val_acc: 0.0909\n",
            "Epoch 237/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2523 - acc: 0.5110 - val_loss: 2.0066 - val_acc: 0.0909\n",
            "Epoch 238/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2511 - acc: 0.5110 - val_loss: 2.0032 - val_acc: 0.0909\n",
            "Epoch 239/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2498 - acc: 0.5094 - val_loss: 1.9965 - val_acc: 0.0682\n",
            "Epoch 240/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2478 - acc: 0.5110 - val_loss: 2.0309 - val_acc: 0.0455\n",
            "Epoch 241/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2467 - acc: 0.5094 - val_loss: 2.0063 - val_acc: 0.0455\n",
            "Epoch 242/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2472 - acc: 0.5094 - val_loss: 2.0169 - val_acc: 0.0455\n",
            "Epoch 243/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2444 - acc: 0.5094 - val_loss: 1.9540 - val_acc: 0.0909\n",
            "Epoch 244/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2424 - acc: 0.5125 - val_loss: 1.9928 - val_acc: 0.0909\n",
            "Epoch 245/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2398 - acc: 0.5110 - val_loss: 2.0559 - val_acc: 0.0909\n",
            "Epoch 246/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2389 - acc: 0.5141 - val_loss: 2.0569 - val_acc: 0.0909\n",
            "Epoch 247/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2381 - acc: 0.5110 - val_loss: 2.0513 - val_acc: 0.0909\n",
            "Epoch 248/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2368 - acc: 0.5110 - val_loss: 2.0265 - val_acc: 0.0909\n",
            "Epoch 249/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2343 - acc: 0.5141 - val_loss: 2.0336 - val_acc: 0.0909\n",
            "Epoch 250/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2348 - acc: 0.5110 - val_loss: 2.0715 - val_acc: 0.0909\n",
            "Epoch 251/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2429 - acc: 0.5125 - val_loss: 2.1000 - val_acc: 0.0909\n",
            "Epoch 252/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2333 - acc: 0.5094 - val_loss: 2.0174 - val_acc: 0.0909\n",
            "Epoch 253/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2288 - acc: 0.5141 - val_loss: 2.0506 - val_acc: 0.0909\n",
            "Epoch 254/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2265 - acc: 0.5125 - val_loss: 2.0010 - val_acc: 0.0909\n",
            "Epoch 255/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2261 - acc: 0.5125 - val_loss: 2.0689 - val_acc: 0.0909\n",
            "Epoch 256/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2255 - acc: 0.5110 - val_loss: 2.0355 - val_acc: 0.0909\n",
            "Epoch 257/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2234 - acc: 0.5141 - val_loss: 2.0105 - val_acc: 0.0909\n",
            "Epoch 258/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2246 - acc: 0.5125 - val_loss: 1.9290 - val_acc: 0.0682\n",
            "Epoch 259/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2262 - acc: 0.5125 - val_loss: 2.0055 - val_acc: 0.0909\n",
            "Epoch 260/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2204 - acc: 0.5141 - val_loss: 2.0123 - val_acc: 0.0909\n",
            "Epoch 261/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2201 - acc: 0.5157 - val_loss: 1.9591 - val_acc: 0.0909\n",
            "Epoch 262/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2191 - acc: 0.5141 - val_loss: 1.9935 - val_acc: 0.0909\n",
            "Epoch 263/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2169 - acc: 0.5141 - val_loss: 2.0096 - val_acc: 0.0909\n",
            "Epoch 264/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2159 - acc: 0.5125 - val_loss: 2.0086 - val_acc: 0.0909\n",
            "Epoch 265/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2198 - acc: 0.5141 - val_loss: 1.9662 - val_acc: 0.0909\n",
            "Epoch 266/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2160 - acc: 0.5125 - val_loss: 2.0380 - val_acc: 0.0909\n",
            "Epoch 267/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2115 - acc: 0.5141 - val_loss: 2.0290 - val_acc: 0.0909\n",
            "Epoch 268/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.2115 - acc: 0.5141 - val_loss: 2.0459 - val_acc: 0.1136\n",
            "Epoch 269/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.2094 - acc: 0.5141 - val_loss: 1.9987 - val_acc: 0.1136\n",
            "Epoch 270/300\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2087 - acc: 0.5125 - val_loss: 2.0079 - val_acc: 0.0909\n",
            "Epoch 271/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2070 - acc: 0.5141 - val_loss: 2.0323 - val_acc: 0.1136\n",
            "Epoch 272/300\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.2058 - acc: 0.5157 - val_loss: 1.9913 - val_acc: 0.1136\n",
            "Epoch 273/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2047 - acc: 0.5141 - val_loss: 2.0123 - val_acc: 0.1136\n",
            "Epoch 274/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.2043 - acc: 0.5157 - val_loss: 1.9706 - val_acc: 0.1136\n",
            "Epoch 275/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2049 - acc: 0.5157 - val_loss: 1.9599 - val_acc: 0.1136\n",
            "Epoch 276/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.2091 - acc: 0.5141 - val_loss: 2.0212 - val_acc: 0.0909\n",
            "Epoch 277/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2051 - acc: 0.5141 - val_loss: 1.9947 - val_acc: 0.1136\n",
            "Epoch 278/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2015 - acc: 0.5141 - val_loss: 2.0043 - val_acc: 0.1136\n",
            "Epoch 279/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1983 - acc: 0.5157 - val_loss: 2.0218 - val_acc: 0.1136\n",
            "Epoch 280/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1965 - acc: 0.5141 - val_loss: 2.0163 - val_acc: 0.1136\n",
            "Epoch 281/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1953 - acc: 0.5141 - val_loss: 1.9927 - val_acc: 0.1136\n",
            "Epoch 282/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1948 - acc: 0.5141 - val_loss: 1.9774 - val_acc: 0.1136\n",
            "Epoch 283/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1941 - acc: 0.5157 - val_loss: 2.0054 - val_acc: 0.1136\n",
            "Epoch 284/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1935 - acc: 0.5141 - val_loss: 1.9696 - val_acc: 0.1136\n",
            "Epoch 285/300\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1933 - acc: 0.5157 - val_loss: 1.9693 - val_acc: 0.1136\n",
            "Epoch 286/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.1972 - acc: 0.5141 - val_loss: 1.9379 - val_acc: 0.1136\n",
            "Epoch 287/300\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.1927 - acc: 0.5157 - val_loss: 1.9788 - val_acc: 0.1136\n",
            "Epoch 288/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1891 - acc: 0.5172 - val_loss: 1.9991 - val_acc: 0.1136\n",
            "Epoch 289/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1883 - acc: 0.5157 - val_loss: 2.0209 - val_acc: 0.1136\n",
            "Epoch 290/300\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1868 - acc: 0.5172 - val_loss: 1.9973 - val_acc: 0.1136\n",
            "Epoch 291/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1852 - acc: 0.5157 - val_loss: 1.9880 - val_acc: 0.1136\n",
            "Epoch 292/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1846 - acc: 0.5157 - val_loss: 1.9935 - val_acc: 0.1136\n",
            "Epoch 293/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1844 - acc: 0.5157 - val_loss: 1.9644 - val_acc: 0.1136\n",
            "Epoch 294/300\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1853 - acc: 0.5172 - val_loss: 1.9309 - val_acc: 0.1136\n",
            "Epoch 295/300\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1846 - acc: 0.5157 - val_loss: 2.0010 - val_acc: 0.1136\n",
            "Epoch 296/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1828 - acc: 0.5172 - val_loss: 1.9367 - val_acc: 0.1136\n",
            "Epoch 297/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1820 - acc: 0.5157 - val_loss: 1.9775 - val_acc: 0.1136\n",
            "Epoch 298/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1797 - acc: 0.5157 - val_loss: 1.9983 - val_acc: 0.1136\n",
            "Epoch 299/300\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1782 - acc: 0.5172 - val_loss: 1.9957 - val_acc: 0.1136\n",
            "Epoch 300/300\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1770 - acc: 0.5172 - val_loss: 2.0340 - val_acc: 0.1136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03cfe27750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO8TyENbFfkW"
      },
      "source": [
        "model.save('/content/drive/MyDrive/final6_after_fine.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ke4wennGjwl",
        "outputId": "768a2619-8479-4f9d-ba87-f720dc52310c"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_54\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_58 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, None, 50)     95600       input_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) [(None, 200), (None, 120800      embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 200)          0           bidirectional_6[0][1]            \n",
            "                                                                 bidirectional_6[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 200)          0           bidirectional_6[0][2]            \n",
            "                                                                 bidirectional_6[0][2]            \n",
            "==================================================================================================\n",
            "Total params: 216,400\n",
            "Trainable params: 216,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqHOM9qEGqFj"
      },
      "source": [
        "#Create sampling model\n",
        "decoder_state_input_h  = Input(shape=(200,))\n",
        "decoder_state_input_c = Input(shape=(200,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "final_dex2= dex(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNrftwQ_GuUj"
      },
      "source": [
        "def decode_sequence_faq(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((20,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = vocab_to_int['<SOS>']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = int_to_vocab[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '<EOS>' or\n",
        "           len(decoded_sentence) > 100):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((20,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFxqy6NgGyUD",
        "outputId": "b744176f-3d14-47e8-f8f7-ec8fc87caacb"
      },
      "source": [
        "for i in range(10):\n",
        "  seq_index = np.random.randint(1, len(encoder_input_data_faq))\n",
        "  # Take one sequence (part of the training set)\n",
        "  # for trying out decoding.\n",
        "  input_seq = encoder_input_data_faq[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print('-')\n",
        "  print('Input sentence:', faq_ques_final[seq_index: seq_index + 1])\n",
        "  print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ['when will homework be released ']\n",
            "Decoded sentence:  the homework release is subject to change and hence will\n",
            "-\n",
            "Input sentence: ['where do i find the recording ']\n",
            "Decoded sentence:  recording links will be posted on edstem <EOS>\n",
            "-\n",
            "Input sentence: ['i have a different approach to the exercise why is the autograder still failing me']\n",
            "Decoded sentence:  unfortunately the autograder does not accept alternative\n",
            "-\n",
            "Input sentence: ['what is the link for the slack channel']\n",
            "Decoded sentence:  all official communication by prof protopapas will be\n",
            "-\n",
            "Input sentence: ['will there be any extension allowed for the quiz and exercise submission']\n",
            "Decoded sentence:  only for exceptional cases <EOS>\n",
            "-\n",
            "Input sentence: ['where do i find the recording ']\n",
            "Decoded sentence:  recording links will be posted on edstem <EOS>\n",
            "-\n",
            "Input sentence: ['when will the reading be published ']\n",
            "Decoded sentence:  after every lab we will post the reading for the next\n",
            "-\n",
            "Input sentence: ['what is the zoom the link for oh ']\n",
            "Decoded sentence:  please check the course information slide for zoom links\n",
            "-\n",
            "Input sentence: ['will there be any extension in the project deadline']\n",
            "Decoded sentence:  no however exceptional cases may be considered for project\n",
            "-\n",
            "Input sentence: ['will there be any extension in the project deadline']\n",
            "Decoded sentence:  no however exceptional cases may be considered for project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3rUJ-eCG-uD",
        "outputId": "fc130404-2aba-49be-a20a-e4bb91b2b001"
      },
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Questions\n",
              "0            Will the pre-class session be recorded?\n",
              "1          What is the deadline for quiz submission?\n",
              "2      What is the deadline for exercise submission?\n",
              "3  How many hours do I need to complete this course?\n",
              "4                       Who will grade the exercise?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 369
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFSvSCY2HK23"
      },
      "source": [
        "tidy_test_ques = []\n",
        "for conve in test.Questions:\n",
        "    text = clean_text(conve)\n",
        "    tidy_test_ques.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRg2KlzdHOP2"
      },
      "source": [
        "test_ques=[]\n",
        "for i, question in enumerate(tidy_test_ques):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        test_ques.append(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpAvwYKbHQ5y"
      },
      "source": [
        "ques_int_test = []\n",
        "for question in test_ques:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in vocab_to_int:\n",
        "            ints.append(vocab_to_int['<UNK>'])\n",
        "        else:\n",
        "            ints.append(vocab_to_int[word])\n",
        "    ques_int_test.append(ints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lj84YlHHTHi"
      },
      "source": [
        "encoder_input_data_test = pad_sequences(ques_int_test, maxlen=max_line_length, value=vocab_to_int['<PAD>'], padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GyEwkZWHVjC"
      },
      "source": [
        "preds=[]\n",
        "for i in encoder_input_data_test:\n",
        "  input_seq=i\n",
        "  decoded_sentence=decode_sequence_faq(input_seq)\n",
        "  preds.append(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHHkLATTHo1R",
        "outputId": "ca7bd986-48a6-4e6b-ca16-d234f3e59d8f"
      },
      "source": [
        "test['Anwers']=preds\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Anwers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "      <td>the &lt;UNK&gt; is a violation of our code conduct ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "      <td>least sir the hell posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "      <td>least sir the hell posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "      <td>the long comes &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "      <td>you is your violation of the reading it will ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Why is the auto-grader failing me?</td>\n",
              "      <td>i will be posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Do I do the exercises individually?</td>\n",
              "      <td>i will be posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Is the lab compulsory?</td>\n",
              "      <td>least a violation of our code conduct &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Will the sessions be recorded?</td>\n",
              "      <td>the &lt;UNK&gt; is a violation of our code conduct ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Can I have access to the recorded videos?</td>\n",
              "      <td>the homework release is subject to change and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Where are the recordings?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Where can I ask questions regarding reading ma...</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Where to find course material?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Do we have homework?</td>\n",
              "      <td>i will be posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Where can I find the homework?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Where to submit the homework?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Where do I find the homework?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Where do I submit my homework assignment?</td>\n",
              "      <td>yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Will the professor take office hours?</td>\n",
              "      <td>the &lt;UNK&gt; is a violation of our code conduct ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What is the OH zoom link?</td>\n",
              "      <td>least sir the hell posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What will we do in projects?</td>\n",
              "      <td>least sir the hell posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What should be the duration of the presentatio...</td>\n",
              "      <td>least sir the hell posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Do I need to submit the project in a group?</td>\n",
              "      <td>i will be posted on edstem &lt;EOS&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Does attendance count to grading?</td>\n",
              "      <td>the long team truth is to submit the homework...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Questions                                             Anwers\n",
              "0             Will the pre-class session be recorded?   the <UNK> is a violation of our code conduct ...\n",
              "1           What is the deadline for quiz submission?          least sir the hell posted on edstem <EOS>\n",
              "2       What is the deadline for exercise submission?          least sir the hell posted on edstem <EOS>\n",
              "3   How many hours do I need to complete this course?                               the long comes <EOS>\n",
              "4                        Who will grade the exercise?   you is your violation of the reading it will ...\n",
              "5                  Why is the auto-grader failing me?                   i will be posted on edstem <EOS>\n",
              "6                 Do I do the exercises individually?                   i will be posted on edstem <EOS>\n",
              "7                              Is the lab compulsory?        least a violation of our code conduct <EOS>\n",
              "8                      Will the sessions be recorded?   the <UNK> is a violation of our code conduct ...\n",
              "9           Can I have access to the recorded videos?   the homework release is subject to change and...\n",
              "10                          Where are the recordings?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "11  Where can I ask questions regarding reading ma...   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "12                     Where to find course material?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "13                               Do we have homework?                   i will be posted on edstem <EOS>\n",
              "14                     Where can I find the homework?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "15                      Where to submit the homework?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "16                      Where do I find the homework?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "17          Where do I submit my homework assignment?   yeah am yeah oh oh oh oh oh oh oh oh oh oh oh...\n",
              "18              Will the professor take office hours?   the <UNK> is a violation of our code conduct ...\n",
              "19                          What is the OH zoom link?          least sir the hell posted on edstem <EOS>\n",
              "20                       What will we do in projects?          least sir the hell posted on edstem <EOS>\n",
              "21  What should be the duration of the presentatio...          least sir the hell posted on edstem <EOS>\n",
              "22        Do I need to submit the project in a group?                   i will be posted on edstem <EOS>\n",
              "23                  Does attendance count to grading?   the long team truth is to submit the homework..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEUNhgB7PZ_E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}