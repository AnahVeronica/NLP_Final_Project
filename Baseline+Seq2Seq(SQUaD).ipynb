{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline+Seq2Seq(SQUaD).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNoaUGGJV8JF"
      },
      "source": [
        "# Loading Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUayzFTZbnjO",
        "outputId": "b9defef8-05c4-463d-8cbc-5ba7493e0cf5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gensim\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvp84QtjV8JH"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import dot\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LALpNTSQbnjR"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras_preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nmD4B3ebnjR"
      },
      "source": [
        "import requests\n",
        "from gensim.models import Word2Vec\n",
        "from keras import Input, Model\n",
        "from keras.activations import softmax\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91QnCgqbnjS"
      },
      "source": [
        "# Loading Fine-Tune Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH9vdkyabnjS",
        "outputId": "df081eaf-b2df-4784-884d-bef6e0839571"
      },
      "source": [
        "df_finetune = pd.read_csv(\"/Users/shivanibalaji/Downloads/faq.csv\")\n",
        "df_finetune.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Are session pre-class quizzes graded?</td>\n",
              "      <td>No. Pre-class quiz is to just check your under...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When is the deadline for the submission of ses...</td>\n",
              "      <td>5 PM on the day of the following lecture.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Will there be any extension allowed for the qu...</td>\n",
              "      <td>Only for exceptional cases.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours will I need to dedicate to succ...</td>\n",
              "      <td>About 15 hours per week.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade my exercise?</td>\n",
              "      <td>The exercises are auto-graded once you click t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question  \\\n",
              "0              Are session pre-class quizzes graded?   \n",
              "1  When is the deadline for the submission of ses...   \n",
              "2  Will there be any extension allowed for the qu...   \n",
              "3  How many hours will I need to dedicate to succ...   \n",
              "4                        Who will grade my exercise?   \n",
              "\n",
              "                                              Answer  Unnamed: 2  Unnamed: 3  \n",
              "0  No. Pre-class quiz is to just check your under...         NaN         NaN  \n",
              "1          5 PM on the day of the following lecture.         NaN         NaN  \n",
              "2                        Only for exceptional cases.         NaN         NaN  \n",
              "3                           About 15 hours per week.         NaN         NaN  \n",
              "4  The exercises are auto-graded once you click t...         NaN         NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVxpUWXbnjT",
        "outputId": "ee552f72-cf95-45df-dd3a-e967fd9454b9"
      },
      "source": [
        "len(df_finetune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyYY50hmbnjT"
      },
      "source": [
        "# Train Dataset - SQUaD "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YwziYvcV8JJ"
      },
      "source": [
        "#  SQUAD Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_UpoQQ0V8JJ"
      },
      "source": [
        "train = pd.read_json(\"train-v2.0.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCTI2FkNV8JN"
      },
      "source": [
        "### Json to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NPh1_k6V8JO"
      },
      "source": [
        "title = []\n",
        "context = []\n",
        "questions = []\n",
        "answers_text = []\n",
        "answers_start = []\n",
        "id_num = []\n",
        "is_impossible_list = []\n",
        "plausible_answers_list = []\n",
        "plausible_answers_start_list= []\n",
        "\n",
        "for i in range(train.shape[0]):      # all titles(442)\n",
        "    title_name = train.data[i]['title']     \n",
        "\n",
        "    for j in range(len(train.data[i]['paragraphs'])): #number of paragraphs in title\n",
        "        context_name = train.data[i]['paragraphs'][j]['context']\n",
        "        \n",
        "        for k in range(len(train.data[i]['paragraphs'][j]['qas'])): #questions for each paragraph\n",
        "\n",
        "            question_name =  train.data[i]['paragraphs'][j]['qas'][k]['question']\n",
        "            id_number =    train.data[i]['paragraphs'][j]['qas'][k]['id']\n",
        "            \n",
        "            is_impossible = train.data[i]['paragraphs'][j]['qas'][k]['is_impossible']\n",
        "            \n",
        "            if is_impossible == True:\n",
        "                p_answers_text_name = train.data[i]['paragraphs'][j]['qas'][k]['plausible_answers'][0]['text']\n",
        "                p_answers_text_start = train.data[i]['paragraphs'][j]['qas'][k]['plausible_answers'][0]['answer_start']\n",
        "                \n",
        "            elif is_impossible == False:\n",
        "                p_answers_text_name = 'n/a'\n",
        "                p_answers_text_start = 'n/a'\n",
        "                \n",
        "            \n",
        "            \n",
        "            if train.data[i]['paragraphs'][j]['qas'][k]['answers'] != []:\n",
        "                answers_text_name = train.data[i]['paragraphs'][j]['qas'][k]['answers'][0]['text']\n",
        "                answers_start_num = train.data[i]['paragraphs'][j]['qas'][k]['answers'][0]['answer_start']\n",
        "            elif train.data[i]['paragraphs'][j]['qas'][k]['answers'] == []:\n",
        "                answers_text_name = 'n/a'\n",
        "                answers_start_num = 'n/a'\n",
        "            \n",
        "\n",
        "            title.append(title_name)\n",
        "            context.append(context_name)\n",
        "            questions.append(question_name)\n",
        "            answers_text.append(answers_text_name)\n",
        "            answers_start.append(answers_start_num)\n",
        "            id_num.append(id_number)\n",
        "            is_impossible_list.append(is_impossible)\n",
        "            plausible_answers_list.append(p_answers_text_name)\n",
        "            plausible_answers_start_list.append(p_answers_text_start)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hn3Mo2QbnjW",
        "outputId": "66bf597b-2921-403b-9984-57959c40bb2f"
      },
      "source": [
        "plausible_answers_list[130314]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'matter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGXeqFEzbnjW",
        "outputId": "2b9ac044-64bd-4049-8d9b-89ff32259413"
      },
      "source": [
        "train.data[441]['paragraphs'][1]['qas'][0]['plausible_answers'][0]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a cloud'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IryG4gd2V8JQ"
      },
      "source": [
        "squad = pd.DataFrame({\"title\":title, \"context\": context, \"questions\": questions, \"answers_text\": answers_text,\"answers_start\":answers_start, \"id_num\":id_num, \"is_impossible\":is_impossible_list, \"plausible_answer\":plausible_answers_list, \"plausible_answer_start\":plausible_answers_start_list  })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ro7XoPDxV8JS",
        "outputId": "774dc03b-43ad-43a8-ebad-de009c346965"
      },
      "source": [
        "squad.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>questions</th>\n",
              "      <th>answers_text</th>\n",
              "      <th>answers_start</th>\n",
              "      <th>id_num</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130314</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Physics has broadly agreed on the definition o...</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>5a7e070b70df9f001a875439</td>\n",
              "      <td>True</td>\n",
              "      <td>matter</td>\n",
              "      <td>485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130315</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Who coined the term partonic matter?</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>5a7e070b70df9f001a87543a</td>\n",
              "      <td>True</td>\n",
              "      <td>Alfvén</td>\n",
              "      <td>327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130316</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What is another name for anti-matter?</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>5a7e070b70df9f001a87543b</td>\n",
              "      <td>True</td>\n",
              "      <td>Gk. common matter</td>\n",
              "      <td>350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130317</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Matter usually does not need to be used in con...</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>5a7e070b70df9f001a87543c</td>\n",
              "      <td>True</td>\n",
              "      <td>a specifying modifier</td>\n",
              "      <td>529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130318</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What field of study has a variety of unusual c...</td>\n",
              "      <td>n/a</td>\n",
              "      <td>n/a</td>\n",
              "      <td>5a7e070b70df9f001a87543d</td>\n",
              "      <td>True</td>\n",
              "      <td>physics</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         title                                            context  \\\n",
              "130314  Matter  The term \"matter\" is used throughout physics i...   \n",
              "130315  Matter  The term \"matter\" is used throughout physics i...   \n",
              "130316  Matter  The term \"matter\" is used throughout physics i...   \n",
              "130317  Matter  The term \"matter\" is used throughout physics i...   \n",
              "130318  Matter  The term \"matter\" is used throughout physics i...   \n",
              "\n",
              "                                                questions answers_text  \\\n",
              "130314  Physics has broadly agreed on the definition o...          n/a   \n",
              "130315               Who coined the term partonic matter?          n/a   \n",
              "130316              What is another name for anti-matter?          n/a   \n",
              "130317  Matter usually does not need to be used in con...          n/a   \n",
              "130318  What field of study has a variety of unusual c...          n/a   \n",
              "\n",
              "       answers_start                    id_num  is_impossible  \\\n",
              "130314           n/a  5a7e070b70df9f001a875439           True   \n",
              "130315           n/a  5a7e070b70df9f001a87543a           True   \n",
              "130316           n/a  5a7e070b70df9f001a87543b           True   \n",
              "130317           n/a  5a7e070b70df9f001a87543c           True   \n",
              "130318           n/a  5a7e070b70df9f001a87543d           True   \n",
              "\n",
              "             plausible_answer plausible_answer_start  \n",
              "130314                 matter                    485  \n",
              "130315                 Alfvén                    327  \n",
              "130316      Gk. common matter                    350  \n",
              "130317  a specifying modifier                    529  \n",
              "130318                physics                     37  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1iFyXTdbnjZ"
      },
      "source": [
        "squad.to_csv(r'Downloads\\SQUaD4BERT.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMYzBFt5bnjZ"
      },
      "source": [
        "squad_small= squad[:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8V0VJnDbnja"
      },
      "source": [
        "squad_small.to_csv(r'Downloads\\SQUaD4seq2seq.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRStUDplbnja"
      },
      "source": [
        "### Removing Impossible Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw6w5HVPV8JT",
        "outputId": "c0812654-e295-4394-a108-ec31f67472a4"
      },
      "source": [
        "squad['is_impossible'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    86821\n",
              "True     43498\n",
              "Name: is_impossible, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvnOea7wV8JU"
      },
      "source": [
        "squad_new = squad[['questions', 'answers_text']]\n",
        "squad_new = squad_new[squad_new.answers_text != 'n/a']\n",
        "squad_new = squad_new.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YjD2Nzhbnjb"
      },
      "source": [
        "### Subsetting Data (due computational constraints)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nunYN-dQV8JV"
      },
      "source": [
        "squad_new = squad_new[:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "wvTPo4AqV8JW",
        "outputId": "1e928a68-ebe4-45dc-dd69-e594602b5e56"
      },
      "source": [
        "squad_new.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>answers_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>The 1080i30 or 1080i60 notion identifies inter...</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>The 720p60 notion identifies progressive scann...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>What three scanning rates do 50 Hz systems sup...</td>\n",
              "      <td>50i, 25p and 50p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>Which system suports 59.94i, 60i, 23.976p, 24p...</td>\n",
              "      <td>60 Hz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>SD television fractional rates were often roun...</td>\n",
              "      <td>whole numbers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               questions      answers_text\n",
              "19995  The 1080i30 or 1080i60 notion identifies inter...                30\n",
              "19996  The 720p60 notion identifies progressive scann...                60\n",
              "19997  What three scanning rates do 50 Hz systems sup...  50i, 25p and 50p\n",
              "19998  Which system suports 59.94i, 60i, 23.976p, 24p...             60 Hz\n",
              "19999  SD television fractional rates were often roun...     whole numbers"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr-w2RQrV8JW"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHWruCx8bnjd"
      },
      "source": [
        "### Defining Inputs & Targets for Train, Fine-Tune and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ-fk_ODbnjd"
      },
      "source": [
        "questions_train = [quest for quest in squad_new.questions]\n",
        "answers_train = [ans for ans in squad_new.answers_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pfuzxqobnje"
      },
      "source": [
        "questions_ft = [quest for quest in df_finetune.Question]\n",
        "answers_ft = [ans for ans in df_finetune.Answer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agfMpSuyV8JX"
      },
      "source": [
        "### Cleaning + Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzUFtHBybnje"
      },
      "source": [
        "def clean_data(sentence_list):\n",
        "    \n",
        "    clean_sent = [re.sub(\"[^a-zA-Z0-9 ]\", \"\", i) for i in sentence_list]\n",
        "    clean_sent = [word.lower() for word in clean_sent]\n",
        "\n",
        "    return clean_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPLRFSBhbnjf"
      },
      "source": [
        "clean_question_train = clean_data(questions_train)\n",
        "clean_answer_train = clean_data(answers_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLoej1D1bnjf"
      },
      "source": [
        "clean_question_ft = clean_data(questions_ft)\n",
        "clean_answer_ft = clean_data(answers_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqEzWUJ8bnjg"
      },
      "source": [
        "### Lematization + stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SDNzqdbnjg"
      },
      "source": [
        "#stopwords_set = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz3gaCofbnjh"
      },
      "source": [
        "#lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXjgkYabbnjh"
      },
      "source": [
        "def lemma_stop_words(sentence_list):\n",
        "    lemma_sent = [word_tokenize(i) for i in sentence_list]\n",
        "    lemma_sent = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in lemma_sent]\n",
        "    lemma_sent = [[i for i in l if i not in stopwords_set] for l in lemma_sent]\n",
        "    lemma_sent = [(\" \").join(sent) for sent in lemma_sent]\n",
        "    \n",
        "    return lemma_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgKjb0pNV8Ja"
      },
      "source": [
        "#clean_question_train = lemma_stop_words(clean_question_train)\n",
        "#clean_answer_train = lemma_stop_words(clean_answer_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCDSO4lRbnji"
      },
      "source": [
        "#clean_question_ft = lemma_stop_words(clean_question_ft)\n",
        "#clean_answer_ft = lemma_stop_words(clean_answer_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHfL_0gMbnji"
      },
      "source": [
        "#clean_questions_test = lemma_stop_words(clean_questions_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4re8A4pV8Jb"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUWGgGMIbnji"
      },
      "source": [
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSGltc1Fbnjj"
      },
      "source": [
        "tokenized_questions_train = [word_tokenize(i) for i in clean_question_train]\n",
        "tokenized_answers_train = [word_tokenize(i) for i in clean_answer_train]\n",
        "\n",
        "tokenized_questions_ft = [word_tokenize(i) for i in clean_question_ft]\n",
        "tokenized_answers_ft = [word_tokenize(i) for i in clean_answer_ft]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOXvaLOVbnjj"
      },
      "source": [
        "#tokenizer = Tokenizer()\n",
        "#tokenizer.fit_on_texts(clean_question_train + clean_answer_train + clean_question_ft + clean_answer_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7nKNsvbnjj"
      },
      "source": [
        "#tokenized_questions_train = tokenizer.texts_to_sequences(clean_question_train)\n",
        "#tokenized_answers_train = tokenizer.texts_to_sequences(clean_answer_train)\n",
        "\n",
        "#tokenized_questions_ft = tokenizer.texts_to_sequences(clean_question_ft)\n",
        "#tokenized_answers_ft = tokenizer.texts_to_sequences(clean_answer_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO4aHFE7bnjj"
      },
      "source": [
        "all_words_train = tokenized_questions_train + tokenized_answers_train  \n",
        "all_words_ft = tokenized_questions_ft + tokenized_answers_ft\n",
        "\n",
        "all_words_train = list(np.concatenate(all_words_train).flat)\n",
        "all_words_ft = list(np.concatenate(all_words_ft).flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsQ8jpo_bnjk"
      },
      "source": [
        "unique_words_train = list(set(all_words_train))\n",
        "unique_words_ft = list(set(all_words_ft))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIpwZoLhbnjk",
        "outputId": "5c665911-0961-4509-ad66-2b0be9fa5831"
      },
      "source": [
        "print(\"Unique words for train = \", len(unique_words_train))\n",
        "print(\"Unique words for fine tune = \", len(unique_words_ft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words for train =  21629\n",
            "Unique words for fine tune =  240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj4-LRqXbnjk"
      },
      "source": [
        "### Creating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWFqKiUCbnjl",
        "outputId": "83352c88-0fac-4985-d7d2-cd7a875100e4"
      },
      "source": [
        "vocab= [word for word, word_count in Counter(unique_words_train).most_common(6000)]\n",
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9jBPFTfbnjl"
      },
      "source": [
        "total_vocab = vocab + unique_words_ft\n",
        "total_vocab= list(set(total_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggq01-VObnjl",
        "outputId": "ca8dac6a-6b69-482b-a3c3-37d8484b5e21"
      },
      "source": [
        "print(\"Vocab size = \", len(total_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size =  6186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMhqp7yWbnjl"
      },
      "source": [
        "word2idx = {}\n",
        "idx2word = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baeE1ayWbnjm"
      },
      "source": [
        "i = 4\n",
        "for word in total_vocab:\n",
        "    if word not in word2idx:\n",
        "        word2idx[word] = i\n",
        "        idx2word[i] = word\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPpOLkNSbnjm"
      },
      "source": [
        "idx2word[0]= 'padding'\n",
        "idx2word[1]= 'unk'\n",
        "idx2word[2] = '<s>'\n",
        "idx2word[3] = '</s>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qot_rIODbnjm"
      },
      "source": [
        "word2idx['padding']= 0\n",
        "word2idx['unk']= 1\n",
        "word2idx['<s>'] = 2\n",
        "word2idx['</s>'] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy6N2MmPbnjm"
      },
      "source": [
        "vocab_size = len(word2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPz7o49Pbnjn",
        "outputId": "fcb038a6-9e34-445f-9337-f92010ab9af9"
      },
      "source": [
        "print(\"Vocabulary Size:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 6190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5kvsl3_bnjn"
      },
      "source": [
        "### Adding Start and End Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB2ChrZWbnjn"
      },
      "source": [
        "def start_end(token_sentence_list):\n",
        "    for sent in token_sentence_list:\n",
        "        sent.append('</s>')\n",
        "        sent.insert(0,'<s>')\n",
        "    return token_sentence_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5ys-YJbnjn"
      },
      "source": [
        "tokenized_answers_ft = start_end(tokenized_answers_ft)\n",
        "tokenized_answers_train = start_end(tokenized_answers_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47IG6TAbnjo"
      },
      "source": [
        "### Enocde Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwc234UCbnjo"
      },
      "source": [
        "def encode(sentence_list):\n",
        "    for i in range(len(sentence_list)):\n",
        "        sentence = []\n",
        "        \n",
        "        for word in sentence_list[i]:\n",
        "            \n",
        "            if word in word2idx.keys():\n",
        "                word = word2idx[word]\n",
        "            else: \n",
        "                word = word2idx['unk']\n",
        "                \n",
        "            sentence.append(word)\n",
        "        \n",
        "        sentence_list[i] = sentence\n",
        "        \n",
        "    return sentence_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJw7NQ7vbnjo"
      },
      "source": [
        "encoded_questions_train = encode(tokenized_questions_train)\n",
        "encoded_answers_train = encode(tokenized_answers_train)\n",
        "\n",
        "encoded_questions_ft = encode(tokenized_questions_ft)\n",
        "encoded_answers_ft = encode(tokenized_answers_ft)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPnhFBTQbnjo"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJXr3_9ibnjp"
      },
      "source": [
        "maxlen_questions = max(max([len(x) for x in encoded_questions_train]), max([len(x) for x in encoded_questions_ft]))\n",
        "maxlen_answers = max(max([len(x) for x in encoded_answers_train]), max([len(x) for x in encoded_answers_ft]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgA0REKHbnjp"
      },
      "source": [
        "maxlen = max(maxlen_questions, maxlen_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn_EcmWlbnjp"
      },
      "source": [
        "X_pad_train= pad_sequences(encoded_questions_train, maxlen=maxlen, padding='post', value = word2idx['padding'])\n",
        "X_pad_ft = pad_sequences(encoded_questions_ft, maxlen=maxlen, padding='post', value = word2idx['padding'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n-gqfGWbnjq"
      },
      "source": [
        "Y_pad_train= pad_sequences(encoded_answers_train, maxlen=maxlen, padding='post', value = word2idx['padding'])\n",
        "Y_pad_ft = pad_sequences(encoded_answers_ft, maxlen=maxlen, padding='post', value = word2idx['padding'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCPdm_xebnjq",
        "outputId": "08059380-0f3d-4043-fa3d-04bbc46f1c21"
      },
      "source": [
        "maxlen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpHUXbt_V8J3"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QiItOf-bnjr"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsc9n9HAbnjr"
      },
      "source": [
        "Define and train a simple RNN network with 2 RNN layers as your baseline model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIP9mJLhV8J5"
      },
      "source": [
        "Y_pad_train_m1 = [to_categorical(j, num_classes=vocab_size) for j in Y_pad_train]\n",
        "Y_pad_ft_m1 = [to_categorical(j, num_classes=vocab_size) for j in Y_pad_ft]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv-ZQJd0V8J9"
      },
      "source": [
        "embedding_size = 10\n",
        "vocab_size = len(word2idx)\n",
        "maxlen = maxlen\n",
        "\n",
        "\n",
        "model_baseline = Sequential()\n",
        "\n",
        "model_baseline.add(Embedding(vocab_size, embedding_size, \\\n",
        "                    input_length=maxlen))\n",
        "\n",
        "model_baseline.add(SimpleRNN(64, return_sequences = True))\n",
        "model_baseline.add(SimpleRNN(64, return_sequences = True))\n",
        "model_baseline.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model_baseline.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf21anXmV8J9",
        "outputId": "b79d0a86-1fc6-41aa-8c54-d6b557886c34"
      },
      "source": [
        "model_baseline.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 53, 10)            61900     \n",
            "_________________________________________________________________\n",
            "simple_rnn_8 (SimpleRNN)     (None, 53, 64)            4800      \n",
            "_________________________________________________________________\n",
            "simple_rnn_9 (SimpleRNN)     (None, 53, 64)            8256      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 53, 6190)          402350    \n",
            "=================================================================\n",
            "Total params: 477,306\n",
            "Trainable params: 477,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2_dANXV8KA"
      },
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2cjctl_V8KB",
        "outputId": "ab9f4735-f4cb-4a6f-977c-990e57ff5a26"
      },
      "source": [
        "#model_baseline.fit(X_pad_train, np.array(Y_pad_train_m1), epochs=50,  callbacks = callback, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "563/563 [==============================] - 116s 201ms/step - loss: 2.2438 - accuracy: 0.8959 - val_loss: 0.3172 - val_accuracy: 0.9439\n",
            "Epoch 2/50\n",
            "563/563 [==============================] - 127s 225ms/step - loss: 0.2406 - accuracy: 0.9555 - val_loss: 0.3075 - val_accuracy: 0.9432\n",
            "Epoch 3/50\n",
            "563/563 [==============================] - 120s 214ms/step - loss: 0.2326 - accuracy: 0.9559 - val_loss: 0.3038 - val_accuracy: 0.9440\n",
            "Epoch 4/50\n",
            "563/563 [==============================] - 120s 214ms/step - loss: 0.2256 - accuracy: 0.9570 - val_loss: 0.3018 - val_accuracy: 0.9448\n",
            "Epoch 5/50\n",
            "563/563 [==============================] - 122s 216ms/step - loss: 0.2223 - accuracy: 0.9568 - val_loss: 0.2983 - val_accuracy: 0.9447\n",
            "Epoch 6/50\n",
            "563/563 [==============================] - 123s 219ms/step - loss: 0.2169 - accuracy: 0.9568 - val_loss: 0.3054 - val_accuracy: 0.9451\n",
            "Epoch 7/50\n",
            "563/563 [==============================] - 123s 219ms/step - loss: 0.2136 - accuracy: 0.9575 - val_loss: 0.3072 - val_accuracy: 0.9448\n",
            "Epoch 8/50\n",
            "563/563 [==============================] - 123s 219ms/step - loss: 0.2121 - accuracy: 0.9579 - val_loss: 0.3066 - val_accuracy: 0.9451\n",
            "Epoch 9/50\n",
            "563/563 [==============================] - 116s 205ms/step - loss: 0.2137 - accuracy: 0.9578 - val_loss: 0.3128 - val_accuracy: 0.9449\n",
            "Epoch 10/50\n",
            "563/563 [==============================] - 113s 201ms/step - loss: 0.2082 - accuracy: 0.9581 - val_loss: 0.3219 - val_accuracy: 0.9448\n",
            "Epoch 11/50\n",
            "563/563 [==============================] - 104s 185ms/step - loss: 0.2068 - accuracy: 0.9583 - val_loss: 0.3129 - val_accuracy: 0.9449\n",
            "Epoch 12/50\n",
            "563/563 [==============================] - 104s 185ms/step - loss: 0.2009 - accuracy: 0.9590 - val_loss: 0.3169 - val_accuracy: 0.9444\n",
            "Epoch 13/50\n",
            "563/563 [==============================] - 105s 186ms/step - loss: 0.2064 - accuracy: 0.9583 - val_loss: 0.3340 - val_accuracy: 0.9443\n",
            "Epoch 14/50\n",
            "563/563 [==============================] - 104s 185ms/step - loss: 0.2031 - accuracy: 0.9585 - val_loss: 0.3379 - val_accuracy: 0.9443\n",
            "Epoch 15/50\n",
            "563/563 [==============================] - 105s 186ms/step - loss: 0.1987 - accuracy: 0.9591 - val_loss: 0.3350 - val_accuracy: 0.9432\n",
            "Epoch 16/50\n",
            "563/563 [==============================] - 106s 189ms/step - loss: 0.1999 - accuracy: 0.9588 - val_loss: 0.3268 - val_accuracy: 0.9442\n",
            "Epoch 17/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.2005 - accuracy: 0.9587 - val_loss: 0.3402 - val_accuracy: 0.9442\n",
            "Epoch 18/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1969 - accuracy: 0.9592 - val_loss: 0.3373 - val_accuracy: 0.9436\n",
            "Epoch 19/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1962 - accuracy: 0.9592 - val_loss: 0.3320 - val_accuracy: 0.9443\n",
            "Epoch 20/50\n",
            "563/563 [==============================] - 114s 202ms/step - loss: 0.1969 - accuracy: 0.9591 - val_loss: 0.3340 - val_accuracy: 0.9439\n",
            "Epoch 21/50\n",
            "563/563 [==============================] - 113s 201ms/step - loss: 0.1919 - accuracy: 0.9597 - val_loss: 0.3392 - val_accuracy: 0.9437\n",
            "Epoch 22/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1955 - accuracy: 0.9590 - val_loss: 0.3369 - val_accuracy: 0.9441\n",
            "Epoch 23/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1933 - accuracy: 0.9596 - val_loss: 0.3410 - val_accuracy: 0.9429\n",
            "Epoch 24/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1931 - accuracy: 0.9595 - val_loss: 0.3436 - val_accuracy: 0.9437\n",
            "Epoch 25/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1936 - accuracy: 0.9594 - val_loss: 0.3541 - val_accuracy: 0.9439\n",
            "Epoch 26/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1909 - accuracy: 0.9596 - val_loss: 0.3447 - val_accuracy: 0.9436\n",
            "Epoch 27/50\n",
            "563/563 [==============================] - 116s 206ms/step - loss: 0.1894 - accuracy: 0.9599 - val_loss: 0.3522 - val_accuracy: 0.9427\n",
            "Epoch 28/50\n",
            "563/563 [==============================] - 114s 203ms/step - loss: 0.1887 - accuracy: 0.9600 - val_loss: 0.3494 - val_accuracy: 0.9438\n",
            "Epoch 29/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1880 - accuracy: 0.9604 - val_loss: 0.3489 - val_accuracy: 0.9432\n",
            "Epoch 30/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1884 - accuracy: 0.9598 - val_loss: 0.3505 - val_accuracy: 0.9428\n",
            "Epoch 31/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1847 - accuracy: 0.9609 - val_loss: 0.3475 - val_accuracy: 0.9431\n",
            "Epoch 32/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1888 - accuracy: 0.9594 - val_loss: 0.3523 - val_accuracy: 0.9436\n",
            "Epoch 33/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1848 - accuracy: 0.9605 - val_loss: 0.3487 - val_accuracy: 0.9435\n",
            "Epoch 34/50\n",
            "563/563 [==============================] - 116s 206ms/step - loss: 0.1829 - accuracy: 0.9606 - val_loss: 0.3559 - val_accuracy: 0.9427\n",
            "Epoch 35/50\n",
            "563/563 [==============================] - 116s 206ms/step - loss: 0.1833 - accuracy: 0.9609 - val_loss: 0.3649 - val_accuracy: 0.9377\n",
            "Epoch 36/50\n",
            "563/563 [==============================] - 116s 207ms/step - loss: 0.1790 - accuracy: 0.9612 - val_loss: 0.3563 - val_accuracy: 0.9424\n",
            "Epoch 37/50\n",
            "563/563 [==============================] - 115s 204ms/step - loss: 0.1788 - accuracy: 0.9614 - val_loss: 0.3542 - val_accuracy: 0.9432\n",
            "Epoch 38/50\n",
            "563/563 [==============================] - 115s 203ms/step - loss: 0.1811 - accuracy: 0.9610 - val_loss: 0.3565 - val_accuracy: 0.9424\n",
            "Epoch 39/50\n",
            "563/563 [==============================] - 115s 205ms/step - loss: 0.1788 - accuracy: 0.9614 - val_loss: 0.3580 - val_accuracy: 0.9429\n",
            "Epoch 40/50\n",
            "563/563 [==============================] - 116s 206ms/step - loss: 0.1817 - accuracy: 0.9608 - val_loss: 0.3598 - val_accuracy: 0.9426\n",
            "Epoch 41/50\n",
            "563/563 [==============================] - 115s 205ms/step - loss: 0.1792 - accuracy: 0.9611 - val_loss: 0.3539 - val_accuracy: 0.9430\n",
            "Epoch 42/50\n",
            "563/563 [==============================] - 118s 210ms/step - loss: 0.1814 - accuracy: 0.9608 - val_loss: 0.3573 - val_accuracy: 0.9435\n",
            "Epoch 43/50\n",
            "563/563 [==============================] - 121s 215ms/step - loss: 0.1749 - accuracy: 0.9617 - val_loss: 0.3603 - val_accuracy: 0.9425\n",
            "Epoch 44/50\n",
            "563/563 [==============================] - 117s 208ms/step - loss: 0.1760 - accuracy: 0.9615 - val_loss: 0.3574 - val_accuracy: 0.9432\n",
            "Epoch 45/50\n",
            "563/563 [==============================] - 116s 206ms/step - loss: 0.1771 - accuracy: 0.9612 - val_loss: 0.3635 - val_accuracy: 0.9428\n",
            "Epoch 46/50\n",
            "563/563 [==============================] - 113s 200ms/step - loss: 0.1735 - accuracy: 0.9616 - val_loss: 0.3627 - val_accuracy: 0.9421\n",
            "Epoch 47/50\n",
            "563/563 [==============================] - 119s 212ms/step - loss: 0.1777 - accuracy: 0.9610 - val_loss: 0.3637 - val_accuracy: 0.9426\n",
            "Epoch 48/50\n",
            "563/563 [==============================] - 128s 227ms/step - loss: 0.1745 - accuracy: 0.9616 - val_loss: 0.3639 - val_accuracy: 0.9428\n",
            "Epoch 49/50\n",
            "563/563 [==============================] - 119s 212ms/step - loss: 0.1742 - accuracy: 0.9616 - val_loss: 0.3667 - val_accuracy: 0.9432\n",
            "Epoch 50/50\n",
            "563/563 [==============================] - 122s 217ms/step - loss: 0.1711 - accuracy: 0.9620 - val_loss: 0.3617 - val_accuracy: 0.9424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5f69c3610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtnJ66C-V8KC",
        "outputId": "6463e9ab-ff3a-4cc6-8e26-002a45c8e598"
      },
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_baseline.save('model_baseline')\n",
        "\n",
        "with open('shak-nlg-dict.pkl', 'wb') as handle:\n",
        "    pickle.dump(word2idx, handle)\n",
        "\n",
        "with open('shak-nlg-maxlen.pkl', 'wb') as handle:\n",
        "    pickle.dump(maxlen, handle)\n",
        "print(\"Model Saved!\") \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "INFO:tensorflow:Assets written to: model_baseline/assets\n",
            "Model Saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNLtunmIbnjt"
      },
      "source": [
        "#model_baseline.save_weights('model_baseline_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8k-jIZVV8KC"
      },
      "source": [
        "#import pickle\n",
        "#import keras\n",
        "\n",
        "model_baseline = keras.models.load_model('model_baseline')\n",
        "maxlen = pickle.load(open('shak-nlg-maxlen.pkl', 'rb'))\n",
        "word_indexes = pickle.load(open('shak-nlg-dict.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmil7Hyrbnjt"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF3uHNlpbnju"
      },
      "source": [
        "def predict(sentence, model): \n",
        "    \n",
        "    #preprocessing\n",
        "    clean_sent = re.sub(\"[^a-zA-Z0-9 ]\", \"\", sentence)\n",
        "    clean_sent = clean_sent.lower()\n",
        "    \n",
        "    #tokenize\n",
        "    token_sent = word_tokenize(clean_sent)\n",
        "\n",
        "    #encode\n",
        "    token_sent_new = []\n",
        "    \n",
        "    for word in token_sent:\n",
        "        if word in word2idx.keys():\n",
        "            word = word2idx[word]\n",
        "        else: \n",
        "            word = word2idx['unk']\n",
        "        \n",
        "        token_sent_new.append(word)\n",
        "            \n",
        "    #padding\n",
        "    sent_padded = pad_sequences([token_sent_new], maxlen=maxlen, padding='post', value = word2idx['padding'])\n",
        "\n",
        "    \n",
        "    #prediction\n",
        "    prediction = np.argmax(model.predict(sent_padded), axis=2)\n",
        "    prediction = np.delete(prediction[0], np.where(prediction[0] == 0))\n",
        "    \n",
        "    #decode    \n",
        "    decoded_sentence = [idx2word[p] for p in prediction]\n",
        "    \n",
        "    #remove start/end tags\n",
        "    \n",
        "    if '<s>' in decoded_sentence: \n",
        "        decoded_sentence.remove('<s>')\n",
        "    if '</s>' in decoded_sentence: \n",
        "        decoded_sentence.remove('</s>')\n",
        "    \n",
        "    #final sentence\n",
        "    predicted_sentence = (\" \").join(decoded_sentence)\n",
        "    \n",
        "    \n",
        "    return predicted_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXzN6oscbnju",
        "outputId": "0e4b0231-41a4-4bab-f296-62e38afa72e0"
      },
      "source": [
        "predict('This is Beyonce', model_baseline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unk unk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFseIA0wbnjv"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNvJviVgbnjv"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC841ImHbnjw",
        "outputId": "f21c4bec-5b2d-4a2a-9ae2-f113b48821e0"
      },
      "source": [
        "embedding_size = 10\n",
        "vocab_size = len(word2idx)\n",
        "maxlen = maxlen\n",
        "\n",
        "\n",
        "ft_model_baseline = Sequential()\n",
        "\n",
        "ft_model_baseline.add(Embedding(vocab_size, embedding_size, \\\n",
        "                    input_length=maxlen))\n",
        "\n",
        "ft_model_baseline.add(SimpleRNN(64, return_sequences = True))\n",
        "ft_model_baseline.add(SimpleRNN(64, return_sequences = True))\n",
        "ft_model_baseline.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "\n",
        "ft_model_baseline.load_weights('model_baseline_weights.h5')\n",
        "ft_model_baseline.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "ft_model_baseline.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 53, 10)            61900     \n",
            "_________________________________________________________________\n",
            "simple_rnn_10 (SimpleRNN)    (None, 53, 64)            4800      \n",
            "_________________________________________________________________\n",
            "simple_rnn_11 (SimpleRNN)    (None, 53, 64)            8256      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 53, 6190)          402350    \n",
            "=================================================================\n",
            "Total params: 477,306\n",
            "Trainable params: 477,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip8sZjFSbnjw"
      },
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV-ey9yWbnjw",
        "outputId": "c94f6461-0115-4002-ab78-5ffba7405dec"
      },
      "source": [
        "ft_model_baseline.fit(X_pad_ft, np.array(Y_pad_ft_m1), epochs=100,  callbacks = callback )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 2s 38ms/step - loss: 3.6055 - accuracy: 0.7595\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 3.4183 - accuracy: 0.7584\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 3.2386 - accuracy: 0.7572\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 3.0671 - accuracy: 0.7555\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.9459 - accuracy: 0.7531\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.8072 - accuracy: 0.7536\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.7249 - accuracy: 0.7532\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.6616 - accuracy: 0.7456\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.5903 - accuracy: 0.7432\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.5684 - accuracy: 0.7299\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2.4883 - accuracy: 0.7489\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.4396 - accuracy: 0.7559\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.4003 - accuracy: 0.7586\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2.3907 - accuracy: 0.7576\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2.3609 - accuracy: 0.7576\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 2.3338 - accuracy: 0.7584\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2.2857 - accuracy: 0.7599\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 2.2487 - accuracy: 0.7593\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 2.2087 - accuracy: 0.7605\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 2.2057 - accuracy: 0.7601\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 2.1628 - accuracy: 0.7595\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 2.1259 - accuracy: 0.7611\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.0922 - accuracy: 0.7628\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 2.0681 - accuracy: 0.7616\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 2.0581 - accuracy: 0.7599\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 2.0343 - accuracy: 0.7582\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 2.0094 - accuracy: 0.7588\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.9868 - accuracy: 0.7578\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1.9822 - accuracy: 0.7560\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.9695 - accuracy: 0.7551\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.9485 - accuracy: 0.7551\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.9354 - accuracy: 0.7576\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.9003 - accuracy: 0.7609\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.8995 - accuracy: 0.7607\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.8885 - accuracy: 0.7610\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.8431 - accuracy: 0.7646\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.8483 - accuracy: 0.7628\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1.8320 - accuracy: 0.7634\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.8111 - accuracy: 0.7653\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 1.7955 - accuracy: 0.7647\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1.8083 - accuracy: 0.7628\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.7772 - accuracy: 0.7649\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1.7808 - accuracy: 0.7645\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 1.7579 - accuracy: 0.7657\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.7356 - accuracy: 0.7671\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 1.7296 - accuracy: 0.7651\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.7180 - accuracy: 0.7634\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.6971 - accuracy: 0.7628\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1.6795 - accuracy: 0.7640\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.6842 - accuracy: 0.7634\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.6588 - accuracy: 0.7641\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.6503 - accuracy: 0.7645\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.6355 - accuracy: 0.7657\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.6288 - accuracy: 0.7643\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.6298 - accuracy: 0.7633\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.6007 - accuracy: 0.7665\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.6073 - accuracy: 0.7637\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.5971 - accuracy: 0.7663\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.5824 - accuracy: 0.7663\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.5876 - accuracy: 0.7659\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.5634 - accuracy: 0.7686\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.5732 - accuracy: 0.7661\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.5578 - accuracy: 0.7661\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.5342 - accuracy: 0.7682\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.5223 - accuracy: 0.7682\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 1.5222 - accuracy: 0.7680\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.5013 - accuracy: 0.7692\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.4845 - accuracy: 0.7692\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.4838 - accuracy: 0.7692\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.4713 - accuracy: 0.7674\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.4570 - accuracy: 0.7657\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.4389 - accuracy: 0.7674\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.4188 - accuracy: 0.7649\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.4905 - accuracy: 0.7576\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1.4549 - accuracy: 0.7697\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1.4438 - accuracy: 0.7721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5f7edc310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZGivRtabnjx"
      },
      "source": [
        "## Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6vTm4-Xbnjx",
        "outputId": "09ac557a-6f1f-4173-87eb-87a8cef6fabc"
      },
      "source": [
        "df_test = pd.read_csv(\"/Users/shivanibalaji/Downloads/test-2.csv\")\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Questions\n",
              "0            Will the pre-class session be recorded?\n",
              "1          What is the deadline for quiz submission?\n",
              "2      What is the deadline for exercise submission?\n",
              "3  How many hours do I need to complete this course?\n",
              "4                       Who will grade the exercise?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCRTXP0abnjx"
      },
      "source": [
        "pred_test = []\n",
        "for q in df_test.Questions.values:\n",
        "    pred_test.append(predict(q, ft_model_baseline))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2YUF5sbnjx"
      },
      "source": [
        "predictions_test_data = pd.DataFrame()\n",
        "predictions_test_data['Questions'] = df_test.Questions.values\n",
        "predictions_test_data['Predicted_answers'] = pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1tmom51bnjy",
        "outputId": "d6f32afa-45cd-4ca1-828c-9ca9c1115e94"
      },
      "source": [
        "predictions_test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Predicted_answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "      <td>no your week will be the &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "      <td>unk official pythons be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "      <td>unk official pythons you will be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "      <td>unk unk the you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "      <td>the unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Why is the auto-grader failing me?</td>\n",
              "      <td>no a and you teams are &lt;s&gt; 4 the the the be &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Do I do the exercises individually?</td>\n",
              "      <td>not your the be your be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Is the lab compulsory?</td>\n",
              "      <td>all will you will be be week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Will the sessions be recorded?</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Can I have access to the recorded videos?</td>\n",
              "      <td>does &lt;s&gt; video the the be is be be be be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Where are the recordings?</td>\n",
              "      <td>all of will be be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Where can I ask questions regarding reading ma...</td>\n",
              "      <td>no of you will week on links &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Where to find course material?</td>\n",
              "      <td>all week week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Do we have homework?</td>\n",
              "      <td>unk official cases will be be be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Where can I find the homework?</td>\n",
              "      <td>no of cases will be be be be be &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Where to submit the homework?</td>\n",
              "      <td>all week the &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Where do I find the homework?</td>\n",
              "      <td>all your week will be be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Where do I submit my homework assignment?</td>\n",
              "      <td>all your week &lt;s&gt; the be be &lt;/s&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Will the professor take office hours?</td>\n",
              "      <td>no your and will</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What is the OH zoom link?</td>\n",
              "      <td>unk official &lt;s&gt; a week</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What will we do in projects?</td>\n",
              "      <td>unk unk is your be the be be be be be be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What should be the duration of the presentatio...</td>\n",
              "      <td>unk official will be be be will</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Do I need to submit the project in a group?</td>\n",
              "      <td>not your the week &lt;s&gt; unk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Does attendance count to grading?</td>\n",
              "      <td>emporia durand unk unk the you will be be be b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Questions  \\\n",
              "0             Will the pre-class session be recorded?   \n",
              "1           What is the deadline for quiz submission?   \n",
              "2       What is the deadline for exercise submission?   \n",
              "3   How many hours do I need to complete this course?   \n",
              "4                        Who will grade the exercise?   \n",
              "5                  Why is the auto-grader failing me?   \n",
              "6                 Do I do the exercises individually?   \n",
              "7                              Is the lab compulsory?   \n",
              "8                      Will the sessions be recorded?   \n",
              "9           Can I have access to the recorded videos?   \n",
              "10                          Where are the recordings?   \n",
              "11  Where can I ask questions regarding reading ma...   \n",
              "12                     Where to find course material?   \n",
              "13                               Do we have homework?   \n",
              "14                     Where can I find the homework?   \n",
              "15                      Where to submit the homework?   \n",
              "16                      Where do I find the homework?   \n",
              "17          Where do I submit my homework assignment?   \n",
              "18              Will the professor take office hours?   \n",
              "19                          What is the OH zoom link?   \n",
              "20                       What will we do in projects?   \n",
              "21  What should be the duration of the presentatio...   \n",
              "22        Do I need to submit the project in a group?   \n",
              "23                  Does attendance count to grading?   \n",
              "\n",
              "                                    Predicted_answers  \n",
              "0                       no your week will be the </s>  \n",
              "1                             unk official pythons be  \n",
              "2           unk official pythons you will be be be be  \n",
              "3                                     unk unk the you  \n",
              "4                                             the unk  \n",
              "5    no a and you teams are <s> 4 the the the be </s>  \n",
              "6                    not your the be your be be be be  \n",
              "7                        all will you will be be week  \n",
              "8                                                  no  \n",
              "9   does <s> video the the be is be be be be be be be  \n",
              "10                         all of will be be be be be  \n",
              "11                  no of you will week on links </s>  \n",
              "12                                      all week week  \n",
              "13          unk official cases will be be be be be be  \n",
              "14               no of cases will be be be be be </s>  \n",
              "15                                  all week the </s>  \n",
              "16                  all your week will be be be be be  \n",
              "17                   all your week <s> the be be </s>  \n",
              "18                                   no your and will  \n",
              "19                            unk official <s> a week  \n",
              "20           unk unk is your be the be be be be be be  \n",
              "21                    unk official will be be be will  \n",
              "22                          not your the week <s> unk  \n",
              "23  emporia durand unk unk the you will be be be b...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPP9ecMHNg8"
      },
      "source": [
        "# Seq 2 Seq Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKk8VBxpbnjy"
      },
      "source": [
        "clean_question_train = clean_data(questions_train)[:10000]\n",
        "clean_answer_train = clean_data(answers_train)[:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTQVWmTHbnjz"
      },
      "source": [
        "clean_question_ft = clean_data(questions_ft)\n",
        "clean_answer_ft = clean_data(answers_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-zaPHhdbnjz",
        "outputId": "6f6c43f3-c3e9-4d10-a31c-7a5124fc688e"
      },
      "source": [
        "target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n",
        "\n",
        "tokenizer = Tokenizer(filters=target_regex)\n",
        "tokenizer.fit_on_texts(clean_question_train + clean_answer_train +clean_question_ft +  clean_answer_ft)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size : 12497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9n3oRd7bnj0",
        "outputId": "b37db772-beeb-4e83-cadc-3fea94664218"
      },
      "source": [
        "tokenized_questions_train = tokenizer.texts_to_sequences(clean_question_train)\n",
        "\n",
        "maxlen_questions_train = max([len(x) for x in tokenized_questions_train])\n",
        "encoder_input_data_train = pad_sequences(tokenized_questions_train,\n",
        "                                   maxlen=maxlen_questions_train,\n",
        "                                   padding='post')\n",
        "\n",
        "print(encoder_input_data_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIEapXGJbnj0",
        "outputId": "473a3463-5349-4f5a-ceb8-d83ed3cbf95c"
      },
      "source": [
        "tokenized_answers_train = tokenizer.texts_to_sequences(clean_answer_train)\n",
        "\n",
        "maxlen_answers_train = max([len(x) for x in tokenized_answers_train])\n",
        "decoder_input_data_train = pad_sequences(tokenized_answers_train,\n",
        "                                   maxlen=maxlen_answers_train,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcjxwomnbnj1",
        "outputId": "f8dd4152-a305-43da-8e28-ad25dad1f3b8"
      },
      "source": [
        "for i in range(len(tokenized_answers_train)):\n",
        "    tokenized_answers_train[i] = tokenized_answers_train[i][1:]\n",
        "padded_answers_train = pad_sequences(tokenized_answers_train, maxlen=maxlen_answers_train, padding='post')\n",
        "decoder_output_data_train = to_categorical(padded_answers_train, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_output_data_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 43, 12497)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahLAU8Kabnj1",
        "outputId": "678c9835-2a28-4dfb-f2eb-3c4225983fbf"
      },
      "source": [
        "'''\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callback = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model.fit([encoder_input_data_train, decoder_input_data_train],\n",
        "          decoder_output_data_train,\n",
        "          batch_size=50,\n",
        "          epochs=100, \n",
        "          callbacks = callback, \n",
        "          validation_split = 0.2)\n",
        "model.save('model_seq2seq.h5')\n",
        "\n",
        "''' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, None, 200)    2499400     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, None, 200)    2499400     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_16 (LSTM)                  [(None, 200), (None, 320800      embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_17 (LSTM)                  [(None, None, 200),  320800      embedding_23[0][0]               \n",
            "                                                                 lstm_16[0][1]                    \n",
            "                                                                 lstm_16[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, None, 12497)  2511897     lstm_17[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 8,152,297\n",
            "Trainable params: 8,152,297\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "160/160 [==============================] - 124s 734ms/step - loss: 0.3132 - val_loss: 0.2814\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 107s 666ms/step - loss: 0.2432 - val_loss: 0.2825\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 107s 667ms/step - loss: 0.2381 - val_loss: 0.2835\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 106s 663ms/step - loss: 0.2329 - val_loss: 0.2839\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 107s 666ms/step - loss: 0.2298 - val_loss: 0.2841\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 103s 647ms/step - loss: 0.2164 - val_loss: 0.2879\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 104s 650ms/step - loss: 0.2185 - val_loss: 0.2880\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 103s 645ms/step - loss: 0.2023 - val_loss: 0.2919\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 106s 663ms/step - loss: 0.1942 - val_loss: 0.2953\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 109s 683ms/step - loss: 0.1915 - val_loss: 0.2984\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 108s 676ms/step - loss: 0.1834 - val_loss: 0.3018\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 104s 652ms/step - loss: 0.1811 - val_loss: 0.3139\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 106s 666ms/step - loss: 0.1699 - val_loss: 0.3096\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 104s 653ms/step - loss: 0.1745 - val_loss: 0.3189\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 106s 661ms/step - loss: 0.1620 - val_loss: 0.3196\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 104s 649ms/step - loss: 0.1607 - val_loss: 0.3176\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 104s 652ms/step - loss: 0.1512 - val_loss: 0.3267\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 105s 658ms/step - loss: 0.1477 - val_loss: 0.3296\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 104s 651ms/step - loss: 0.1407 - val_loss: 0.3315\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 107s 671ms/step - loss: 0.1351 - val_loss: 0.3332\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 126s 792ms/step - loss: 0.1309 - val_loss: 0.3403\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 125s 784ms/step - loss: 0.1258 - val_loss: 0.3433\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 116s 723ms/step - loss: 0.1214 - val_loss: 0.3501\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 110s 691ms/step - loss: 0.1141 - val_loss: 0.3474\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 117s 729ms/step - loss: 0.1114 - val_loss: 0.3523\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 113s 705ms/step - loss: 0.1089 - val_loss: 0.3531\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 108s 674ms/step - loss: 0.1035 - val_loss: 0.3564\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 112s 701ms/step - loss: 0.1014 - val_loss: 0.3576\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 114s 714ms/step - loss: 0.0944 - val_loss: 0.3579\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 116s 726ms/step - loss: 0.0919 - val_loss: 0.3591\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 116s 727ms/step - loss: 0.0870 - val_loss: 0.3666\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 132s 827ms/step - loss: 0.0872 - val_loss: 0.3651\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 114s 712ms/step - loss: 0.0827 - val_loss: 0.3691\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 117s 730ms/step - loss: 0.0797 - val_loss: 0.3669\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 126s 786ms/step - loss: 0.0774 - val_loss: 0.3700\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 131s 817ms/step - loss: 0.0715 - val_loss: 0.3732\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 135s 846ms/step - loss: 0.0725 - val_loss: 0.3774\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 134s 840ms/step - loss: 0.0668 - val_loss: 0.3767\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 139s 870ms/step - loss: 0.0686 - val_loss: 0.3753\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 130s 813ms/step - loss: 0.0639 - val_loss: 0.3776\n",
            "Epoch 41/100\n",
            "160/160 [==============================] - 133s 834ms/step - loss: 0.0620 - val_loss: 0.3767\n",
            "Epoch 42/100\n",
            "160/160 [==============================] - 137s 858ms/step - loss: 0.0580 - val_loss: 0.3786\n",
            "Epoch 43/100\n",
            "160/160 [==============================] - 134s 841ms/step - loss: 0.0561 - val_loss: 0.3755\n",
            "Epoch 44/100\n",
            "160/160 [==============================] - 128s 796ms/step - loss: 0.0521 - val_loss: 0.3759\n",
            "Epoch 45/100\n",
            "160/160 [==============================] - 154s 961ms/step - loss: 0.0518 - val_loss: 0.3772\n",
            "Epoch 46/100\n",
            "160/160 [==============================] - 148s 925ms/step - loss: 0.0486 - val_loss: 0.3790\n",
            "Epoch 47/100\n",
            "160/160 [==============================] - 145s 907ms/step - loss: 0.0475 - val_loss: 0.3799\n",
            "Epoch 48/100\n",
            "160/160 [==============================] - 146s 911ms/step - loss: 0.0471 - val_loss: 0.3828\n",
            "Epoch 49/100\n",
            "160/160 [==============================] - 154s 963ms/step - loss: 0.0453 - val_loss: 0.3849\n",
            "Epoch 50/100\n",
            "160/160 [==============================] - 158s 988ms/step - loss: 0.0436 - val_loss: 0.3810\n",
            "Epoch 51/100\n",
            "160/160 [==============================] - 150s 938ms/step - loss: 0.0428 - val_loss: 0.3842\n",
            "Epoch 52/100\n",
            "160/160 [==============================] - 149s 930ms/step - loss: 0.0396 - val_loss: 0.3840\n",
            "Epoch 53/100\n",
            "160/160 [==============================] - 142s 890ms/step - loss: 0.0395 - val_loss: 0.3856\n",
            "Epoch 54/100\n",
            "160/160 [==============================] - 146s 915ms/step - loss: 0.0376 - val_loss: 0.3826\n",
            "Epoch 55/100\n",
            "160/160 [==============================] - 156s 977ms/step - loss: 0.0349 - val_loss: 0.3861\n",
            "Epoch 56/100\n",
            "160/160 [==============================] - 144s 899ms/step - loss: 0.0361 - val_loss: 0.3854\n",
            "Epoch 57/100\n",
            "160/160 [==============================] - 140s 876ms/step - loss: 0.0336 - val_loss: 0.3835\n",
            "Epoch 58/100\n",
            "160/160 [==============================] - 134s 837ms/step - loss: 0.0333 - val_loss: 0.3828\n",
            "Epoch 59/100\n",
            "160/160 [==============================] - 129s 804ms/step - loss: 0.0337 - val_loss: 0.3839\n",
            "Epoch 60/100\n",
            "160/160 [==============================] - 134s 837ms/step - loss: 0.0318 - val_loss: 0.3841\n",
            "Epoch 61/100\n",
            "160/160 [==============================] - 128s 802ms/step - loss: 0.0315 - val_loss: 0.3850\n",
            "Epoch 62/100\n",
            "160/160 [==============================] - 118s 736ms/step - loss: 0.0297 - val_loss: 0.3876\n",
            "Epoch 63/100\n",
            "160/160 [==============================] - 133s 835ms/step - loss: 0.0297 - val_loss: 0.3842\n",
            "Epoch 64/100\n",
            "160/160 [==============================] - 138s 866ms/step - loss: 0.0294 - val_loss: 0.3875\n",
            "Epoch 65/100\n",
            "160/160 [==============================] - 134s 839ms/step - loss: 0.0280 - val_loss: 0.3891\n",
            "Epoch 66/100\n",
            "160/160 [==============================] - 126s 789ms/step - loss: 0.0270 - val_loss: 0.3874\n",
            "Epoch 67/100\n",
            "160/160 [==============================] - 134s 840ms/step - loss: 0.0254 - val_loss: 0.3887\n",
            "Epoch 68/100\n",
            "160/160 [==============================] - 141s 880ms/step - loss: 0.0249 - val_loss: 0.3882\n",
            "Epoch 69/100\n",
            "160/160 [==============================] - 144s 903ms/step - loss: 0.0252 - val_loss: 0.3906\n",
            "Epoch 70/100\n",
            "160/160 [==============================] - 143s 894ms/step - loss: 0.0241 - val_loss: 0.3897\n",
            "Epoch 71/100\n",
            "160/160 [==============================] - 136s 852ms/step - loss: 0.0240 - val_loss: 0.3902\n",
            "Epoch 72/100\n",
            "160/160 [==============================] - 135s 843ms/step - loss: 0.0235 - val_loss: 0.3924\n",
            "Epoch 73/100\n",
            "160/160 [==============================] - 130s 816ms/step - loss: 0.0235 - val_loss: 0.3883\n",
            "Epoch 74/100\n",
            "160/160 [==============================] - 129s 804ms/step - loss: 0.0217 - val_loss: 0.3904\n",
            "Epoch 75/100\n",
            "160/160 [==============================] - 131s 821ms/step - loss: 0.0226 - val_loss: 0.3912\n",
            "Epoch 76/100\n",
            "160/160 [==============================] - 149s 931ms/step - loss: 0.0202 - val_loss: 0.3961\n",
            "Epoch 77/100\n",
            "160/160 [==============================] - 142s 889ms/step - loss: 0.0213 - val_loss: 0.3950\n",
            "Epoch 78/100\n",
            "160/160 [==============================] - 145s 908ms/step - loss: 0.0196 - val_loss: 0.3944\n",
            "Epoch 79/100\n",
            "160/160 [==============================] - 143s 893ms/step - loss: 0.0198 - val_loss: 0.3948\n",
            "Epoch 80/100\n",
            "160/160 [==============================] - 142s 888ms/step - loss: 0.0197 - val_loss: 0.3951\n",
            "Epoch 81/100\n",
            "160/160 [==============================] - 144s 899ms/step - loss: 0.0191 - val_loss: 0.3964\n",
            "Epoch 82/100\n",
            "160/160 [==============================] - 130s 812ms/step - loss: 0.0181 - val_loss: 0.3939\n",
            "Epoch 83/100\n",
            "160/160 [==============================] - 146s 912ms/step - loss: 0.0174 - val_loss: 0.3941\n",
            "Epoch 84/100\n",
            "160/160 [==============================] - 163s 1s/step - loss: 0.0175 - val_loss: 0.3947\n",
            "Epoch 85/100\n",
            "160/160 [==============================] - 148s 924ms/step - loss: 0.0170 - val_loss: 0.3980\n",
            "Epoch 86/100\n",
            "160/160 [==============================] - 152s 951ms/step - loss: 0.0161 - val_loss: 0.3946\n",
            "Epoch 87/100\n",
            "160/160 [==============================] - 143s 896ms/step - loss: 0.0164 - val_loss: 0.3966\n",
            "Epoch 88/100\n",
            "160/160 [==============================] - 136s 848ms/step - loss: 0.0162 - val_loss: 0.3952\n",
            "Epoch 89/100\n",
            "160/160 [==============================] - 135s 846ms/step - loss: 0.0155 - val_loss: 0.3957\n",
            "Epoch 90/100\n",
            "160/160 [==============================] - 140s 874ms/step - loss: 0.0147 - val_loss: 0.3975\n",
            "Epoch 91/100\n",
            "160/160 [==============================] - 133s 832ms/step - loss: 0.0148 - val_loss: 0.3965\n",
            "Epoch 92/100\n",
            "160/160 [==============================] - 120s 750ms/step - loss: 0.0143 - val_loss: 0.3964\n",
            "Epoch 93/100\n",
            "160/160 [==============================] - 117s 729ms/step - loss: 0.0145 - val_loss: 0.3978\n",
            "Epoch 94/100\n",
            "160/160 [==============================] - 118s 738ms/step - loss: 0.0141 - val_loss: 0.3982\n",
            "Epoch 95/100\n",
            "160/160 [==============================] - 118s 739ms/step - loss: 0.0133 - val_loss: 0.3960\n",
            "Epoch 96/100\n",
            "160/160 [==============================] - 118s 737ms/step - loss: 0.0136 - val_loss: 0.3988\n",
            "Epoch 97/100\n",
            "160/160 [==============================] - 116s 726ms/step - loss: 0.0130 - val_loss: 0.3992\n",
            "Epoch 98/100\n",
            "160/160 [==============================] - 115s 718ms/step - loss: 0.0128 - val_loss: 0.3979\n",
            "Epoch 99/100\n",
            "160/160 [==============================] - 115s 717ms/step - loss: 0.0126 - val_loss: 0.3988\n",
            "Epoch 100/100\n",
            "160/160 [==============================] - 113s 706ms/step - loss: 0.0121 - val_loss: 0.3980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ITnzzFlbnj1"
      },
      "source": [
        "#model.save_weights('model_seq2seq_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AxrgT1xbnj2"
      },
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7eHAyoBbnj2"
      },
      "source": [
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0FqEOsbnj2",
        "outputId": "c20a02d4-caff-4156-8ed1-c421f47254ce"
      },
      "source": [
        "#enc_model, dec_model = make_inference_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, None, 200)    2499400     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_17 (LSTM)                  [(None, None, 200),  320800      embedding_23[0][0]               \n",
            "                                                                 input_28[0][0]                   \n",
            "                                                                 input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, None, 12497)  2511897     lstm_17[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,332,097\n",
            "Trainable params: 5,332,097\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_26 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_22 (Embedding)     (None, None, 200)         2499400   \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               [(None, 200), (None, 200) 320800    \n",
            "=================================================================\n",
            "Total params: 2,820,200\n",
            "Trainable params: 2,820,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SsE1jg9bnj3"
      },
      "source": [
        "def decode_sequnce(input_sentence):\n",
        "    states_values = enc_model.predict(\n",
        "        str_to_tokens(input_sentence))\n",
        "    \n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'end':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "        \n",
        "        return decoded_translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU5W5bnbbnj3",
        "outputId": "edcf141b-b60e-4fde-8cee-6537563691a2"
      },
      "source": [
        "decode_sequnce('Where is Beyonce')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' in origin royal in solar power royal royal a tibetan power and power royal power and and royal piano royal power plant royal in music royal royal canadian congo royal heating royal royal and royal canadian air and pure royal royal royal and or and small'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFzswi8bnj3"
      },
      "source": [
        "## Fine Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N1TZJ6Mbnj4",
        "outputId": "818c2220-6647-4493-e944-cb500b166805"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size : {}'.format(VOCAB_SIZE_ft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size : 241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGM_YLisbnj4",
        "outputId": "a5c422a6-cb93-49ec-9c79-bbde63809c27"
      },
      "source": [
        "tokenized_questions_ft = tokenizer.texts_to_sequences(clean_question_ft)\n",
        "\n",
        "maxlen_questions_ft = max([len(x) for x in tokenized_questions_ft])\n",
        "encoder_input_data_ft = pad_sequences(tokenized_questions_ft,\n",
        "                                   maxlen=maxlen_questions_ft,\n",
        "                                   padding='post')\n",
        "\n",
        "print(encoder_input_data_ft.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzgVzuwIbnj5",
        "outputId": "d7df7307-2436-490b-f3f3-33c169dcaba1"
      },
      "source": [
        "maxlen_questions_ft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M12Flgtbnj5",
        "outputId": "7280d28a-37fb-4a65-a1a0-2d611d23be1d"
      },
      "source": [
        "maxlen_answers_ft"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcIJwiwDbnj6",
        "outputId": "658d8606-0ab4-40ca-fa83-4b476f23743b"
      },
      "source": [
        "tokenized_answers_ft = tokenizer.texts_to_sequences(clean_answer_ft)\n",
        "\n",
        "maxlen_answers_ft = max([len(x) for x in tokenized_answers_ft])\n",
        "decoder_input_data_ft = pad_sequences(tokenized_answers_ft,\n",
        "                                   maxlen=maxlen_answers_ft,\n",
        "                                   padding='post')\n",
        "print(decoder_input_data_ft.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7J-nm1bnj6",
        "outputId": "2b181709-0f34-43bb-96f0-e6c3af5dcc00"
      },
      "source": [
        "for i in range(len(tokenized_answers_ft)):\n",
        "    tokenized_answers_ft[i] = tokenized_answers_ft[i][1:]\n",
        "padded_answers_ft = pad_sequences(tokenized_answers_ft, maxlen=maxlen_answers_ft, padding='post')\n",
        "decoder_output_data_ft = to_categorical(padded_answers_ft, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_output_data_ft.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 51, 12497)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg_GcIUlbnj7"
      },
      "source": [
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model_ft = Model([enc_inputs, dec_inputs], output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svQlI08Kbnj7"
      },
      "source": [
        "model_ft.load_weights('model_seq2seq_new.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqUkhwC8bnj7"
      },
      "source": [
        "model_ft.compile(optimizer=RMSprop(), loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKG54Rgjbnj7"
      },
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YwWpWK_bnj7"
      },
      "source": [
        "model_ft.fit([encoder_input_data_ft, decoder_input_data_ft],decoder_output_data_ft,batch_size=50,epochs=100, callbacks = callback, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MFfdixwbnj8"
      },
      "source": [
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhJsK9i2bnj8",
        "outputId": "1a26fbd4-dc68-402b-b41a-d966b485fbff"
      },
      "source": [
        "enc_model, dec_model = make_inference_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    2499400     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 12497)  2511897     lstm_1[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 5,332,097\n",
            "Trainable params: 5,332,097\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 200)         2499400   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, 200), (None, 200) 320800    \n",
            "=================================================================\n",
            "Total params: 2,820,200\n",
            "Trainable params: 2,820,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKK5cTdkbnj8"
      },
      "source": [
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions_ft,\n",
        "                         padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNtLuSqxbnj9"
      },
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    states_values = enc_model.predict(str_to_tokens(input_sentence))\n",
        "    \n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'end':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers_ft:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "        \n",
        "    return decoded_translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g64tYfaNbnj9"
      },
      "source": [
        "## Test Preditions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgzTnwm-bnj9",
        "outputId": "9453fb8e-0a7f-4a1c-9bfc-7e22df4902eb"
      },
      "source": [
        "df_test = pd.read_csv(\"/Users/shivanibalaji/Downloads/test-2.csv\")\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Questions\n",
              "0            Will the pre-class session be recorded?\n",
              "1          What is the deadline for quiz submission?\n",
              "2      What is the deadline for exercise submission?\n",
              "3  How many hours do I need to complete this course?\n",
              "4                       Who will grade the exercise?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J9PryuSbnj-"
      },
      "source": [
        "pred_test = []\n",
        "for q in df_test.Questions.values:\n",
        "    pred_test.append(decode_sequence(q))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCwby_ubnj-"
      },
      "source": [
        "predictions_test_data = pd.DataFrame()\n",
        "predictions_test_data['Questions'] = df_test.Questions.values\n",
        "predictions_test_data['Predicted_answers'] = pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-HaOAY2bnj-",
        "outputId": "5745e6b7-7bfd-4830-ab4a-67d287a9e5a2"
      },
      "source": [
        "predictions_test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Predicted_answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Will the pre-class session be recorded?</td>\n",
              "      <td>and part for the following the of the followi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the deadline for quiz submission?</td>\n",
              "      <td>in special each and the force royal of the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the deadline for exercise submission?</td>\n",
              "      <td>of the canadian army army royal the world to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How many hours do I need to complete this course?</td>\n",
              "      <td>of the water of naraka royal a music awards r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who will grade the exercise?</td>\n",
              "      <td>and are are royal are and all all the with an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Why is the auto-grader failing me?</td>\n",
              "      <td>are is be royal is material royal in this roy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Do I do the exercises individually?</td>\n",
              "      <td>for a each of each royal and the heat royal r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Is the lab compulsory?</td>\n",
              "      <td>of the united provinces school royal the be k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Will the sessions be recorded?</td>\n",
              "      <td>and are is will royal and understanding royal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Can I have access to the recorded videos?</td>\n",
              "      <td>all of the canadian forces royal each of the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Where are the recordings?</td>\n",
              "      <td>and long for their project royal in this roya...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Where can I ask questions regarding reading ma...</td>\n",
              "      <td>a minute royal and to the your buddha royal w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Where to find course material?</td>\n",
              "      <td>week royal be royal and in this is the intern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Do we have homework?</td>\n",
              "      <td>and the global royal life royal and a tibetan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Where can I find the homework?</td>\n",
              "      <td>links a be posted on the material of the budd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Where to submit the homework?</td>\n",
              "      <td>and the tibetan all of the buddha royal is th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Where do I find the homework?</td>\n",
              "      <td>all the four royal canadian be on the th cana...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Where do I submit my homework assignment?</td>\n",
              "      <td>and are your royal tibetan office royal are t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Will the professor take office hours?</td>\n",
              "      <td>of the global the you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What is the OH zoom link?</td>\n",
              "      <td>be you defence la royal the material of the m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What will we do in projects?</td>\n",
              "      <td>and the force royal royal canadian army and f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What should be the duration of the presentatio...</td>\n",
              "      <td>this is the global royal the chinese you the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Do I need to submit the project in a group?</td>\n",
              "      <td>links royal the day of my life royal royal in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Does attendance count to grading?</td>\n",
              "      <td>of the all own germany royal and the united b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Questions  \\\n",
              "0             Will the pre-class session be recorded?   \n",
              "1           What is the deadline for quiz submission?   \n",
              "2       What is the deadline for exercise submission?   \n",
              "3   How many hours do I need to complete this course?   \n",
              "4                        Who will grade the exercise?   \n",
              "5                  Why is the auto-grader failing me?   \n",
              "6                 Do I do the exercises individually?   \n",
              "7                              Is the lab compulsory?   \n",
              "8                      Will the sessions be recorded?   \n",
              "9           Can I have access to the recorded videos?   \n",
              "10                          Where are the recordings?   \n",
              "11  Where can I ask questions regarding reading ma...   \n",
              "12                     Where to find course material?   \n",
              "13                               Do we have homework?   \n",
              "14                     Where can I find the homework?   \n",
              "15                      Where to submit the homework?   \n",
              "16                      Where do I find the homework?   \n",
              "17          Where do I submit my homework assignment?   \n",
              "18              Will the professor take office hours?   \n",
              "19                          What is the OH zoom link?   \n",
              "20                       What will we do in projects?   \n",
              "21  What should be the duration of the presentatio...   \n",
              "22        Do I need to submit the project in a group?   \n",
              "23                  Does attendance count to grading?   \n",
              "\n",
              "                                    Predicted_answers  \n",
              "0    and part for the following the of the followi...  \n",
              "1    in special each and the force royal of the to...  \n",
              "2    of the canadian army army royal the world to ...  \n",
              "3    of the water of naraka royal a music awards r...  \n",
              "4    and are are royal are and all all the with an...  \n",
              "5    are is be royal is material royal in this roy...  \n",
              "6    for a each of each royal and the heat royal r...  \n",
              "7    of the united provinces school royal the be k...  \n",
              "8    and are is will royal and understanding royal...  \n",
              "9    all of the canadian forces royal each of the ...  \n",
              "10   and long for their project royal in this roya...  \n",
              "11   a minute royal and to the your buddha royal w...  \n",
              "12   week royal be royal and in this is the intern...  \n",
              "13   and the global royal life royal and a tibetan...  \n",
              "14   links a be posted on the material of the budd...  \n",
              "15   and the tibetan all of the buddha royal is th...  \n",
              "16   all the four royal canadian be on the th cana...  \n",
              "17   and are your royal tibetan office royal are t...  \n",
              "18                              of the global the you  \n",
              "19   be you defence la royal the material of the m...  \n",
              "20   and the force royal royal canadian army and f...  \n",
              "21   this is the global royal the chinese you the ...  \n",
              "22   links royal the day of my life royal royal in...  \n",
              "23   of the all own germany royal and the united b...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}